{
  
    
        "post0": {
            "title": "Hands-on Machine Learning with Scikit-learn, Keras and Tensorflow - Ch3 (Part 2)",
            "content": "from sklearn.datasets import fetch_openml import numpy as np mnist = fetch_openml(&#39;mnist_784&#39;, version=1) . data, target = mnist.data, mnist.target x_train, y_train, x_test, y_test = data[:60000], target[:60000], data[60000:], target[60000:] . from sklearn.linear_model import SGDClassifier from sklearn.multiclass import OneVsOneClassifier # sometime (very rarely) we might want to train a one vs one classifier instead of one vs all classifier sgd = SGDClassifier(random_state=2) sgd_ovo = OneVsOneClassifier(SGDClassifier(random_state=2)) sgd.fit(x_train, y_train) sgd_ovo.fit(x_train, y_train) # SGDClassifier will automatically stack one vs all classifier while working with multi-class print(&#39;The shape of weights of default SGD multi-class classifier is:&#39;, sgd.coef_.shape) # training the classifier in a one vs one way means training &#39;1&#39; vs &#39;2&#39;, &#39;1&#39; vs &#39;3&#39;... 45 estimators # prediction will pick the label winning the most in all estimators print(&#39;There are %d estimater in one vs one SGDClassifier.&#39; % len(sgd_ovo.estimators_)) print(sgd_ovo.estimators_[0].coef_.shape) . The shape of weights of default SGD multi-class classifier is: (10, 784) There are 45 estimater in one vs one SGDClassifier. (1, 784) . Of course some classifier like decision tree can classify multi-class in its original form (different class are different leaf in the case of decision tree). . from sklearn.preprocessing import StandardScaler import numpy as np scaler = StandardScaler() x_train_scaled = scaler.fit_transform(x_train) . from sklearn.model_selection import cross_val_score result = cross_val_score(SGDClassifier(random_state=2, n_jobs=-1), x_train, y_train, cv=3, scoring=&#39;accuracy&#39;) print(&#39;without scaling, the accuracy is %f +/- %f&#39; % (result.mean(), result.std())) result2 = cross_val_score(SGDClassifier(random_state=2, n_jobs=-1), x_train_scaled, y_train, cv=3, scoring=&#39;accuracy&#39;) print(&#39;with scaling, the accuracy is %f +/- %f&#39; % (result2.mean(), result2.std())) . without scaling, the accuracy is 0.872133 +/- 0.002477 . /Users/sk/anaconda3/envs/myenv/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, . with scaling, the accuracy is 0.897383 +/- 0.005675 . import matplotlib.pyplot as plt import matplotlib as mpt from sklearn.metrics import confusion_matrix from sklearn.model_selection import cross_val_predict # for error analysis, confusion matrix could help us analysis which class the model perform badly pred = cross_val_predict(sgd, x_train_scaled, y_train, cv=3, n_jobs=-1) confusion = confusion_matrix(y_train, pred) fig = plt.figure(figsize=(7, 7)) plt.matshow(confusion, cmap=plt.cm.gray, fignum=fig.number) plt.show() . num_class = confusion.sum(axis=1, keepdims=True) cm_norm = confusion / num_class np.fill_diagonal(cm_norm, 0) fig = plt.figure(figsize=(7, 7)) plt.matshow(cm_norm, cmap=plt.cm.gray, fignum=fig.number) plt.show() . Now we can see that the model tends to mis-classify other sample as &#39;8&#39;, while for the real &#39;8&#39;s actually the model preforming okay. To improve the model, we can, for example, gather more samples looks like &#39;8&#39; or introducing new features helping the identify a non-&#39;8&#39; figure. . Multilabel Classification . Sometimes our task is not just identify the letter of the hand-written number, for example to identify multi-person&#39;s face in the picture, or to classify what topics a paragraph is about, in which the authors put the task as &quot;multilabel&quot; classification. In practice a simple solution is training one to all model for each label, and replace the picking the highest predict_proba to a threshold, and put the label on if the probability is higher than the threshold. .",
            "url": "https://tikuischan.github.io/fastpages_blog/machinelearning/notes/classification/2022/04/30/Hands-On-Machine-Learning-Ch3-Part2.html",
            "relUrl": "/machinelearning/notes/classification/2022/04/30/Hands-On-Machine-Learning-Ch3-Part2.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Hands-on Machine Learning with Scikit-learn, Keras and Tensorflow - Ch3 (Part 1)",
            "content": "MNIST data set: &quot;Hello world&quot; dataset of machine learning, with 70_000 hand write digit images . from sklearn.datasets import fetch_openml import numpy as np mnist = fetch_openml(&#39;mnist_784&#39;, version=1) . type(mnist) . sklearn.utils.Bunch . mnist.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;categories&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;details&#39;, &#39;url&#39;]) . print(mnist.DESCR) . **Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges **Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown **Please cite**: The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field. With some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST&#39;s NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST&#39;s datasets. The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available. Downloaded from openml.org. . data, target = mnist.data, mnist.target print(&#39;data is a %s and target is a %s&#39; % (type(data), type(target))) print(&#39;The shape of data is %d, %d, and the lenght of target is (as expected) %d&#39; % (data.shape[0], data.shape[1], len(target))) . data is a &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; and target is a &lt;class &#39;pandas.core.series.Series&#39;&gt; The shape of data is 70000, 784, and the lenght of target is (as expected) 70000 . target = target.astype(np.uint8) . import matplotlib as mpl import matplotlib.pyplot as plt # get a random sample sample_data = data.sample(1) # convert to numpy series =&gt; reshape to 28x28 matrix x = sample_data.to_numpy().reshape(28, 28) idx = sample_data.index # show image plt.imshow(x, cmap = mpl.cm.binary, interpolation=&quot;nearest&quot;) plt.axis(&quot;off&quot;) plt.show() # show label print(target[idx]) . 59523 1 Name: class, dtype: uint8 . # according to description: first 60_000 as train and last 10_000 as train set x_train, y_train, x_test, y_test = data[:60000], target[:60000], data[60000:], target[60000:] . 1. Binary Classification . i.e. Only determine if the image is the selected class (number) or not . TARGET = 2 y_train_binary = y_train == TARGET y_test_binary = y_test == TARGET . from sklearn.linear_model import SGDClassifier sgd = SGDClassifier() sgd.fit(x_train, y_train_binary) . SGDClassifier() . sgd.predict(sample_data) . array([False]) . from sklearn.model_selection import cross_val_score from sklearn.model_selection import StratifiedKFold from sklearn.base import BaseEstimator skfold = StratifiedKFold(n_splits=3) # calc. model performance result = cross_val_score( SGDClassifier(), x_train, y_train_binary, cv=skfold, scoring=&#39;accuracy&#39;, n_jobs=-1 ) print(f&#39;Accuracy of SGD classifier is : {result.mean()} +/- {result.std()}&#39;) # create a dummy classifier which always predict false class Dummy(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return np.zeros((len(X), 1), dtype=bool) # calc. dummy model performance result_dummy = cross_val_score( Dummy(), x_train, y_train_binary, cv=skfold, scoring=&#39;accuracy&#39;, n_jobs=-1 ) print(f&#39;Accuracy of dummy classifier is : {result_dummy.mean()} +/- {result_dummy.std()}&#39;) . Accuracy of SGD classifier is : 0.9677666666666666 +/- 0.007091348406489577 Accuracy of dummy classifier is : 0.9007 +/- 0.0 . For skewed data set (only 10% data is &#39;2&#39;), we can get 90% accuracy by predicting not &#39;2&#39; without thinking. That&#39;s why in machine learning people always use confusion matrix (precision and recall, and eventually $F_{ beta}$ score) instead of accuracy. . Precision: For all the items predicted as &#39;positive&#39;, proportion of true &#39;positive&#39; . $ frac{True Positive}{ALL Predicted As Positive} = frac{True Positive}{True Positive + False Positive}$ . Recall: For all the items labeled as positive (by human / expert), proportion of the model predicted as &#39;positive&#39; . $ frac{True Positive}{False Negative + True Positive} $ . from sklearn.metrics import confusion_matrix from sklearn.model_selection import cross_val_predict # use cross_val_predict to get &quot;clean&quot; prediction (using validation data) y_binary_pred = cross_val_predict( SGDClassifier(random_state=2), x_train, y_train_binary, cv=skfold, n_jobs=-1 ) confusion_matrix(y_train_binary, y_binary_pred) . array([[53480, 562], [ 1296, 4662]]) . cm = confusion_matrix(y_train_binary, y_binary_pred) # use matshow to display confusion matrix as graph plt.matshow(cm, cmap=plt.cm.gray) plt.show() . While F1 score (balancing precision and recall) is relatively common in analysing Machine Learning model, a more general form of $F_{ beta}$ is as followed: . $F_{ beta} = (1 + beta ^ 2) frac{precision times recall}{ beta ^ 2 times precision + recall} = frac{(1 + beta ^ 2) tp}{(1 + beta ^ 2) tp + beta ^ 2 fn + np}$ . in whice $ beta &gt; 1$ the penalty on false negative is higher (i.e. focus more on recall). . Reference: DeepAI . from sklearn.metrics import precision_recall_curve # to visualize the precision/recall tradeoff, we can plot the precision and recall against different threshold # default `method` is `predict`, now we are going to get the decision function and compare it with different threshold y_scores = cross_val_predict( SGDClassifier(random_state=2), x_train, y_train_binary, cv=skfold, n_jobs=-1, method=&#39;decision_function&#39; ) precisions, recalls, thresholds = precision_recall_curve(y_train_binary, y_scores) . plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;) plt.title(&#39;Precision and Recall vs Thresholds&#39;) plt.legend() plt.show() . plt.figure(figsize=(7, 7)) plt.plot(recalls, precisions, &quot;b-&quot;) plt.title(&#39;Precision vs Recall&#39;) plt.xlabel(&#39;recall&#39;) plt.ylabel(&#39;precision&#39;) plt.show() . from sklearn.metrics import roc_curve # another commonly used metrics is roc curve, which is the true positive rate vs false positive rate fpr, tpr, thresholds = roc_curve(y_train_binary, y_scores) plt.figure(figsize=(7,7)) plt.plot(fpr, tpr) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.ylabel(&#39;True positive rate&#39;) plt.xlabel(&#39;False positive rate&#39;) plt.title(&#39;ROC curve&#39;) plt.show() . And the performance of the model is summarized by the area under curve (AUC), a purely random model has AUC of 0.5 (same as the diagonal line), while a perfect model should have AUC of 1. . For an example, we can compare the preformances of the SGD trained with different number of samples. . sample_sizes = [100, 1_000, 10_000] plt.figure(figsize=(7,7)) for sample_size in sample_sizes: model = SGDClassifier(random_state=2) y_scores = cross_val_predict( model, x_train[:sample_size], y_train_binary[:sample_size], cv=3, n_jobs=-1, method=&#39;decision_function&#39; ) fpr, tpr, thresholds = roc_curve(y_train_binary[:sample_size], y_scores) plt.plot(fpr, tpr, label=f&#39;sample size: {sample_size}&#39;) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.ylabel(&#39;True positive rate&#39;) plt.xlabel(&#39;False positive rate&#39;) plt.legend() plt.title(&#39;ROC curve&#39;) plt.show() . As the training sample size increase, the ROC curve is expending towards top left corner and the AUC is increasing, meaning that the model is getting better. .",
            "url": "https://tikuischan.github.io/fastpages_blog/machinelearning/notes/classification/2022/04/24/Hands-On-Machine-Learning-Ch3-Part1.html",
            "relUrl": "/machinelearning/notes/classification/2022/04/24/Hands-On-Machine-Learning-Ch3-Part1.html",
            "date": " • Apr 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Searching Algorithm",
            "content": "Defining a searching problem . can either &quot;walk&quot; or &quot;tram&quot; on each state | when choose &quot;walk&quot;, count add 1 and cost add 1 | when choose &quot;tram&quot;, count multiply by 2 and cost add 2 | . class CountGame: def __init__(self, goal): self.goal = goal def state(self): return self.goal def is_end(self, state): return state == self.goal def succ_and_cost(self, state): output = [] if state &lt;= self.goal // 2: output.append((&#39;tram&#39;, state * 2, 2)) if state &lt; self.goal: output.append((&#39;walk&#39;, state + 1, 1)) return output . recursion method - brute force . import math . def recursion(problem): result = { &#39;cost&#39;: math.inf, # we can also use a very large number (1,000,000 for example) instead &#39;history&#39;: [] } def recurse(state, history, total_cost): if problem.is_end(state): # return if reach end state if total_cost &lt; result[&#39;cost&#39;]: # update best result if total cost is lower result[&#39;cost&#39;] = total_cost result[&#39;history&#39;] = history return for action, new_state, cost in game.succ_and_cost(state): next_history = history + [(action, new_state, total_cost + cost)] recurse(new_state, next_history, total_cost + cost) recurse(1, [], 0) return result . %%time game = CountGame(200) recursion(game) . CPU times: user 6.68 s, sys: 24.6 ms, total: 6.71 s Wall time: 6.74 s . {&#39;cost&#39;: 15, &#39;history&#39;: [(&#39;walk&#39;, 2, 1), (&#39;walk&#39;, 3, 2), (&#39;tram&#39;, 6, 4), (&#39;tram&#39;, 12, 6), (&#39;tram&#39;, 24, 8), (&#39;walk&#39;, 25, 9), (&#39;tram&#39;, 50, 11), (&#39;tram&#39;, 100, 13), (&#39;tram&#39;, 200, 15)]} . It takes more than 6 seconds to search for best path towards 200... . Dynamic programming . def dynamic_programming(problem): cashe = {} # store calculated items in cashe to save computation time result = { &#39;cost&#39;: math.inf, &#39;history&#39;: [] } def future_cost(state): if cashe.get(state): return cashe[state] if problem.is_end(state): return 0, [] # minor change to trace action history and store in cashe tot_cost = math.inf for action, new_state, cost in problem.succ_and_cost(state): f_cost, new_history = future_cost(new_state) if cost + f_cost &lt; tot_cost: tot_cost = cost + f_cost next_action = [(action, new_state, tot_cost)] + new_history cashe[state] = tot_cost, next_action # original code in lecutre video, action history is not output # cashe[state] = min(cost + future_cost(new_state) # for action, new_state, cost in problem.succ_and_cost(state)) return cashe[state] result[&#39;cost&#39;], result[&#39;history&#39;] = future_cost(1) return result . %%time game = CountGame(1000) dynamic_programming(game) . CPU times: user 4.87 ms, sys: 729 µs, total: 5.6 ms Wall time: 5.59 ms . {&#39;cost&#39;: 22, &#39;history&#39;: [(&#39;walk&#39;, 2, 22), (&#39;walk&#39;, 3, 21), (&#39;tram&#39;, 6, 20), (&#39;walk&#39;, 7, 18), (&#39;tram&#39;, 14, 17), (&#39;walk&#39;, 15, 15), (&#39;tram&#39;, 30, 14), (&#39;walk&#39;, 31, 12), (&#39;tram&#39;, 62, 11), (&#39;tram&#39;, 124, 9), (&#39;walk&#39;, 125, 7), (&#39;tram&#39;, 250, 6), (&#39;tram&#39;, 500, 4), (&#39;tram&#39;, 1000, 2)]} . much faster. But will exceed the recursion limit when search up to 10,000 . Uniform search . Define data structure to store the frontier and explored nodes . from operator import itemgetter class UniformSearchFrontier: def __init__(self): self.frontier = [] def update(self, previous_state, action, new_state, cost): i = None for n, (ps, a, ns, c) in enumerate(self.frontier): if ns != new_state: continue if c &lt;= cost: # if cost is larger than the cost already in frontier =&gt; do nothing return i = n if i is None: # insert new frontier self.frontier.append((previous_state, action, new_state, cost)) return self.frontier[i] = (previous_state, action, new_state, cost) # replace frontier item with lower cost # insert frontier item in order (slower) # cost_list = [x[3] for x in self.frontier] # insert_pos = self.find_insert_index(cost, cost_list) # self.frontier = self.frontier[:insert_pos] + [(previous_state, action, new_state, cost)] + self.frontier[insert_pos:] def find_insert_index(self, cost, sorted_list): # binary search pos if len(sorted_list) == 1: return 1 if sorted_list[0] &gt; cost else 0 if len(sorted_list) == 0: return 0 index = len(sorted_list) // 2 if sorted_list[index] &gt; cost: return index + self.find_insert_index(cost, sorted_list[index:]) else: return self.find_insert_index(cost, sorted_list[:index]) def sort_frontier(self): self.frontier = sorted(self.frontier, key=itemgetter(3), reverse=True) def pop_min_item(self): if not self.frontier: raise ValueError(&#39;No frontier left.&#39;) self.sort_frontier() # sort frontier when needed (faster) return self.frontier.pop() . With the frontier data structure defined, the rest is quite straight forward. . def uniform_search(problem): explored = {} frontier = UniformSearchFrontier() frontier.update(0, &#39;start&#39;, 1, 0) while True: previous_state, action, state, tot_cost = frontier.pop_min_item() explored[state] = (previous_state, tot_cost, action) if problem.is_end(state): break for action, new_state, cost in problem.succ_and_cost(state): if new_state in explored: continue frontier.update(state, action, new_state, tot_cost + cost) # trace action history for output result = [] s = problem.goal while explored.get(s): result.append((explored[s][2], s, explored[s][1])) s = explored[s][0] return result . %%time game = CountGame(1000) uniform_search(game) . CPU times: user 24.8 ms, sys: 1.69 ms, total: 26.5 ms Wall time: 25.2 ms . [(&#39;tram&#39;, 1000, 22), (&#39;tram&#39;, 500, 20), (&#39;tram&#39;, 250, 18), (&#39;walk&#39;, 125, 16), (&#39;tram&#39;, 124, 15), (&#39;tram&#39;, 62, 13), (&#39;walk&#39;, 31, 11), (&#39;tram&#39;, 30, 10), (&#39;walk&#39;, 15, 8), (&#39;tram&#39;, 14, 7), (&#39;walk&#39;, 7, 5), (&#39;tram&#39;, 6, 4), (&#39;walk&#39;, 3, 2), (&#39;walk&#39;, 2, 1), (&#39;start&#39;, 1, 0)] . In this case uniform search is not restricted by recursion limit. . %%time game = CountGame(100000) uniform_search(game) . CPU times: user 1min 21s, sys: 204 ms, total: 1min 21s Wall time: 1min 21s . [(&#39;tram&#39;, 100000, 36), (&#39;tram&#39;, 50000, 34), (&#39;tram&#39;, 25000, 32), (&#39;tram&#39;, 12500, 30), (&#39;tram&#39;, 6250, 28), (&#39;walk&#39;, 3125, 26), (&#39;tram&#39;, 3124, 25), (&#39;tram&#39;, 1562, 23), (&#39;walk&#39;, 781, 21), (&#39;tram&#39;, 780, 20), (&#39;tram&#39;, 390, 18), (&#39;walk&#39;, 195, 16), (&#39;tram&#39;, 194, 15), (&#39;walk&#39;, 97, 13), (&#39;tram&#39;, 96, 12), (&#39;tram&#39;, 48, 10), (&#39;tram&#39;, 24, 8), (&#39;tram&#39;, 12, 6), (&#39;tram&#39;, 6, 4), (&#39;walk&#39;, 3, 2), (&#39;walk&#39;, 2, 1), (&#39;start&#39;, 1, 0)] . reference: . Stanford CS221: AI (Autumn 2019) | .",
            "url": "https://tikuischan.github.io/fastpages_blog/python/searchingalgorithm/2021/11/13/Searching-Algorithm.html",
            "relUrl": "/python/searchingalgorithm/2021/11/13/Searching-Algorithm.html",
            "date": " • Nov 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Kaggle Lux AI Session 1 - Day 2 & 3",
            "content": "Highlight of some rules and my thoughts. . . Important: Both teams have complete information about the entire game state and will need to make use of that information to optimize resource collection, compete for scarce resources against the opponent, and build cities to gain points. . Having complete information about the entire game is huge, we can consider strategy like avoiding opponent&#39;s base whatever. . . Important: Each competitor must program their own agent in their language of choice. Each turn, your agent gets 3 seconds to submit their actions, excess time is not saved across turns. In each game, you are given a pool of 60 seconds that is tapped into each time you go over a turn&#8217;s 3-second limit. Upon using up all 60 seconds and going over the 3-second limit, your agent freezes and can no longer submit additional actions. . Total action submition period is 60 seconds, and there is a 3-second limit each turn. I suppose that means computation time is 3 seconds max for each turn? May need a bit experinment on that. . . Important: In order to prevent maps from favoring one player over another, it is guaranteed that maps are always symmetric by vertical or horizontal reflection. . That means calculating one half of the map should be enough, might help in shortening the computation time each turn. . . Important: There are 3 kinds of resources: Wood, Coal, and Uranium (in order of increasing fuel efficiency). These resources are collected by workers, then dropped off once a worker moves on top of a CityTile to then be converted into fuel for the city. Some resources require research points before they are possible to collect. . Not sure how to get the research points, although coal and uranium are better resources, if the research point is too difficult to get, that is still not a very good duel. Will need some kind of margin to balance in late game. . . Important: Wood in particular can regrow. Each turn, every wood tile&#8217;s wood amount increases by 2.5% of its current wood amount rounded up. Wood tiles that have been depleted will not regrow. Only wood tiles with less than 500 wood will regrow. . This is a big one, only wood is somehow an infinite resource. Controlling wood tile could affect the long run. . Resource Type Research Points Pre-requisite Fuel Value per Unit Units Collected per Turn . 0 wood | 0 | 1 | 20 | . 1 coal | 50 | 10 | 5 | . 2 uranium | 200 | 40 | 2 | . For wood, each turn can collect 20 fuel value, 50 for coal and 80 for uranium. Convert to research point per increase in fuel value collection rate increase, 3/5 for wood to coal and 1/5 for coal to uranium. The marginal return seems obvious. But there might also be some map with large amount of coal and uranium but not much trees. . . Important: After 360 turns the winner is whichever team has the most CityTiles on the map. If that is a tie, then whichever team has the most units owned on the board wins. If still a tie, the game is marked as a tie. A game may end early if a team no longer has any more Units or CityTiles. Then the other team wins. . There are at most 360 turns, so the first goal should to survive that many of turn, resources more than that are considered as wasted. . . Important: Moreover, all units can carry raw resources gained from automatic mining or resource transfer. Workers are capped at 100 units of resources and Carts are capped at 2000 units of resources. . Some advance stragy in resource managment, building a cart and build a worker, if build a cart, how much resource should it collect etc. don&#39;t want to bother it at the beginning. . . Important: The Day/Night cycle consists of a 40 turn cycle, the first 30 turns being day turns, the last 10 being night turns. There are a total of 360 turns in a match, forming 9 cycles. During the night, Units and Cities need to produce light to survive. Each turn of night, each Unit and CityTile will consume an amount of fuel, see table below for rates. Units in particular will use their carried resources to produce light whereas CityTiles will use their fuel to produce light. . The most important part of the game. Unit automatically disappear if resources is too low, stragies like asking worker to go back to cityTile close to night might be needed later. .",
            "url": "https://tikuischan.github.io/fastpages_blog/luxai/kaggle/2021/10/08/LUX-AI-Session-1-Day2andDay3.html",
            "relUrl": "/luxai/kaggle/2021/10/08/LUX-AI-Session-1-Day2andDay3.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Kaggle Lux AI Session 1 - A Week Later",
            "content": "After the last blog update I started to modify the starter template by Stone TAO for my own version of baseline and messing around a bit. . The starter template the action is simple - ask the (initial) worker to go to the closest (and accessable) resources and come back to city when its pocket is full, repeatedly until the end of the world. So that the final score is always 1 (1 city) unless there are accident that the worker stucked somewhere, as there is no path finding algorithm in the template. So my attempt is to give it the ability to get more points and keep it simple and basic (actually I&#39;d love to see how far can basic algorithms go before switching to reinforcement learning or event deep reinforcement learning). . So I marked the first worker as a special worker to build city, and set the building condition to be the amount of fuel of cities is high enough for the coming night (otherwise we will loss all cities immediately after the first night). The expension tile is any tile next to the first city and closest to the worker. Cities build worker whenever they could, and those new workers simply follow the basic algorithm to gather resources. The modification is not too bad at the very beginning, but soon workers start jamming, the supply chain stopped and eventually everything dies out(even the program bug out because there is no unit left to choose LOL). Another problem is that the workers automatically drop all resources when it reach a cityTile, sometimes (maybe half of the times) the building worker will drop all resources while moving to the new cityTile position. . So I patch the script a bit: . rewrite the &quot;path finding&quot; function, go another way or simply do not move if it cannot (just to stop the clashing alert) | add &quot;barrier&quot; concept to the &quot;path finding&quot; function, conditionally add cityTile as barrier to avoid worker accidentially drop all resources while trying to build a new cityTile | go for multiple resources (need review logic) | Finally v0.18 seems good enough to beat the simple one base ai with a relatively high chance. .",
            "url": "https://tikuischan.github.io/fastpages_blog/luxai/kaggle/2021/10/08/LUX-AI-Session-1-A-Week-Later.html",
            "relUrl": "/luxai/kaggle/2021/10/08/LUX-AI-Session-1-A-Week-Later.html",
            "date": " • Oct 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Kaggle Lux AI Session 1 - Day 1",
            "content": "The Lux AI Challenge of Kaggle is a 1v1 match in which each side have to gather resources from the map and expand CityTiles to win. The challenge will be held until December and you can check it out at here. sentdex has a youtube walking through how he is doing in this compedition (I watched the first minutes and close immediately so that I can struggle for several more weeks) and you and find it here. At day 1 there is nothing special, just read through the introduction, learning how to use the API and try to follow the kaggle template for the first submittion (at least I submit once! yay!). The rules seems a bit long and I will try to do an anlysis for that and mess around with the API tomorrow. .",
            "url": "https://tikuischan.github.io/fastpages_blog/luxai/kaggle/2021/10/06/LUX-AI-Session-1-Day1.html",
            "relUrl": "/luxai/kaggle/2021/10/06/LUX-AI-Session-1-Day1.html",
            "date": " • Oct 6, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "I Took Kaggle Intermediate Machine Learning Course and Here is What I've Learnt",
            "content": "Kaggle held a 30 days of ML challenge aimming to help people to get start with Data Science and Machine Learning (&quot;Machine learning beginner → Kaggle competitor in 30 days. Non-coders welcome.&quot; they claimed). During the challenge Kaggle will send an email to participants everyday assiging a task (maybe read a tutorial or finish an exercise). At the first 14 days are the Python, Intro to Machine Learning and Intermediate Machine Learning. In this blog I will share what I&#39;ve learnt in the Intermediate Machine Learning course of Kaggle. . Who is this course for? . This intermediate course is for someone who know basic python and machine learning. i.e. someone have finished Python and Intro to Machine Learning course (or already know the content). . How long does it take? . According to Kaggle&#39;s description it takes 4 hours to finish, with some experiment / exploring I would say about a day or two to completely understand what&#39;s going on. . One sentance summary of what to expected in this course . Introduction to a relatively complete Machine Learning work flow, from data cleansing to generate prediction. . Do I recommend it? . As an &quot;Intermediate Machine Learning&quot; course, I would say the content is not too intermediate. But considering it is a short course of 4 hours and a sequel of the Intro to Machine Learning course, this course does a decent job. Following the course students can build a complete machine learning pipeline with scikit learn and xgboost. . One thing I did not get used to is that throughout the two courses they emphasis decision tree and gredient boosting, maybe comparing different models is more advance so that they decided to put it in advance machine learning course (if they have). Also as an &quot;intermediate&quot; course I expect to left more &quot;waiting for you to explore&quot; moment inside the pipeline for students who are interested, although I understand that as a short course of only 4 hours it may be not a good place to add too much content. . To conclude I think this is a interesting course and I enjoy it (plus xgboost is new to me, surely I&#39;d love to spend a bit more time to do some exploration). If you just finish the Intro Machine Learning Course, I highly recommend taking the intermediate machine learning course to complete the picture. .",
            "url": "https://tikuischan.github.io/fastpages_blog/machinelearning/kaggle/2021/09/03/I-took-Kaggle-intermediate-ML-course.html",
            "relUrl": "/machinelearning/kaggle/2021/09/03/I-took-Kaggle-intermediate-ML-course.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "I Took Kaggle Introduction to Machine Learning Course and Here is What I've Learnt",
            "content": "Kaggle held a 30 days of ML challenge aimming to help people to get start with Data Science and Machine Learning (&quot;Machine learning beginner → Kaggle competitor in 30 days. Non-coders welcome.&quot; they claimed). During the challenge Kaggle will send an email to participants everyday assiging a task (maybe read a tutorial or finish an exercise). At the first 14 days are the Python, Intro to Machine Learning and Intermediate Machine Learning. In this blog I will share what I&#39;ve learnt in the Intro to Machine Learning course of Kaggle. . Spoilar: If you 100% know what the following codes doing, you could skip this course. . # The following codes are directly from kaggle, please visit the intro to machine learning course for more details # Import helpful libraries import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error from sklearn.model_selection import train_test_split # Load the data, and separate the target iowa_file_path = &#39;../input/train.csv&#39; home_data = pd.read_csv(iowa_file_path) y = home_data.SalePrice # Create X (After completing the exercise, you can return to modify this line!) features = [&#39;LotArea&#39;, &#39;YearBuilt&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;FullBath&#39;, &#39;BedroomAbvGr&#39;, &#39;TotRmsAbvGrd&#39;] # Select columns corresponding to features, and preview the data X = home_data[features] X.head() # Split into validation and training data train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) # Define a random forest model rf_model = RandomForestRegressor(random_state=1) rf_model.fit(train_X, train_y) rf_val_predictions = rf_model.predict(val_X) rf_val_mae = mean_absolute_error(rf_val_predictions, val_y) print(&quot;Validation MAE for Random Forest Model: {:,.0f}&quot;.format(rf_val_mae)) . . Who is this course for? . This course, and the python course, are suitable for absolute beginners who have zero or very few coding experience or self-learner at the beginning stage who want some guidelines on how to start. . How long does it take? . According to Kaggle&#39;s description it takes 3 hours to finish, with some experiment / exploring I would say about a day or two to completely understand what&#39;s going on. . One sentance summary of what to expected in this course . A very soft introduction on how to build a machine learning model. . Do I recommend it? . I like the style of the course that it have tutorial page and exercises with some empty lines for students to fill in, which is much better than those &quot;just run all cells and you are done&quot; type notebooks. The level suit people with very few coding experiences and there are also summary at the end of the course, all those are very good for an absolute beginner. As a intro to Machine Learning this is a very good starting point. . On the other hand, as a Data Scientist I focus more on how to tackle a problem (e.g. baseline model and feature extraction etc.), maybe a bit closer to the AI Ethics side of the story. . In short, this course is a good introduction to machine learning technique, I will recommand to combine with some data science ethics concepts before starting your own data project. .",
            "url": "https://tikuischan.github.io/fastpages_blog/machinelearning/kaggle/2021/08/16/I-Took-Kaggle-Intro2ML-Course.html",
            "relUrl": "/machinelearning/kaggle/2021/08/16/I-Took-Kaggle-Intro2ML-Course.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "An Brief Introduction to Neuron Networks",
            "content": "To begin with, let&#39;s talk about what is AI, Mechine learning and neuron networks, what are they and is there any differences between them. . Aritficial Intelligence: Generally all sorts of program / mechines with logic behide to make decision, no matter the logic is explicitly writen in program or using machine learning, all of them belongs to area of aritficial intelligence; | Machine Learning: Instead of writing a bunch of &quot;if this happened, do this&quot; explicitly in the program, we can also let the machine to &quot;learn&quot; from data. Through feeding the model (we call the machine learning program &quot;model&quot;) the machine can &quot;learn&quot; how to handle different data by adjusting it&#39;s parameters. | Neuron Networks: A type of machine learning algorithm that mimic (although the underline principle are not so alike) the brain of human. The basic component of a neuron network is, as expected, a neuron. Each neuron takes input, multiply with weights (parameters) and combine them to form the output of the neuron. Single neuron is basically a linear regression model and is not quite useful, but combining neurons to form neuron networks can do some amazing things. -Deep Neuron Networks / Deep Learning: Any neuron networks that with 2 hidden layers is called deep neruon networks. | . So these terms can be considered as sub-class relation: AI $ ni$ Machine Learning $ ni$ Neuron Networks $ ni$ Deep Learning . Data . import numpy as np # create 250 sample with random coordinate mu, sigma = 1, 0.3 # mean and standard deviation num_data = 250 X = np.array(list(zip(np.random.normal(mu, sigma, num_data), np.random.normal(mu, sigma, num_data)))) # group 1 if x &gt; 1 else group 2 def get_class(x): return 1 if x &gt; 1 else 0 # label Y = [get_class(x1) for (x1, x2) in X] . First 5 of our sample data: . import itertools for x, y in itertools.islice(zip(X, Y), 5): print(&#39;coordinate: &#39;, x, &#39;, &#39;, &#39;group: &#39;, y) . coordinate: [1.22570236 1.36142329] , group: 1 coordinate: [0.96461332 1.04534488] , group: 0 coordinate: [0.76080813 0.66571496] , group: 0 coordinate: [1.14608594 1.26698552] , group: 1 coordinate: [1.46947276 0.87573308] , group: 1 . import matplotlib.pyplot as plt # group 0 in red and group 1 in blue color = list(map(lambda y: &#39;r&#39; if y == 0 else &#39;b&#39;, Y)) # plotting the sample data plt.rcParams[&quot;figure.figsize&quot;] = (8, 8) plt.scatter(X[:,0], X[:,1], c=color) plt.title(&#39;Toy Sample Data&#39;) plt.xlim(0, 2) plt.ylim(0, 2) plt.xlabel(&quot;X1&quot;) plt.ylabel(&quot;X2&quot;) plt.show() . Create a DataLoader (or the DataLoaders in fastai) object for feeding data to our model. Let&#39;s use 150 data points as training data and the rest of them as testing data. . Creating the first deep neuron networks . We will now create our first deep (i.e. 2 hidden layers) neural networks, although most of the time tensorflow was used in industry, Pytorch is gaining its place in the recent years, especially in academic. Our Deep Learning Networks is relatively simple, with 2 input neurons in the first layer, two hidden layer with 4 neurons and output layer with 2 neurons. . ** figure generated using NN-SVG . import torch import torch.nn as nn import torch.nn.functional as F class OurFirstNet(nn.Module): def __init__(self): super(OurFirstNet, self).__init__() # define the architecture here self.fc1 = nn.Linear(2, 4) self.fc2 = nn.Linear(4, 4) self.fc3 = nn.Linear(4, 4) self.fc4 = nn.Linear(4, 2) def forward(self, x): # Pass data through fc1 x = self.fc1(x) # Use the rectified-linear activation function over x x = F.relu(x) # same method in the hidden layers x = self.fc2(x) x = F.relu(x) x = self.fc3(x) x = F.relu(x) # and we are going to pass through a sigmoid function before output x = self.fc4(x) # output = torch.sigmoid(x) output = F.relu(x) return x test = OurFirstNet() print(test) . OurFirstNet( (fc1): Linear(in_features=2, out_features=4, bias=True) (fc2): Linear(in_features=4, out_features=4, bias=True) (fc3): Linear(in_features=4, out_features=4, bias=True) (fc4): Linear(in_features=4, out_features=2, bias=True) ) . Actually the network layers can also be hold in a torch.Sequential, so that we don&#39;t need to repeatly copy and palsing &quot;x = self.fcX(x) ...&quot; in the forward function. . And we made our first deep neuron networks!, let&#39;s try it out by feeding it with a random data... . random_data = torch.rand(1, 2) # to get the output, simply run random_data_output = test(random_data) print(random_data_output) . tensor([[-0.2467, -0.1208]], grad_fn=&lt;AddmmBackward&gt;) . Notice that the output is vector of length 2, indicating the probability (not a valid value now, but will become more reasonable after training) of the input data being group 1 or 2. . from torch.utils.data import DataLoader train_dataloader = DataLoader(list(zip(X, Y)), batch_size=64, shuffle=True) . from torch import optim # Initialize the loss function and the optimizer loss_fn = nn.CrossEntropyLoss() # loss_fn = nn.NLLLoss() optimizer = torch.optim.Adam(test.parameters(), lr=0.001) # optimizer = torch.optim.SGD(test.parameters(), lr=0.001) # how many time we want our data to pass through the model EPOCHS = 100 for epoch_num in range(EPOCHS): for batcb_num, (X, y) in enumerate(train_dataloader): test.zero_grad() pred = test(X.float()) loss = loss_fn(pred, y) loss.backward() optimizer.step() print(&#39;epoch: &#39;, epoch_num, &#39;loss: &#39;, loss) . epoch: 0 loss: tensor(0.6775, grad_fn=&lt;NllLossBackward&gt;) epoch: 1 loss: tensor(0.6803, grad_fn=&lt;NllLossBackward&gt;) epoch: 2 loss: tensor(0.6797, grad_fn=&lt;NllLossBackward&gt;) epoch: 3 loss: tensor(0.6752, grad_fn=&lt;NllLossBackward&gt;) epoch: 4 loss: tensor(0.6778, grad_fn=&lt;NllLossBackward&gt;) epoch: 5 loss: tensor(0.6775, grad_fn=&lt;NllLossBackward&gt;) epoch: 6 loss: tensor(0.6659, grad_fn=&lt;NllLossBackward&gt;) epoch: 7 loss: tensor(0.6688, grad_fn=&lt;NllLossBackward&gt;) epoch: 8 loss: tensor(0.6728, grad_fn=&lt;NllLossBackward&gt;) epoch: 9 loss: tensor(0.6751, grad_fn=&lt;NllLossBackward&gt;) epoch: 10 loss: tensor(0.6715, grad_fn=&lt;NllLossBackward&gt;) epoch: 11 loss: tensor(0.6726, grad_fn=&lt;NllLossBackward&gt;) epoch: 12 loss: tensor(0.6659, grad_fn=&lt;NllLossBackward&gt;) epoch: 13 loss: tensor(0.6607, grad_fn=&lt;NllLossBackward&gt;) epoch: 14 loss: tensor(0.6613, grad_fn=&lt;NllLossBackward&gt;) epoch: 15 loss: tensor(0.6520, grad_fn=&lt;NllLossBackward&gt;) epoch: 16 loss: tensor(0.6729, grad_fn=&lt;NllLossBackward&gt;) epoch: 17 loss: tensor(0.6513, grad_fn=&lt;NllLossBackward&gt;) epoch: 18 loss: tensor(0.6556, grad_fn=&lt;NllLossBackward&gt;) epoch: 19 loss: tensor(0.6507, grad_fn=&lt;NllLossBackward&gt;) epoch: 20 loss: tensor(0.6428, grad_fn=&lt;NllLossBackward&gt;) epoch: 21 loss: tensor(0.6413, grad_fn=&lt;NllLossBackward&gt;) epoch: 22 loss: tensor(0.6599, grad_fn=&lt;NllLossBackward&gt;) epoch: 23 loss: tensor(0.6360, grad_fn=&lt;NllLossBackward&gt;) epoch: 24 loss: tensor(0.6341, grad_fn=&lt;NllLossBackward&gt;) epoch: 25 loss: tensor(0.6232, grad_fn=&lt;NllLossBackward&gt;) epoch: 26 loss: tensor(0.6299, grad_fn=&lt;NllLossBackward&gt;) epoch: 27 loss: tensor(0.6473, grad_fn=&lt;NllLossBackward&gt;) epoch: 28 loss: tensor(0.6147, grad_fn=&lt;NllLossBackward&gt;) epoch: 29 loss: tensor(0.5953, grad_fn=&lt;NllLossBackward&gt;) epoch: 30 loss: tensor(0.6400, grad_fn=&lt;NllLossBackward&gt;) epoch: 31 loss: tensor(0.6159, grad_fn=&lt;NllLossBackward&gt;) epoch: 32 loss: tensor(0.5910, grad_fn=&lt;NllLossBackward&gt;) epoch: 33 loss: tensor(0.6162, grad_fn=&lt;NllLossBackward&gt;) epoch: 34 loss: tensor(0.6147, grad_fn=&lt;NllLossBackward&gt;) epoch: 35 loss: tensor(0.5909, grad_fn=&lt;NllLossBackward&gt;) epoch: 36 loss: tensor(0.5907, grad_fn=&lt;NllLossBackward&gt;) epoch: 37 loss: tensor(0.5930, grad_fn=&lt;NllLossBackward&gt;) epoch: 38 loss: tensor(0.5959, grad_fn=&lt;NllLossBackward&gt;) epoch: 39 loss: tensor(0.5757, grad_fn=&lt;NllLossBackward&gt;) epoch: 40 loss: tensor(0.5930, grad_fn=&lt;NllLossBackward&gt;) epoch: 41 loss: tensor(0.5685, grad_fn=&lt;NllLossBackward&gt;) epoch: 42 loss: tensor(0.5821, grad_fn=&lt;NllLossBackward&gt;) epoch: 43 loss: tensor(0.6053, grad_fn=&lt;NllLossBackward&gt;) epoch: 44 loss: tensor(0.5669, grad_fn=&lt;NllLossBackward&gt;) epoch: 45 loss: tensor(0.5845, grad_fn=&lt;NllLossBackward&gt;) epoch: 46 loss: tensor(0.5538, grad_fn=&lt;NllLossBackward&gt;) epoch: 47 loss: tensor(0.5400, grad_fn=&lt;NllLossBackward&gt;) epoch: 48 loss: tensor(0.5182, grad_fn=&lt;NllLossBackward&gt;) epoch: 49 loss: tensor(0.5513, grad_fn=&lt;NllLossBackward&gt;) epoch: 50 loss: tensor(0.5242, grad_fn=&lt;NllLossBackward&gt;) epoch: 51 loss: tensor(0.4909, grad_fn=&lt;NllLossBackward&gt;) epoch: 52 loss: tensor(0.5504, grad_fn=&lt;NllLossBackward&gt;) epoch: 53 loss: tensor(0.4952, grad_fn=&lt;NllLossBackward&gt;) epoch: 54 loss: tensor(0.4822, grad_fn=&lt;NllLossBackward&gt;) epoch: 55 loss: tensor(0.4417, grad_fn=&lt;NllLossBackward&gt;) epoch: 56 loss: tensor(0.5019, grad_fn=&lt;NllLossBackward&gt;) epoch: 57 loss: tensor(0.4571, grad_fn=&lt;NllLossBackward&gt;) epoch: 58 loss: tensor(0.3999, grad_fn=&lt;NllLossBackward&gt;) epoch: 59 loss: tensor(0.3907, grad_fn=&lt;NllLossBackward&gt;) epoch: 60 loss: tensor(0.4505, grad_fn=&lt;NllLossBackward&gt;) epoch: 61 loss: tensor(0.4170, grad_fn=&lt;NllLossBackward&gt;) epoch: 62 loss: tensor(0.4603, grad_fn=&lt;NllLossBackward&gt;) epoch: 63 loss: tensor(0.4129, grad_fn=&lt;NllLossBackward&gt;) epoch: 64 loss: tensor(0.3935, grad_fn=&lt;NllLossBackward&gt;) epoch: 65 loss: tensor(0.4071, grad_fn=&lt;NllLossBackward&gt;) epoch: 66 loss: tensor(0.4035, grad_fn=&lt;NllLossBackward&gt;) epoch: 67 loss: tensor(0.4109, grad_fn=&lt;NllLossBackward&gt;) epoch: 68 loss: tensor(0.3959, grad_fn=&lt;NllLossBackward&gt;) epoch: 69 loss: tensor(0.3583, grad_fn=&lt;NllLossBackward&gt;) epoch: 70 loss: tensor(0.3833, grad_fn=&lt;NllLossBackward&gt;) epoch: 71 loss: tensor(0.3846, grad_fn=&lt;NllLossBackward&gt;) epoch: 72 loss: tensor(0.3771, grad_fn=&lt;NllLossBackward&gt;) epoch: 73 loss: tensor(0.3460, grad_fn=&lt;NllLossBackward&gt;) epoch: 74 loss: tensor(0.3379, grad_fn=&lt;NllLossBackward&gt;) epoch: 75 loss: tensor(0.3971, grad_fn=&lt;NllLossBackward&gt;) epoch: 76 loss: tensor(0.3067, grad_fn=&lt;NllLossBackward&gt;) epoch: 77 loss: tensor(0.3618, grad_fn=&lt;NllLossBackward&gt;) epoch: 78 loss: tensor(0.4085, grad_fn=&lt;NllLossBackward&gt;) epoch: 79 loss: tensor(0.3308, grad_fn=&lt;NllLossBackward&gt;) epoch: 80 loss: tensor(0.3449, grad_fn=&lt;NllLossBackward&gt;) epoch: 81 loss: tensor(0.3873, grad_fn=&lt;NllLossBackward&gt;) epoch: 82 loss: tensor(0.3514, grad_fn=&lt;NllLossBackward&gt;) epoch: 83 loss: tensor(0.3111, grad_fn=&lt;NllLossBackward&gt;) epoch: 84 loss: tensor(0.3332, grad_fn=&lt;NllLossBackward&gt;) epoch: 85 loss: tensor(0.2431, grad_fn=&lt;NllLossBackward&gt;) epoch: 86 loss: tensor(0.2647, grad_fn=&lt;NllLossBackward&gt;) epoch: 87 loss: tensor(0.3185, grad_fn=&lt;NllLossBackward&gt;) epoch: 88 loss: tensor(0.3591, grad_fn=&lt;NllLossBackward&gt;) epoch: 89 loss: tensor(0.2908, grad_fn=&lt;NllLossBackward&gt;) epoch: 90 loss: tensor(0.2668, grad_fn=&lt;NllLossBackward&gt;) epoch: 91 loss: tensor(0.2537, grad_fn=&lt;NllLossBackward&gt;) epoch: 92 loss: tensor(0.2574, grad_fn=&lt;NllLossBackward&gt;) epoch: 93 loss: tensor(0.2624, grad_fn=&lt;NllLossBackward&gt;) epoch: 94 loss: tensor(0.2704, grad_fn=&lt;NllLossBackward&gt;) epoch: 95 loss: tensor(0.2548, grad_fn=&lt;NllLossBackward&gt;) epoch: 96 loss: tensor(0.2900, grad_fn=&lt;NllLossBackward&gt;) epoch: 97 loss: tensor(0.2432, grad_fn=&lt;NllLossBackward&gt;) epoch: 98 loss: tensor(0.2268, grad_fn=&lt;NllLossBackward&gt;) epoch: 99 loss: tensor(0.2178, grad_fn=&lt;NllLossBackward&gt;) . Let&#39;s generate a set of test data to test the model out. . mu, sigma = 1, 0.3 # mean and standard deviation num_data = 100 X_test = np.array(list(zip(np.random.normal(mu, sigma, num_data), np.random.normal(mu, sigma, num_data)))) y_test = Y = [get_class(x1) for (x1, x2) in X_test] test_dataloader = DataLoader(list(zip(X_test, y_test)), batch_size=64) correct = 0 total = 0 y_pred = [] x_test = torch.tensor([]) # using torch.no_grad() to tell pytorch we are not training the model so the gradient should not be counted with torch.no_grad(): for data in test_dataloader: X, y = data test_pred = test(X.float()) for idx, probs in enumerate(test_pred): if torch.argmax(probs) == y[idx]: correct += 1 total += 1 y_pred.append(torch.argmax(probs)) x_test = torch.cat((x_test, X.float()), 0) print(&#39;Accuracy: &#39;, correct / total) . Accuracy: 0.96 . The argmax in Pytorch simply return the greatest index from a tensor. . aaa = torch.tensor([1, 2, 3, 4, 5]) torch.argmax(aaa) . tensor(4) . import matplotlib.pyplot as plt # group 0 in red and group 1 in blue color = list(map(lambda y: &#39;r&#39; if y == 0 else &#39;b&#39;, y_pred)) # plotting the sample data plt.rcParams[&quot;figure.figsize&quot;] = (8, 8) plt.scatter(X_test[:,0], X_test[:,1], c=color) plt.xlim(0, 2) plt.ylim(0, 2) plt.xlabel(&quot;X1&quot;) plt.ylabel(&quot;X2&quot;) plt.show() . And the tutorial of Pytroch is simply put these process into functions to make the process more flexable and reusable. . import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor, Lambda training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) . train_dataloader = DataLoader(training_data, batch_size=64) test_dataloader = DataLoader(test_data, batch_size=64) class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() # flatten is just a function to make n*n matrix to vector (see below) self.flatten = nn.Flatten() # instead of puting the flow process in forward, nn.Sequential can also help to arrange the data flow self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), nn.ReLU() ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork() . # 1(number of image), 1(channel, i.e. rgb), 5 (width), 5(height) a = torch.randn(1, 1, 5, 5) a_flatten = nn.Flatten() print(&#39;before flatten:&#39;) print(a) print(&#39;size of tensor:&#39;, a.size()) print(&#39;after flatten:&#39;) print(a_flatten(a)) print(&#39;size of tensor:&#39;, a_flatten(a).size()) . before flatten: tensor([[[[-0.6237, 1.1621, 0.7894, 0.3365, 0.2160], [-1.5031, -0.1433, -0.2100, -0.1838, 1.8247], [-1.2745, 1.2917, -0.3287, 0.4327, -1.8417], [-1.3246, 0.1096, -0.4065, 0.6527, -0.7675], [-2.3109, -1.2873, -0.5552, 1.6039, -1.3372]]]]) size of tensor: torch.Size([1, 1, 5, 5]) after flatten: tensor([[-0.6237, 1.1621, 0.7894, 0.3365, 0.2160, -1.5031, -0.1433, -0.2100, -0.1838, 1.8247, -1.2745, 1.2917, -0.3287, 0.4327, -1.8417, -1.3246, 0.1096, -0.4065, 0.6527, -0.7675, -2.3109, -1.2873, -0.5552, 1.6039, -1.3372]]) size of tensor: torch.Size([1, 25]) . learning_rate = 1e-3 batch_size = 64 epochs = 5 # Initialize the loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) def train_loop(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) for batch, (X, y) in enumerate(dataloader): # Compute prediction and loss pred = model(X) loss = loss_fn(pred, y) # Backpropagation optimizer.zero_grad() loss.backward() optimizer.step() if batch % 100 == 0: loss, current = loss.item(), batch * len(X) print(f&quot;loss: {loss:&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]&quot;) def test_loop(dataloader, model, loss_fn): size = len(dataloader.dataset) test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= size correct /= size print(f&quot;Test Error: n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} n&quot;) . loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) epochs = 10 for t in range(epochs): print(f&quot;Epoch {t+1} n-&quot;) train_loop(train_dataloader, model, loss_fn, optimizer) test_loop(test_dataloader, model, loss_fn) print(&quot;Done!&quot;) . Epoch 1 - loss: 2.306720 [ 0/60000] loss: 2.301634 [ 6400/60000] loss: 2.294614 [12800/60000] loss: 2.296652 [19200/60000] loss: 2.290558 [25600/60000] loss: 2.269639 [32000/60000] loss: 2.270712 [38400/60000] loss: 2.251582 [44800/60000] loss: 2.261595 [51200/60000] loss: 2.255388 [57600/60000] Test Error: Accuracy: 36.0%, Avg loss: 0.035457 Epoch 2 - loss: 2.247549 [ 0/60000] loss: 2.253090 [ 6400/60000] loss: 2.231417 [12800/60000] loss: 2.253865 [19200/60000] loss: 2.242770 [25600/60000] loss: 2.195854 [32000/60000] loss: 2.199622 [38400/60000] loss: 2.166881 [44800/60000] loss: 2.198376 [51200/60000] loss: 2.172091 [57600/60000] Test Error: Accuracy: 36.6%, Avg loss: 0.034347 Epoch 3 - loss: 2.162021 [ 0/60000] loss: 2.174610 [ 6400/60000] loss: 2.130791 [12800/60000] loss: 2.186779 [19200/60000] loss: 2.153177 [25600/60000] loss: 2.078413 [32000/60000] loss: 2.087565 [38400/60000] loss: 2.031947 [44800/60000] loss: 2.106886 [51200/60000] loss: 2.044634 [57600/60000] Test Error: Accuracy: 36.5%, Avg loss: 0.032701 Epoch 4 - loss: 2.037334 [ 0/60000] loss: 2.062602 [ 6400/60000] loss: 1.993937 [12800/60000] loss: 2.095409 [19200/60000] loss: 2.039101 [25600/60000] loss: 1.934680 [32000/60000] loss: 1.948391 [38400/60000] loss: 1.877739 [44800/60000] loss: 1.993188 [51200/60000] loss: 1.908290 [57600/60000] Test Error: Accuracy: 37.1%, Avg loss: 0.030941 Epoch 5 - loss: 1.893154 [ 0/60000] loss: 1.949244 [ 6400/60000] loss: 1.856164 [12800/60000] loss: 2.001300 [19200/60000] loss: 1.925787 [25600/60000] loss: 1.816379 [32000/60000] loss: 1.815959 [38400/60000] loss: 1.750075 [44800/60000] loss: 1.867669 [51200/60000] loss: 1.760976 [57600/60000] Test Error: Accuracy: 37.8%, Avg loss: 0.029036 Epoch 6 - loss: 1.750824 [ 0/60000] loss: 1.845483 [ 6400/60000] loss: 1.714886 [12800/60000] loss: 1.880751 [19200/60000] loss: 1.819644 [25600/60000] loss: 1.625673 [32000/60000] loss: 1.726092 [38400/60000] loss: 1.621866 [44800/60000] loss: 1.717177 [51200/60000] loss: 1.597406 [57600/60000] Test Error: Accuracy: 39.2%, Avg loss: 0.027276 Epoch 7 - loss: 1.628418 [ 0/60000] loss: 1.759441 [ 6400/60000] loss: 1.598869 [12800/60000] loss: 1.800857 [19200/60000] loss: 1.736522 [25600/60000] loss: 1.525792 [32000/60000] loss: 1.656990 [38400/60000] loss: 1.541648 [44800/60000] loss: 1.631294 [51200/60000] loss: 1.519829 [57600/60000] Test Error: Accuracy: 40.2%, Avg loss: 0.026182 Epoch 8 - loss: 1.541113 [ 0/60000] loss: 1.693837 [ 6400/60000] loss: 1.519067 [12800/60000] loss: 1.751086 [19200/60000] loss: 1.671645 [25600/60000] loss: 1.465767 [32000/60000] loss: 1.607249 [38400/60000] loss: 1.485267 [44800/60000] loss: 1.572585 [51200/60000] loss: 1.465806 [57600/60000] Test Error: Accuracy: 41.2%, Avg loss: 0.025364 Epoch 9 - loss: 1.479244 [ 0/60000] loss: 1.640915 [ 6400/60000] loss: 1.459539 [12800/60000] loss: 1.709971 [19200/60000] loss: 1.622573 [25600/60000] loss: 1.420780 [32000/60000] loss: 1.569286 [38400/60000] loss: 1.439601 [44800/60000] loss: 1.527301 [51200/60000] loss: 1.422758 [57600/60000] Test Error: Accuracy: 42.4%, Avg loss: 0.024681 Epoch 10 - loss: 1.430659 [ 0/60000] loss: 1.596116 [ 6400/60000] loss: 1.409850 [12800/60000] loss: 1.670957 [19200/60000] loss: 1.581994 [25600/60000] loss: 1.382255 [32000/60000] loss: 1.538056 [38400/60000] loss: 1.399810 [44800/60000] loss: 1.488065 [51200/60000] loss: 1.387307 [57600/60000] Test Error: Accuracy: 43.4%, Avg loss: 0.024078 Done! .",
            "url": "https://tikuischan.github.io/fastpages_blog/deeplearning/neuronnetwork/pytorch/2021/05/29/Neuron-Network-Basic.html",
            "relUrl": "/deeplearning/neuronnetwork/pytorch/2021/05/29/Neuron-Network-Basic.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Building Simple Deep Learning Model using PyTorch and Fastai",
            "content": "All codes are base on the fastai course Deep Learning for Coders Chapter 2. Some changes were made to ensure everything works in colab environment. For other enverinment (as well as some detail description of function), possibility you would like to refer to the original notebook. . Prepare environment . ** Accelerate using GPU: Training of Deep Learning Model can be hugely accelerated by GPU (and that is the main reason why we choice colab instead of running jupyter notebook on our own pc). To start a session with GPU in colab, go to Runtime &gt; Change runtime type and select to use GPU. . fastai is a python library that patch Pytorch to make model training easier, somehow similar to keras and tensorflow. Although Pytorch is pre-installed in colab environment, fastai is not. We can use pip to install the library, colab require user to login and copy a token for the installation. . (the &quot;!&quot; in jupyter notebook / colab env indicate that is not a python code and should run in the shell) . !pip install -Uqq fastbook # default setup step import fastbook fastbook.setup_book() from fastbook import * from fastai.vision.widgets import * . |████████████████████████████████| 727kB 7.6MB/s |████████████████████████████████| 51kB 6.1MB/s |████████████████████████████████| 204kB 15.1MB/s |████████████████████████████████| 1.2MB 21.4MB/s |████████████████████████████████| 61kB 8.6MB/s |████████████████████████████████| 51kB 7.4MB/s Mounted at /content/gdrive . Data Collection (duckduckgo) . Instead of using Bing API to download images, fastai also provide another method using duckduckgo which does not require register. But somehow we have to remove the &quot;.decode()&quot; in the pip version of &quot;search_images_ddg&quot; in order to make it work... . That&#39;s why we define the function again. . def search_images_ddg(term, max_images=200): &quot;Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images&quot; assert max_images&lt;1000 url = &#39;https://duckduckgo.com/&#39; res = urlread(url,data={&#39;q&#39;:term}) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;, res) assert searchObj requestUrl = url + &#39;i.js&#39; params = dict(l=&#39;us-en&#39;, o=&#39;json&#39;, q=term, vqd=searchObj.group(1), f=&#39;,,,&#39;, p=&#39;1&#39;, v7exp=&#39;a&#39;) urls,data = set(),{&#39;next&#39;:1} while len(urls)&lt;max_images and &#39;next&#39; in data: try: data = urljson(requestUrl,data=params) urls.update(L(data[&#39;results&#39;]).itemgot(&#39;image&#39;)) requestUrl = url + data[&#39;next&#39;] except (URLError,HTTPError): pass time.sleep(0.2) return L(urls) . The function will return a list of urls. . keyword_to_search = &#39;grizzly bear&#39; # max_images control the maximum number of urls return =&gt; the length of the list urls = search_images_ddg(keyword_to_search, max_images=100) len(urls),urls[0] . (100, &#39;http://2.bp.blogspot.com/-9JAlJayP2XU/UQN3Wffpa8I/AAAAAAAAFbs/eMP5BcuLetc/w1200-h630-p-k-nu/Grizzly+Bear-2013-0pic-02.jpg&#39;) . Use the &quot;download_url&quot; function to download the images to desired folder. (Here we create a &quot;images&quot; folder, and set the downloaded image name as &quot;bear.jpg&quot;) . To show the image, simply use the &quot;open&quot; function of the &quot;Image&quot; class to create an instance of the image. . os.mkdir(&#39;images&#39;) dest = &#39;images/bear.jpg&#39; download_url(urls[0], dest) im = Image.open(dest) im.thumbnail((256,256)) im . Combine the above functions (with some functions in python standard library), we can generate our first data set. . class_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; . # type -- grizzly # |- black # |- teddy path = Path(&#39;type&#39;) if not path.exists(): path.mkdir() for o in class_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_ddg(o, max_images=150) download_images(dest, urls=results) . fastai&#39;s vision library contain tools that help us to find and remove corrupted image file: . fns = get_image_files(path) failed = verify_images(fns) failed.map(Path.unlink) . (#14) [None,None,None,None,None,None,None,None,None,None...] . Training . To train a model, we have to feed it with data (as well as telling it what data is that, i.e. supervised learning). In Pytorch we pass data to a model by generating a DataLoader object, fastai has a DataBlock class (a factory method, in short, a class that create some functions / class with certain design) to help us with it. . data_block = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) # give the data_block the path of our data to generate DataLoaders data_loaders = data_block.dataloaders(path) # create model using resnet with 18 layers learn = cnn_learner(data_loaders, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.168376 | 0.390310 | 0.102041 | 00:19 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . epoch train_loss valid_loss error_rate time . 0 | 0.153412 | 0.079060 | 0.020408 | 00:18 | . 1 | 0.100322 | 0.038834 | 0.010204 | 00:18 | . 2 | 0.072304 | 0.032311 | 0.010204 | 00:18 | . 3 | 0.059285 | 0.041482 | 0.020408 | 00:18 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . Now we can plot the confustion matrix to visialize how our newly trained model behave. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . To test the trained model, we could try to pass a new image (we don&#39;t need to create a DataLoader for single prediction process) . predict_img_path = &#39;&#39; learn.predict(predict_img_path) . Exporting the trained model . Once the model is ready, we can export the architecture and parameters to a python pickle file. . learn.export() . And the model can be loaded by load_learner function of fastai library . learn_inf = load_learner(&#39;export.pkl&#39;) . # you can also upload to google drive, change the image path and comment out the search_images_ddg # and download_url functions urls = search_images_ddg(&#39;teddy bear&#39;, max_images=1) test_img = &#39;images/test_img.jpg&#39; download_url(urls[0], test_img) im = Image.open(test_img) im.thumbnail((256,256)) im . learn_inf.predict(test_img) . (&#39;teddy&#39;, tensor(2), tensor([5.3844e-06, 5.3117e-06, 9.9999e-01])) . And seems the model is doing its thing :) .",
            "url": "https://tikuischan.github.io/fastpages_blog/deeplearning/pytorch/fastai/2021/05/19/Building-Simple-Deep-Learning-Model.html",
            "relUrl": "/deeplearning/pytorch/fastai/2021/05/19/Building-Simple-Deep-Learning-Model.html",
            "date": " • May 19, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Maniplating Dictionaries",
            "content": "While working on data extraction and cleansing, oftenly we play with dictionary form data. And there are different ways to handle according to different needs. In this post I will cover several use case I met. . Simple dictionary: . data = { &#39;a&#39;: 1, &#39;b&#39;: 1, &#39;c&#39;: 1, &#39;d&#39;: 1, &#39;e&#39;: 1, &#39;f&#39;: 3, &#39;g&#39;: 1, &#39;h&#39;: 1, &#39;i&#39;: 1, &#39;j&#39;: 1, &#39;k&#39;: 1, } . Delete item with conditions: . If we need to screen through every values in a dictionary, and drop according to some conditions, we can use a for loop with a copy of the dictionary (we cannot drop item while running through the loop!). . for k, v in data.items(): if k == &#39;e&#39;: data.pop(k) . RuntimeError Traceback (most recent call last) &lt;ipython-input-6-387841d80c9f&gt; in &lt;module&gt; -&gt; 1 for k, v in data.items(): 2 if k == &#39;e&#39;: 3 data.pop(k) RuntimeError: dictionary changed size during iteration . from copy import copy loop_data = copy(data) for k, v in loop_data.items(): if v == 3: data.pop(k) . data . {&#39;a&#39;: 1, &#39;b&#39;: 1, &#39;c&#39;: 1, &#39;d&#39;: 1, &#39;e&#39;: 1, &#39;g&#39;: 1, &#39;h&#39;: 1, &#39;i&#39;: 1, &#39;j&#39;: 1, &#39;k&#39;: 1} . So now consider we have a list of dictionaries with different contents, and we know that one of the field have to be dropped before passing to the next function. (For example, next function will batch upload data to database, and we are going to write a data cleansing function to screen out useless information.) . sub_data = {&#39;a&#39;: 1, &#39;delete_field&#39;: 2, &#39;c&#39;: 3} data = [sub_data for _ in range(4)] print(&#39;before&#39;) print(data) for item in data: if &#39;delete_field&#39; in item: item.pop(&#39;delete_field&#39;) print(&#39;after&#39;) print(data) . before [{&#39;a&#39;: 1, &#39;delete_field&#39;: 2, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: 2, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: 2, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: 2, &#39;c&#39;: 3}] after [{&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}] . Another possibility is that only remove field if it is &#39;None&#39;. . sub_data = {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3} data = [sub_data for _ in range(4)] print(&#39;before&#39;) print(data) # now we have to loop through the dict to check each value # use .items() to expend key and value, and use if v is None to check the value # if we use &quot;if v:&quot; then field with empty string and 0 will also be removed (both 0 and &#39;&#39; are considered as &#39;not v&#39;) for item in data: for k, v in copy(item).items(): if v is None: item.pop(k) print(&#39;after&#39;) print(data) . before [{&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3}] after [{&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}] . We can also try to create a clean data from a messy one. . sub_data = {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3, &#39;d&#39;: 6, &#39;e&#39;: None, &#39;f&#39;: 3} data = [sub_data for _ in range(4)] + [{&#39;e&#39;: &#39;a&#39;}] + [{&#39;c&#39;: 0}] print(&#39;before&#39;) print(data) # if we need &#39;a&#39;, &#39;c&#39; and &#39;e&#39; only # also we don&#39;t want field with value &#39;None&#39; output = [] for item in data: sub_output = {} for k in [&#39;a&#39;, &#39;c&#39;, &#39;e&#39;]: if item.get(k) is not None: # we have to put is not None here otherwise {&#39;c&#39;: 0} will be dropped sub_output[k] = item[k] if sub_output: output.append(sub_output) print(&#39;after&#39;) print(output) . before [{&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3, &#39;d&#39;: 6, &#39;e&#39;: None, &#39;f&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3, &#39;d&#39;: 6, &#39;e&#39;: None, &#39;f&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3, &#39;d&#39;: 6, &#39;e&#39;: None, &#39;f&#39;: 3}, {&#39;a&#39;: 1, &#39;delete_field&#39;: None, &#39;c&#39;: 3, &#39;d&#39;: 6, &#39;e&#39;: None, &#39;f&#39;: 3}, {&#39;e&#39;: &#39;a&#39;}, {&#39;c&#39;: 0}] after [{&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;a&#39;: 1, &#39;c&#39;: 3}, {&#39;e&#39;: &#39;a&#39;}, {&#39;c&#39;: 0}] .",
            "url": "https://tikuischan.github.io/fastpages_blog/2021/05/07/Manipulating-Dictionaries.html",
            "relUrl": "/2021/05/07/Manipulating-Dictionaries.html",
            "date": " • May 7, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Saving data to excel",
            "content": "Perviously (Data Extraction with Python part 2 and part 3) we&#39;ve learnt how to extract data from excel files. Sometimes it is easier for our client (especially when they are not familiar to programming) to check the data if we provide it in excel form. The simpliest way to do it is using DataFrame().to_excel function in pandas. . import pandas as pd sample_data = {&#39;col_1&#39;: [1, 2, 3], &#39;col_2&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], &#39;col_3&#39;: [4, 5, 6]} sample_df = pd.DataFrame(sample_data) sample_df . col_1 col_2 col_3 . 0 1 | a | 4 | . 1 2 | b | 5 | . 2 3 | c | 6 | . sample_df.to_excel(&#39;sample_df.xlsx&#39;) # we can also put path-like format to save the excel file in specific folder sample_df.to_excel(&#39;home/target_folder/sample_df.xlsx&#39;) . In pratice sometimes things get a bit more complicated. For example there are mulitple tables we need to put in a single excel, or for whatever reason we need to export data in batch, and we want to put all data in a single file, then we need a bit extra work. . For assigning sheet name, we can simply use the parameter sheet_name. . sample_df.to_excel(&#39;sample_df.xlsx&#39;, sheet_name=&#39;sample_df&#39;) sample_df.to_excel(&#39;sample_df.xlsx&#39;, sheet_name=&#39;sample_df2&#39;) . But as we can see in the output file, there is only one sheet named &quot;sample_df2&quot; in the excel file, pandas will create a new file and remove the file with the same name in this case. To tell pandas to write both sheets in the same file, we can use a ExcelWriter object to lock on a single file, and pass to to_excel to write on the same file. . writer = pd.ExcelWriter(&#39;sample_df.xlsx&#39;) sample_df.to_excel(writer, sheet_name=&#39;sample_df&#39;) sample_df.to_excel(writer, sheet_name=&#39;sample_df2&#39;) writer.save() . If the file is already exist, we can add &quot;mode=&#39;a&#39;&quot; to avoid losing all the existing data. . writer = pd.ExcelWriter(&#39;sample_df.xlsx&#39;, mode=&#39;a&#39;) sample_df.to_excel(writer, sheet_name=&#39;sample_df3&#39;) sample_df.to_excel(writer, sheet_name=&#39;sample_df4&#39;) writer.save() . What if we have multiple tables with the same columns and want to concatenate them into same file? There are two ways to do that, the simple way is to combine the tables before write to excel; and the other is to get the maximum row number of the sheet using load_workbook of openpyxl, and feed to &quot;start_row&quot; of &quot;to_excel&quot;. . from openpyxl import load_workbook table = &#39;sample_df&#39; writer = pd.ExcelWriter(&#39;sample_df.xlsx&#39;, mode=&#39;a&#39;) startrow = 0 # try to open an existing workbook writer.book = load_workbook(&#39;sample_df.xlsx&#39;) # get the last row in the existing Excel sheet if startrow == 0 and table in writer.book.sheetnames: # the last row with data is recorded in max_row startrow = writer.book[table].max_row print(f&#39;the last row in {table} is {startrow}&#39;) writer.sheets = {ws.title: ws for ws in writer.book.worksheets} sample_df.to_excel(writer, sheet_name=table, startrow=startrow) writer.save() pd.read_excel(&#39;sample_df.xlsx&#39;) . the last row in sample_df is 4 . Unnamed: 0 col_1 col_2 col_3 . 0 0.0 | 1 | a | 4 | . 1 1.0 | 2 | b | 5 | . 2 2.0 | 3 | c | 6 | . 3 NaN | col_1 | col_2 | col_3 | . 4 0.0 | 1 | a | 4 | . 5 1.0 | 2 | b | 5 | . 6 2.0 | 3 | c | 6 | . Now we have successfully append data after the last row, the only problem is the script above will also include the column name as a row and get rid of the meaningless &quot;Unnamed: 0&quot; column. we can simply use the &quot;header&quot; and &quot;index&quot; parameters to achieve them. . table = &#39;sample_df&#39; writer = pd.ExcelWriter(&#39;sample_df.xlsx&#39;, mode=&#39;a&#39;) startrow = 0 header = True # try to open an existing workbook writer.book = load_workbook(&#39;sample_df.xlsx&#39;) # get the last row in the existing Excel sheet if startrow == 0 and table in writer.book.sheetnames: # the last row with data is recorded in max_row startrow = writer.book[table].max_row print(f&#39;the last row in {table} is {startrow}&#39;) if startrow &gt; 0: header = False # copy existing sheets writer.sheets = {ws.title: ws for ws in writer.book.worksheets} sample_df.to_excel(writer, sheet_name=table, startrow=startrow, header=header) writer.save() pd.read_excel(&#39;sample_df.xlsx&#39;) . the last row in sample_df is 8 . Unnamed: 0 col_1 col_2 col_3 . 0 0.0 | 1 | a | 4 | . 1 1.0 | 2 | b | 5 | . 2 2.0 | 3 | c | 6 | . 3 NaN | col_1 | col_2 | col_3 | . 4 0.0 | 1 | a | 4 | . 5 1.0 | 2 | b | 5 | . 6 2.0 | 3 | c | 6 | . 7 0.0 | 1 | a | 4 | . 8 1.0 | 2 | b | 5 | . 9 2.0 | 3 | c | 6 | . sample_df.to_excel(&#39;sample_df2.xlsx&#39;, sheet_name=&#39;sample_df&#39;, index=False) pd.read_excel(&#39;sample_df2.xlsx&#39;) . col_1 col_2 col_3 . 0 1 | a | 4 | . 1 2 | b | 5 | . 2 3 | c | 6 | . Combine everything together, we can write a simple function like that: . import os # assume the data are in dict form: {table_name1: DataFrame1, ...} def export2excel(dict_form_data, file_path): if os.path.exists(file_path): load_wb = True writer = pd.ExcelWriter(file_path, mode=&#39;a&#39;) else: load_wb = False writer = pd.ExcelWriter(file_path) # for loop in dict loop through the keys for table in dict_form_data: startrow = 0 header = True if load_wb: try: # try to open an existing workbook writer.book = load_workbook(file_path) # get the last row in the existing Excel sheet if startrow == 0 and table in writer.book.sheetnames: startrow = writer.book[table].max_row writer.sheets = {ws.title: ws for ws in writer.book.worksheets} except FileNotFoundError: pass if startrow &gt; 0: header = False dict_form_data[table].to_excel(writer, sheet_name=table, startrow=startrow, index=False, header=header) writer.save() print(f&#39;finish exploting to {file_path}&#39;) return . The function will create a new file if &quot;file_path&quot; not exist. . data = {&#39;sample_sheet_1&#39;: sample_df, &#39;sample_sheet_2&#39;: sample_df, &#39;sample_sheet_3&#39;: sample_df} export2excel(data, &#39;final_sample.xlsx&#39;) . finish exploting to final_sample.xlsx . Run again and now we can see the function actually append data to corresponding sheets. . export2excel(data, &#39;final_sample.xlsx&#39;) . finish exploting to final_sample.xlsx . For a more complete function, here is a reference from stack overflow .",
            "url": "https://tikuischan.github.io/fastpages_blog/python/pandas/openpyxl/excel/2021/04/25/Saving-Data-to-Excel.html",
            "relUrl": "/python/pandas/openpyxl/excel/2021/04/25/Saving-Data-to-Excel.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Data Extraction with Python (part 3)",
            "content": "Last time we try to extract data from excel files using pandas read_excel function. In practice, sometimes the excel files also include pictures, then read_excel might not be a good choice for it. In this post we are going to extract some photos from excel using openpyxl library. . openpyxl is a python external library (i.e. we have to install it) specific for reading and writing excel files. Personally I prefer to use pandas, but sometimes pandas lack the flexibiliy to finish very specific task (that we will cover a few today). . #hide_output # for some unknown reason this method does not work without xlrd installed !pip install openpyxl !pip install openpyxl-image-loader !pip install xlrd . . In this blog post, we will try to parse a sample excel file with pictures. (Acutally this method is quite restricted, the images have to be anchored to a specific cell. For google spreadsheet, use incert image in a cell; for open office calc, we can simply right click the image and choose anchor). The sample excel file was a hero profile table. . To load a xlsx file using openpyxl library, simply use the load_workbook. Unlike read_excel in pandas, which gives back a DataFrame object that can be directly shown in jupyter notebook, openpyxl gives back an object that cannot be shown in jupyter notebook directly. . from openpyxl import load_workbook file_path = &#39;../images/excel_sample2.xlsx&#39; xlsx = load_workbook(file_path) xlsx . &lt;openpyxl.workbook.workbook.Workbook at 0x7fdd3ba7f550&gt; . We can use &#39;__dir__()&#39; (or __dict__) method to see what attribute / options do we have, also reading document could help. . xlsx.__dir__() . [&#39;_sheets&#39;, &#39;_pivots&#39;, &#39;_active_sheet_index&#39;, &#39;defined_names&#39;, &#39;_external_links&#39;, &#39;properties&#39;, &#39;security&#39;, &#39;_Workbook__write_only&#39;, &#39;shared_strings&#39;, &#39;_fonts&#39;, &#39;_alignments&#39;, &#39;_borders&#39;, &#39;_fills&#39;, &#39;_number_formats&#39;, &#39;_date_formats&#39;, &#39;_timedelta_formats&#39;, &#39;_protections&#39;, &#39;_colors&#39;, &#39;_cell_styles&#39;, &#39;_named_styles&#39;, &#39;_table_styles&#39;, &#39;_differential_styles&#39;, &#39;loaded_theme&#39;, &#39;vba_archive&#39;, &#39;is_template&#39;, &#39;code_name&#39;, &#39;_epoch&#39;, &#39;encoding&#39;, &#39;iso_dates&#39;, &#39;rels&#39;, &#39;calculation&#39;, &#39;views&#39;, &#39;_data_only&#39;, &#39;_read_only&#39;, &#39;template&#39;, &#39;__module__&#39;, &#39;__doc__&#39;, &#39;path&#39;, &#39;__init__&#39;, &#39;_setup_styles&#39;, &#39;epoch&#39;, &#39;read_only&#39;, &#39;data_only&#39;, &#39;write_only&#39;, &#39;excel_base_date&#39;, &#39;active&#39;, &#39;create_sheet&#39;, &#39;_add_sheet&#39;, &#39;move_sheet&#39;, &#39;remove&#39;, &#39;remove_sheet&#39;, &#39;create_chartsheet&#39;, &#39;get_sheet_by_name&#39;, &#39;__contains__&#39;, &#39;index&#39;, &#39;get_index&#39;, &#39;__getitem__&#39;, &#39;__delitem__&#39;, &#39;__iter__&#39;, &#39;get_sheet_names&#39;, &#39;worksheets&#39;, &#39;chartsheets&#39;, &#39;sheetnames&#39;, &#39;create_named_range&#39;, &#39;add_named_style&#39;, &#39;named_styles&#39;, &#39;get_named_ranges&#39;, &#39;add_named_range&#39;, &#39;get_named_range&#39;, &#39;remove_named_range&#39;, &#39;mime_type&#39;, &#39;save&#39;, &#39;style_names&#39;, &#39;copy_worksheet&#39;, &#39;close&#39;, &#39;_duplicate_name&#39;, &#39;__dict__&#39;, &#39;__weakref__&#39;, &#39;__repr__&#39;, &#39;__hash__&#39;, &#39;__str__&#39;, &#39;__getattribute__&#39;, &#39;__setattr__&#39;, &#39;__delattr__&#39;, &#39;__lt__&#39;, &#39;__le__&#39;, &#39;__eq__&#39;, &#39;__ne__&#39;, &#39;__gt__&#39;, &#39;__ge__&#39;, &#39;__new__&#39;, &#39;__reduce_ex__&#39;, &#39;__reduce__&#39;, &#39;__subclasshook__&#39;, &#39;__init_subclass__&#39;, &#39;__format__&#39;, &#39;__sizeof__&#39;, &#39;__dir__&#39;, &#39;__class__&#39;] . . The parameter me care most should be &#39;_sheets&#39; or &#39;worksheets&#39;, they return all the sheets (as an object) within the excel file in a list. . xlsx.worksheets . [&lt;Worksheet &#34;Sheet1&#34;&gt;] . In openpyxl object, we can directly call the Cell name like excel, but that is not our focus (as we can simple parse the strings by pandas) . sheet = xlsx[&#39;Sheet1&#39;] print(sheet[&#39;A4&#39;].value) print(sheet[&#39;B4&#39;].value) . batman Bruce Wayne . The only command we care about is the &quot;._images&quot; in a sheet object, which list out all the images within the sheet. . sheet._images . [&lt;openpyxl.drawing.image.Image at 0x7fdd3c8283d0&gt;, &lt;openpyxl.drawing.image.Image at 0x7fdd3c899110&gt;, &lt;openpyxl.drawing.image.Image at 0x7fdd3c8a3d10&gt;] . For cells location containing images, we can use the .anchor._from.col and .anchor._from.row in Image object, which is actually a AnchorMarker object used to locate the image, with col 0 = A, 1 = B etc. . Excel file is actually a zip file with several folder and xml files, which means if we can find the corresponding image path of a cell with image, we can relocate the image file. But openpyxl seems not like this idea too much... . sheet._images[0].anchor._from # col=2 and row=1 =&gt; cell &#39;C2&#39; . &lt;openpyxl.drawing.spreadsheet_drawing.AnchorMarker object&gt; Parameters: col=2, colOff=0, row=1, rowOff=0 . for img in sheet._images: print(&#39;col: &#39;, img.anchor._from.col + 1) print(&#39;row: &#39;, img.anchor._from.row + 1) print(img.path) . col: 3 row: 2 /xl/media/image1.png col: 3 row: 3 /xl/media/image1.png col: 3 row: 4 /xl/media/image1.png . openpyxl return all images with the same path! Somehow while generating the Image object, openpyxl forgot to update the object id and path. Luckily we can still get the image data from the image object by the ._data() method, for display in ipython notebook, I re-create an Image object using PIL. For actual project, we can use the .save() instead of display to save the image into desired location. . from IPython import display from PIL import Image base_path = &#39;../images&#39; for img in xlsx[&#39;Sheet1&#39;]._images: ip_img = display.Image(data=img._data(), format=&#39;png&#39;, embed=True) display.display(ip_img) # get anchor cell number print(&#39;col: &#39;, img.anchor._from.col) print(&#39;row: &#39;, img.anchor._from.row) . col: 2 row: 1 . col: 2 row: 2 . col: 2 row: 3 .",
            "url": "https://tikuischan.github.io/fastpages_blog/python/dataextraction/openpyxl/excel/2021/04/16/Data-Extraction-with-Python-Part3.html",
            "relUrl": "/python/dataextraction/openpyxl/excel/2021/04/16/Data-Extraction-with-Python-Part3.html",
            "date": " • Apr 16, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Data Extraction with Python (part 2)",
            "content": "Last time we talk about data extraction from html type files (or data extraction part of web-scraping), in real life projects sometimes we will meet data stored in excel format. Depending on the data, there are different ways to extract them. . Pandas . The easiest way to extract informations from an excel file should be using the build-in function pd.read_excel() from pandas. Pandas is an external library, we need to install it before use (by pip or conda or other methods). . import pandas as pd # if we call the function without the brackets, python will show some minimum information of the function pd.read_excel . &lt;function pandas.io.excel._base.read_excel(io, sheet_name=0, header=0, names=None, index_col=None, usecols=None, squeeze=False, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=None, thousands=None, comment=None, skipfooter=0, convert_float=True, mangle_dupe_cols=True, storage_options: Union[Dict[str, Any], NoneType] = None)&gt; . The read_excel function will return a DataFrame object, pretty much similar to a excel table. And as you can see in the above description or in the document, the read_excel function has quite some parameters. If we leave everything in default, pandas will grep the first row as the column name and try to convert the format it can recognize (float, datetime, for example, and it won&#39;t turn natural number into integers!). Here I will introduce several of them I think are useful. . In practice, clients always have inconsistencies in their data, maybe the file is corrupted, maybe someone did not follow the format straightly (e.g. someone may input XX-XX-2021 when they are not sure about the date, that will make pandas consider the whole column as string instead of datetime)...It is always a great idea to consider all columns as string, and convert them afterwards. To set all data type as string, we can set the dtype as &#39;object&#39; (there are 4 main types of data in pandas, float64, int64, datetime64 and object). . pd.read_excel(file, dtype=&#39;object&#39;) . Speaking of inconsistant format, there is one thing we might like to convert by pandas, which is the &#39;na&#39; values. In practice, maybe the excel was built by different with different input style, or other reason, it is not difficult to found &#39;NA&#39;, &#39;&#39;, &#39;None&#39; etc. in the same file. In pandas, we can input a list of &#39;na&#39; value to convert all of them. . By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’. . pd.read_excel(file, na_values=[&#39;na&#39;, &#39;&#39;, &#39;None&#39;]) . Sometime the excel file consist of several sheets, to parse through those sheet, pandas provide a parameter for that, which is sheet_name, setting it to None allow us to parse all sheets from the excel file output as a dictionary of DataFrames. It can also accept order (start from 0) or specific name or list of order and names. . pd.read_excel(file, sheet_name=None) . header and names: used together when the first row is not column name. We will set header to None, and input column names manually (list of string) using the names parameter. . pd.read_excel(file, header=None, names=[&#39;first column&#39;, &#39;second column&#39;, &#39;third column&#39;]) . So now we have our excel tables read as DataFrame object. Before we move on to do some calculation / format change / combine / separation of data, there are two simple data cleansing steps we might want to do. . For whatever reason, there is always possibility to have empty rows in between. By using df.dropna function in pandas, we can easily remove the empty rows. . df = pd.DataFrame([[&#39;column a&#39;, &#39;column b&#39;, &#39;column c&#39;], [], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]]) df . 0 1 2 . 0 column a | column b | column c | . 1 None | None | None | . 2 a | b | c | . df = df.dropna(axis=0, how=&#39;all&#39;) df . 0 1 2 . 0 column a | column b | column c | . 2 a | b | c | . # drop=True is used to prevent the old index becoming a new column df = pd.DataFrame([[&#39;column a&#39;, &#39;column b&#39;, &#39;column c&#39;], [], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]]) df = df.dropna(axis=0, how=&#39;all&#39;).reset_index(drop=True) df . 0 1 2 . 0 column a | column b | column c | . 1 a | b | c | . We can also use subset parameter and input a list of column names to tell pandas to drop rows if all of the subset columns are empty. . df = pd.DataFrame([[&#39;column a&#39;, &#39;column b&#39;, &#39;column c&#39;], [&#39;a&#39;, None, None], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]]) df = df.dropna(axis=0, subset=[0]) df . 0 1 2 . 0 column a | column b | column c | . 1 a | None | None | . 2 a | b | c | . df = pd.DataFrame([[&#39;column a&#39;, &#39;column b&#39;, &#39;column c&#39;], [&#39;a&#39;, None, None], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]]) df = df.dropna(axis=0, subset=[1, 2]) df . 0 1 2 . 0 column a | column b | column c | . 2 a | b | c | . The other useful tool in pandas is transpose, which reverse the column and row. . df = pd.DataFrame([[&#39;column a&#39;, &#39;column b&#39;, &#39;column c&#39;], [&#39;a&#39;, None, None], [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]]) df . 0 1 2 . 0 column a | column b | column c | . 1 a | None | None | . 2 a | b | c | . df_transpose = df.T df_transpose . 0 1 2 . 0 column a | a | a | . 1 column b | None | b | . 2 column c | None | c | .",
            "url": "https://tikuischan.github.io/fastpages_blog/python/dataextraction/pandas/excel/2021/04/04/Data-Extraction-with-Python-Part2.html",
            "relUrl": "/python/dataextraction/pandas/excel/2021/04/04/Data-Extraction-with-Python-Part2.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Data Extraction with Python",
            "content": "To start a data project, it is quite often we need to extract data from different kinds of sources (webpage, word, excel, database elsewhere...). In this blog (series?), I will try to cover several library and tracks I used. . Data in html format . The most common way to work with data in html format (i.e. webpage downloaded) should be BeautifulSoup. It is a powerful external library (i.e. it does not come with Python itself, we have to install it - usually by pip, or other method can be found in the documentation page). . To begin with, surely we have to import it, . from bs4 import BeautifulSoup # if you have a saved file, you can open the file by: with open(&#39;path_to_html_file&#39;, &#39;w&#39;) as f: soup = BeautifulSoup(f, &#39;html.parser&#39;) print(soup.prettify()) . or BeautifulSoup can also support text input. By using the prettify() function, BeautifulSoup will display the html in a easy for human read form (computer don&#39;t care at all!) . # as an example I left only content in side body tag html_sample = &#39;&#39;&#39; &lt;body class=&quot;review-template-default single single-review postid-7685 single-format-standard header-image content-sidebar genesis-breadcrumbs-hidden unknown-os ie11 feature-top-outside site-fluid override&quot;&gt;&lt;div class=&quot;site-container&quot;&gt;&lt;ul class=&quot;genesis-skip-link&quot;&gt;&lt;li&gt;&lt;a href=&quot;#genesis-nav-primary&quot; class=&quot;screen-reader-shortcut&quot;&gt; Skip to primary navigation&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#genesis-content&quot; class=&quot;screen-reader-shortcut&quot;&gt; Skip to main content&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;#genesis-sidebar-primary&quot; class=&quot;screen-reader-shortcut&quot;&gt; Skip to primary sidebar&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;header class=&quot;site-header&quot;&gt;&lt;div class=&quot;wrap&quot;&gt;&lt;div class=&quot;title-area&quot;&gt;&lt;p class=&quot;site-title&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/&quot;&gt;Coffee Review&lt;/a&gt;&lt;/p&gt;&lt;p class=&quot;site-description&quot;&gt;The World&amp;#039;s Leading Coffee Guide&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;widget-area header-widget-area&quot;&gt;&lt;section id=&quot;text-11&quot; class=&quot;widget widget_text&quot;&gt;&lt;div class=&quot;widget-wrap&quot;&gt;&lt;div class=&quot;textwidget&quot;&gt;&lt;form role=&quot;search&quot; id=&quot;searchform&quot; method=&quot;get&quot; action=&quot;https://www.coffeereview.com/&quot;&gt;&lt;div class=&quot;header_search_line_1&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;post_type&quot; id=&quot;cr_reviews&quot; value=&quot;review&quot; checked=&quot;checked&quot;&gt; &lt;label for=&quot;cr_reviews&quot;&gt;Reviews&lt;/label&gt; &lt;input type=&quot;radio&quot; name=&quot;post_type&quot; id=&quot;cr_tasting_reports&quot; value=&quot;post&quot;&gt; &lt;label for=&quot;cr_tasting_reports&quot;&gt;Tasting Reports&lt;/label&gt;&lt;/div&gt;&lt;div class=&quot;header_search_line_2&quot;&gt; &lt;input type=&quot;search&quot; value=&quot;&quot; placeholder=&quot;Enter search terms&quot; size=&quot;18&quot; maxlength=&quot;50&quot; name=&quot;s&quot; id=&quot;searchfield&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Search&quot; class=&quot;header_search_button&quot;&gt;&lt;/div&gt;&lt;div class=&quot;header_search_line_3&quot;&gt; &lt;a href=&quot;/advanced-search/&quot;&gt;Advanced Search&lt;/a&gt;&lt;/div&gt;&lt;/form&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;section id=&quot;text-12&quot; class=&quot;widget widget_text&quot;&gt;&lt;div class=&quot;widget-wrap&quot;&gt;&lt;div class=&quot;textwidget&quot;&gt;&lt;p&gt;&lt;a href=&quot;http://bit.ly/2r1silm&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img class=&quot;aligncenter size-full wp-image-19359&quot; src=&quot;https://dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/Hula-Daddy-Button-1st-place.jpg&quot; alt=&quot;Shop for Award-Winning 100% Kona Coffees at Hula Daddy&quot; width=&quot;195&quot; height=&quot;90&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/header&gt;&lt;div class=&quot;responsive-primary-menu-container&quot;&gt;&lt;h3 class=&quot;mobile-primary-toggle&quot;&gt;&lt;/h3&gt;&lt;div class=&quot;responsive-menu-icon&quot;&gt; &lt;span class=&quot;responsive-icon-bar&quot;&gt;&lt;/span&gt; &lt;span class=&quot;responsive-icon-bar&quot;&gt;&lt;/span&gt; &lt;span class=&quot;responsive-icon-bar&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;nav class=&quot;nav-primary&quot; aria-label=&quot;Main&quot; id=&quot;genesis-nav-primary&quot;&gt;&lt;div class=&quot;wrap&quot;&gt;&lt;ul id=&quot;menu-main&quot; class=&quot;menu genesis-nav-menu menu-primary js-superfish&quot;&gt;&lt;li id=&quot;menu-item-4998&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-4998&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/review/&quot;&gt;&lt;span &gt;Reviews&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-13553&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13553&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/review/&quot;&gt;&lt;span &gt;Latest Reviews&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13533&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-13533&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/highest-rated-coffees/&quot;&gt;&lt;span &gt;Top-Rated (94+)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18958&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-18958&quot;&gt;&lt;a href=&quot;https://coffeereview.com/types/espresso/&quot;&gt;&lt;span &gt;Espressos&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13534&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13534&quot;&gt;&lt;a href=&quot;https://coffeereview.com/types/best-value-coffees/&quot;&gt;&lt;span &gt;Best Values&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19057&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-19057&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/top-30-coffees-2019/&quot;&gt;&lt;span &gt;Top 30 Coffees of 2019&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19632&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-19632&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/types/coffees-from-taiwan/&quot;&gt;&lt;span &gt;Taiwan Coffees &amp;#8211; 台灣送評的咖啡豆&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19160&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-19160&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/top-30-coffees-past-rankings/&quot;&gt;&lt;span &gt;Top 30 &amp;#8211; Past Rankings&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13536&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13536&quot;&gt;&lt;a href=&quot;https://coffeereview.com/types/single-serve-capsule/&quot;&gt;&lt;span &gt;Pods and Capsules&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18959&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-18959&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/best-coffee-cities/&quot;&gt;&lt;span &gt;Reviews by U.S. City&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13857&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13857&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/advanced-search/&quot;&gt;&lt;span &gt;Advanced Search&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18960&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-18960&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/&quot;&gt;&lt;span &gt;Reports&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-15819&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-15819&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/&quot;&gt;&lt;span &gt;Latest Reports&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18961&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18961&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/africa/&quot;&gt;&lt;span &gt;Africa&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18962&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18962&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/americas/&quot;&gt;&lt;span &gt;Americas&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18964&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18964&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/asia-pacific-coffees/&quot;&gt;&lt;span &gt;Asia-Pacific&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18966&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18966&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/espressos/&quot;&gt;&lt;span &gt;Espressos&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18963&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18963&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/annual-top-30/&quot;&gt;&lt;span &gt;Annual Top 30&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18967&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18967&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/tasting-report-processing-method/&quot;&gt;&lt;span &gt;Processing Method&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18968&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18968&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/tasting-reports-social-environmental/&quot;&gt;&lt;span &gt;Social/Environmental&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18969&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18969&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/tasting-reports-tree-variety/&quot;&gt;&lt;span &gt;Tree Variety&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18965&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18965&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/articles/coffee-and-espresso-blends/&quot;&gt;&lt;span &gt;Blends&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-15781&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-15781&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/equipment-reports/&quot;&gt;&lt;span &gt;Equipment&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-19950&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-post menu-item-19950&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/burr-coffee-grinder-reviews/&quot;&gt;&lt;span &gt;Mid-Range Burr Coffee Grinders&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19622&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-post menu-item-19622&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/equipment-report-digital-electric-gooseneck-pourover-kettles/&quot;&gt;&lt;span &gt;Electric Gooseneck Kettles&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19586&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19586&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/interpreting-equipment-ratings/&quot;&gt;&lt;span &gt;Interpreting Equipment Ratings&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-12025&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-12025&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/blog/&quot;&gt;&lt;span &gt;Journal&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-19959&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-post menu-item-19959&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/recognizing-the-coffees-communities-and-contributions-of-black-owned-coffee-companies/&quot;&gt;&lt;span &gt;Recognizing Black-Owned Coffee Companies&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19650&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-19650&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/covid-19-info/&quot;&gt;&lt;span &gt;COVID-19 Information&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18977&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-18977&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/our-story/&quot;&gt;&lt;span &gt;About&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-18978&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-18978&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/our-story/&quot;&gt;&lt;span &gt;Our Story&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18971&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-18971&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/kennethdavids/&quot;&gt;&lt;span &gt;Kenneth Davids&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13913&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13913&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/our-team/&quot;&gt;&lt;span &gt;Our Team&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19050&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19050&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/advertisers/&quot;&gt;&lt;span &gt;Our Advertisers&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19677&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19677&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/our-sponsors/&quot;&gt;&lt;span &gt;Our Sponsors&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18975&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-18975&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/learn/&quot;&gt;&lt;span &gt;Learn&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-18973&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-18973&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/interpret-coffee/&quot;&gt;&lt;span &gt;Interpreting Coffee Reviews&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-15773&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-15773&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/coffee-reference/&quot;&gt;&lt;span &gt;Reference&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18974&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-18974&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/coffee-glossary/&quot;&gt;&lt;span &gt;Glossary&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-8925&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-8925&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/contact/&quot;&gt;&lt;span &gt;Contact Us&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18956&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-18956&quot;&gt;&lt;a href=&quot;#&quot;&gt;&lt;span &gt;Trade&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-13549&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-13549&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/calendar/&quot;&gt;&lt;span &gt;Tasting Report Calendar&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13543&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-13543&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/advertising/&quot;&gt;&lt;span &gt;Becoming an Advertiser&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19389&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19389&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/what-we-would-do-campaign-packages/&quot;&gt;&lt;span &gt;Campaign Package Deals&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13548&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-13548&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/review-services/&quot;&gt;&lt;span &gt;Getting Coffees Reviewed&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-18970&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-18970&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/guidelines/&quot;&gt;&lt;span &gt;Quoting Reviews&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19643&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19643&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/award-certificates/&quot;&gt;&lt;span &gt;Award Certificates&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-13544&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13544&quot;&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com/wp-content/uploads/2020/01/CR_Media_Kit_Jan_2020_v5.pdf&quot;&gt;&lt;span &gt;Media Kit&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19401&quot; class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-19401&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/category/blog/green-coffee-origins-and-issues/&quot;&gt;&lt;span &gt;中文 &amp;#8211; Chinese&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-13537&quot; class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-13537&quot;&gt;&lt;a href=&quot;/types/coffees-from-taiwan/&quot;&gt;&lt;span &gt;台灣送評的咖啡豆&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19392&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19392&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/%e5%a6%82%e4%bd%95%e5%b0%87%e6%82%a8%e7%9a%84%e5%92%96-%e5%95%a1%e9%80%81%e8%a9%95/&quot;&gt;&lt;span &gt;如何將您的咖啡送評&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19400&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19400&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/%e8%a1%8c%e9%8a%b7%e6%94%bb%e7%95%a5-%e4%bf%83%e9%8a%b7%e6%b4%bb%e5%8b%95/&quot;&gt;&lt;span &gt;“行銷攻略” 促銷活動&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19674&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-19674&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/why-become-a-member/&quot;&gt;&lt;span &gt;Members&lt;/span&gt;&lt;/a&gt;&lt;ul class=&quot;sub-menu&quot;&gt;&lt;li id=&quot;menu-item-19676&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19676&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/why-become-a-member/&quot;&gt;&lt;span &gt;WHY BECOME A MEMBER?&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19673&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19673&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/member-benefits/&quot;&gt;&lt;span &gt;Member Benefits&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19675&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19675&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/our-sponsors/&quot;&gt;&lt;span &gt;Our Sponsors&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19671&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19671&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/programs-and-initiatives/&quot;&gt;&lt;span &gt;Programs and Initiatives&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li id=&quot;menu-item-19672&quot; class=&quot;menu-item menu-item-type-post_type menu-item-object-page menu-item-19672&quot;&gt;&lt;a href=&quot;https://www.coffeereview.com/member-support/&quot;&gt;&lt;span &gt;Member Support&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/nav&gt;&lt;div class=&quot;site-inner&quot;&gt;&lt;div class=&quot;content-sidebar-wrap&quot;&gt;&lt;main class=&quot;content&quot; id=&quot;genesis-content&quot;&gt;&lt;article class=&quot;post-7685 review type-review status-publish format-standard types-supermarket entry override&quot;&gt;&lt;header class=&quot;entry-header&quot;&gt;&lt;/header&gt;&lt;div class=&quot;entry-content&quot;&gt;&lt;div class=&quot;review-template&quot;&gt;&lt;div class=&quot;row row-1&quot;&gt;&lt;div class=&quot;column col-1&quot;&gt; &lt;span class=&quot;review-template-rating&quot;&gt;84&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;column col-2&quot;&gt;&lt;p class=&quot;review-roaster&quot;&gt;Yuban&lt;/p&gt;&lt;h1 class=&quot;review-title&quot;&gt;100% Colombian&lt;/h1&gt;&lt;/div&gt;&lt;div class=&quot;column col-3&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;row row-2&quot;&gt;&lt;div class=&quot;column col-1&quot;&gt;&lt;table class=&quot;review-template-table&quot;&gt;&lt;tr&gt;&lt;td&gt;Roaster Location:&lt;/td&gt;&lt;td&gt;Rye Brook, New York&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Roast Level:&lt;/td&gt;&lt;td&gt;Very Dark&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Agtron:&lt;/td&gt;&lt;td&gt;0/46&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;div class=&quot;column col-2&quot;&gt;&lt;table class=&quot;review-template-table&quot;&gt;&lt;tr&gt;&lt;td&gt;Review Date:&lt;/td&gt;&lt;td&gt;March 2002&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Aroma:&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Acidity :&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Body:&lt;/td&gt;&lt;td&gt;7&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Flavor:&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Blind Assessment: &lt;/strong&gt;Sweet, balanced, restrained. The fruit and vegetal cocoa notes are faint but pleasant, the roasty tones understated.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Notes: &lt;/strong&gt;Sold roasted and ground in a 12-ounce can; 33 cents per ounce. Visit &lt;website link=&quot;www.yuban.com&quot;&gt;www.yuban.com&lt;/website&gt; or call 1-800-982-2649 for more info.&lt;/p&gt;&lt;p&gt;&lt;strong&gt; Who Should Drink It : &lt;/strong&gt;The sort of self-effacing, versatile coffee that doesn&#39;t call attention to itself, yet three cups later you&#39;re still pouring it.&lt;/p&gt;&lt;div class=&quot;row row-3&quot;&gt;&lt;div class=&quot;column col-1&quot;&gt; &lt;strong&gt;&lt;a href=&quot;/all-reviews/?roaster_name=Yuban&quot; title=&quot;Show all reviews for this roaster&quot;&gt;Show all reviews for this roaster&lt;/a&gt;&lt;/strong&gt;&lt;/div&gt;&lt;div class=&quot;column col-2&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style=&quot;text-align: center;&quot;&gt;&lt;br /&gt;This review originally appeared in the March, 2002 tasting report: &lt;a href=&quot;/the-robusta-fuss&quot; style=&quot;font-weight: bold;&quot;&gt;The Robusta Fuss&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;footer class=&quot;entry-footer&quot;&gt;&lt;/footer&gt;&lt;/article&gt;&lt;img src=&quot;https://dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com/wp-content/themes/dynamik-gen/images/content-filler.png&quot; class=&quot;dynamik-content-filler-img&quot; alt=&quot;&quot;&gt;&lt;/main&gt;&lt;aside class=&quot;sidebar sidebar-primary widget-area&quot; role=&quot;complementary&quot; aria-label=&quot;Primary Sidebar&quot; id=&quot;genesis-sidebar-primary&quot;&gt;&lt;h2 class=&quot;genesis-sidebar-title screen-reader-text&quot;&gt;Primary Sidebar&lt;/h2&gt;&lt;section id=&quot;cr_advertiser_widget-2&quot; class=&quot;widget widget_cr_advertiser_widget&quot;&gt;&lt;div class=&quot;widget-wrap&quot;&gt;&lt;div id=&quot;cr-advertiser-widget-content&quot;&gt;&lt;/div&gt; &lt;script&gt;var passedArray = [{&quot;url&quot;:&quot;https: / /espressorepublic.com /shop /&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2017 /05 /Coffee-Review-Ad-2-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot; &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;&quot;},{&quot;url&quot;:&quot;https: / /paradiseroasters.com&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /paradise-cr-1-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for top-rated coffees at Paradise Roasters &quot; srcset= &quot;https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2014 /04 /paradise-cr-1-300x190.jpg 300w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2014 /04 /paradise-cr-1.jpg 600w &quot; sizes= &quot;(max-width: 300px) 100vw, 300px &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;Elite single-origin coffees and top-rated espressos.&quot;},{&quot;url&quot;:&quot;https: / /www.jbccoffeeroasters.com /product-category /coffee /&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;189 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2018 /11 /JBC-300x190-Dec-2018-300x189.jpeg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for Top-Rated Coffees at JBC Coffee Roasters &quot; srcset= &quot;https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2018 /11 /JBC-300x190-Dec-2018-300x189.jpeg 300w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2018 /11 /JBC-300x190-Dec-2018-768x485.jpeg 768w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2018 /11 /JBC-300x190-Dec-2018.jpeg 1020w &quot; sizes= &quot;(max-width: 300px) 100vw, 300px &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;JBC Coffee Roasters of Madison, Wisconsin is a distinguished small-batch roaster that has produced numerous 90+ point coffees.&quot;},{&quot;url&quot;:&quot;https: / /www.willoughbyscoffee.com /&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /CR_Willoughbys_300x190_vA-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Visit Willoughby&amp;#039;s Coffee And Tea &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;&quot;},{&quot;url&quot;:&quot;https: / /bit.ly /2V86Iaw&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /06 /Thanksgiving_Coffee-CR_ad-2020_Mocha-Java.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for 96-point Mocha Java at Thanksgiving Coffee &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;Thanksgiving Coffee was founded in 1972 by Paul and Joan Katzeff. From their roastery in the small town of Fort Bragg, California comes amazing single origins, complex blends and stunning espressos. &quot;},{&quot;url&quot;:&quot;https: / /www.1stincoffee.com&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /CR_firstincoffee_300x190-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;1st in Coffee Logo &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;Superior service and low prices on top-quality espresso machines, coffee equipment, and accessories. Free shipping.&quot;},{&quot;url&quot;:&quot;https: / /jackrabbitjava.com /&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2018 /11 /2x_BannerAd_JRJV_2020-300x190.png &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for top-rated coffees at great prices at Jackrabbit Java &quot; srcset= &quot;https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2018 /11 /2x_BannerAd_JRJV_2020-300x190.png 300w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2018 /11 /2x_BannerAd_JRJV_2020.png 600w &quot; sizes= &quot;(max-width: 300px) 100vw, 300px &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;&quot;},{&quot;url&quot;:&quot;http: / /www.mysticmonkcoffee.com&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /CR_mysticmonk_300x190-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Mystic Monk Coffee Ad &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;Gourmet coffees roasted by the Carmelite Monks at their monastery in the Rocky Mountains of northern Wyoming.&quot;},{&quot;url&quot;:&quot;https: / /greatergoodsroasting.com /collections /all-coffee&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2016 /03 /GG-CR-LTO-300x190@2X-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for top-rated coffees at Greater Goods &quot; srcset= &quot;https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2016 /03 /GG-CR-LTO-300x190@2X-300x190.jpg 300w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2016 /03 /GG-CR-LTO-300x190@2X.jpg 600w &quot; sizes= &quot;(max-width: 300px) 100vw, 300px &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;&quot;},{&quot;url&quot;:&quot;http: / /www.lexingtoncoffee.com&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /Lexington-300x190-Ad-Dec-2018-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for top-rated coffees at Lexington Coffee Roasters &quot; srcset= &quot;https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2014 /04 /Lexington-300x190-Ad-Dec-2018-300x190.jpg 300w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2014 /04 /Lexington-300x190-Ad-Dec-2018-768x486.jpg 768w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2014 /04 /Lexington-300x190-Ad-Dec-2018-1024x649.jpg 1024w, https: / /dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com /wp-content /uploads /2014 /04 /Lexington-300x190-Ad-Dec-2018.jpg 2048w &quot; sizes= &quot;(max-width: 300px) 100vw, 300px &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot; &quot;Uncommon &amp; Uncompromised &quot; artisan coffees roasted to order and shipped the same day.&quot;},{&quot;url&quot;:&quot;https: / /www.ptscoffee.com&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /PTs-300x190-banner-300x190.png &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for top-rated coffees at PT&amp;#039;s Coffee &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;Award-winning single origin coffees and top-of-the-line equipment for homes and businesses.&quot;},{&quot;url&quot;:&quot;https: / /www.klatchroasting.com /products /out-of-africa-blend&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2018 /11 /CR_OUT_OF_AFR.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for African coffees at Klatch Coffee &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;&quot;},{&quot;url&quot;:&quot;https: / /www.templecoffee.com&quot;,&quot;thumb&quot;:&quot;&lt;img width= &quot;300 &quot; height= &quot;190 &quot; src= &quot;https: / /www.coffeereview.com /wp-content /uploads /2014 /04 /Coffee-Review-Ad-Decv2-300x190.jpg &quot; class= &quot;attachment-medium size-medium wp-post-image &quot; alt= &quot;Shop for top-rated coffees at Temple Coffee &quot; /&gt;&quot;,&quot;excerpt&quot;:&quot;Temple Coffee specializing in artisan coffees from individual farms and cooperatives.&quot;}]; &#39;&#39;&#39; . . from bs4 import BeautifulSoup soup = BeautifulSoup(html_sample, &#39;html.parser&#39;) print(soup.prettify()) . &lt;body class=&#34;review-template-default single single-review postid-7685 single-format-standard header-image content-sidebar genesis-breadcrumbs-hidden unknown-os ie11 feature-top-outside site-fluid override&#34;&gt; &lt;div class=&#34;site-container&#34;&gt; &lt;ul class=&#34;genesis-skip-link&#34;&gt; &lt;li&gt; &lt;a class=&#34;screen-reader-shortcut&#34; href=&#34;#genesis-nav-primary&#34;&gt; Skip to primary navigation &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&#34;screen-reader-shortcut&#34; href=&#34;#genesis-content&#34;&gt; Skip to main content &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=&#34;screen-reader-shortcut&#34; href=&#34;#genesis-sidebar-primary&#34;&gt; Skip to primary sidebar &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;header class=&#34;site-header&#34;&gt; &lt;div class=&#34;wrap&#34;&gt; &lt;div class=&#34;title-area&#34;&gt; &lt;p class=&#34;site-title&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/&#34;&gt; Coffee Review &lt;/a&gt; &lt;/p&gt; &lt;p class=&#34;site-description&#34;&gt; The World&#39;s Leading Coffee Guide &lt;/p&gt; &lt;/div&gt; &lt;div class=&#34;widget-area header-widget-area&#34;&gt; &lt;section class=&#34;widget widget_text&#34; id=&#34;text-11&#34;&gt; &lt;div class=&#34;widget-wrap&#34;&gt; &lt;div class=&#34;textwidget&#34;&gt; &lt;form action=&#34;https://www.coffeereview.com/&#34; id=&#34;searchform&#34; method=&#34;get&#34; role=&#34;search&#34;&gt; &lt;div class=&#34;header_search_line_1&#34;&gt; &lt;input checked=&#34;checked&#34; id=&#34;cr_reviews&#34; name=&#34;post_type&#34; type=&#34;radio&#34; value=&#34;review&#34;/&gt; &lt;label for=&#34;cr_reviews&#34;&gt; Reviews &lt;/label&gt; &lt;input id=&#34;cr_tasting_reports&#34; name=&#34;post_type&#34; type=&#34;radio&#34; value=&#34;post&#34;/&gt; &lt;label for=&#34;cr_tasting_reports&#34;&gt; Tasting Reports &lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;header_search_line_2&#34;&gt; &lt;input id=&#34;searchfield&#34; maxlength=&#34;50&#34; name=&#34;s&#34; placeholder=&#34;Enter search terms&#34; size=&#34;18&#34; type=&#34;search&#34; value=&#34;&#34;/&gt; &lt;input class=&#34;header_search_button&#34; type=&#34;submit&#34; value=&#34;Search&#34;/&gt; &lt;/div&gt; &lt;div class=&#34;header_search_line_3&#34;&gt; &lt;a href=&#34;/advanced-search/&#34;&gt; Advanced Search &lt;/a&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;section class=&#34;widget widget_text&#34; id=&#34;text-12&#34;&gt; &lt;div class=&#34;widget-wrap&#34;&gt; &lt;div class=&#34;textwidget&#34;&gt; &lt;p&gt; &lt;a href=&#34;http://bit.ly/2r1silm&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Shop for Award-Winning 100% Kona Coffees at Hula Daddy&#34; class=&#34;aligncenter size-full wp-image-19359&#34; height=&#34;90&#34; src=&#34;https://dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/Hula-Daddy-Button-1st-place.jpg&#34; width=&#34;195&#34;/&gt; &lt;/a&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;/div&gt; &lt;/div&gt; &lt;/header&gt; &lt;div class=&#34;responsive-primary-menu-container&#34;&gt; &lt;h3 class=&#34;mobile-primary-toggle&#34;&gt; &lt;/h3&gt; &lt;div class=&#34;responsive-menu-icon&#34;&gt; &lt;span class=&#34;responsive-icon-bar&#34;&gt; &lt;/span&gt; &lt;span class=&#34;responsive-icon-bar&#34;&gt; &lt;/span&gt; &lt;span class=&#34;responsive-icon-bar&#34;&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;nav aria-label=&#34;Main&#34; class=&#34;nav-primary&#34; id=&#34;genesis-nav-primary&#34;&gt; &lt;div class=&#34;wrap&#34;&gt; &lt;ul class=&#34;menu genesis-nav-menu menu-primary js-superfish&#34; id=&#34;menu-main&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-4998&#34; id=&#34;menu-item-4998&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/review/&#34;&gt; &lt;span&gt; Reviews &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13553&#34; id=&#34;menu-item-13553&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/review/&#34;&gt; &lt;span&gt; Latest Reviews &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-13533&#34; id=&#34;menu-item-13533&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/highest-rated-coffees/&#34;&gt; &lt;span&gt; Top-Rated (94+) &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-18958&#34; id=&#34;menu-item-18958&#34;&gt; &lt;a href=&#34;https://coffeereview.com/types/espresso/&#34;&gt; &lt;span&gt; Espressos &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13534&#34; id=&#34;menu-item-13534&#34;&gt; &lt;a href=&#34;https://coffeereview.com/types/best-value-coffees/&#34;&gt; &lt;span&gt; Best Values &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-19057&#34; id=&#34;menu-item-19057&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/top-30-coffees-2019/&#34;&gt; &lt;span&gt; Top 30 Coffees of 2019 &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-19632&#34; id=&#34;menu-item-19632&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/types/coffees-from-taiwan/&#34;&gt; &lt;span&gt; Taiwan Coffees – 台灣送評的咖啡豆 &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-19160&#34; id=&#34;menu-item-19160&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/top-30-coffees-past-rankings/&#34;&gt; &lt;span&gt; Top 30 – Past Rankings &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13536&#34; id=&#34;menu-item-13536&#34;&gt; &lt;a href=&#34;https://coffeereview.com/types/single-serve-capsule/&#34;&gt; &lt;span&gt; Pods and Capsules &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-18959&#34; id=&#34;menu-item-18959&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/best-coffee-cities/&#34;&gt; &lt;span&gt; Reviews by U.S. City &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13857&#34; id=&#34;menu-item-13857&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/advanced-search/&#34;&gt; &lt;span&gt; Advanced Search &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-18960&#34; id=&#34;menu-item-18960&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/&#34;&gt; &lt;span&gt; Reports &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-15819&#34; id=&#34;menu-item-15819&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/&#34;&gt; &lt;span&gt; Latest Reports &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18961&#34; id=&#34;menu-item-18961&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/africa/&#34;&gt; &lt;span&gt; Africa &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18962&#34; id=&#34;menu-item-18962&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/americas/&#34;&gt; &lt;span&gt; Americas &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18964&#34; id=&#34;menu-item-18964&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/asia-pacific-coffees/&#34;&gt; &lt;span&gt; Asia-Pacific &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18966&#34; id=&#34;menu-item-18966&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/espressos/&#34;&gt; &lt;span&gt; Espressos &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18963&#34; id=&#34;menu-item-18963&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/annual-top-30/&#34;&gt; &lt;span&gt; Annual Top 30 &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18967&#34; id=&#34;menu-item-18967&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/tasting-report-processing-method/&#34;&gt; &lt;span&gt; Processing Method &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18968&#34; id=&#34;menu-item-18968&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/tasting-reports-social-environmental/&#34;&gt; &lt;span&gt; Social/Environmental &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18969&#34; id=&#34;menu-item-18969&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/tasting-reports-tree-variety/&#34;&gt; &lt;span&gt; Tree Variety &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18965&#34; id=&#34;menu-item-18965&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/articles/coffee-and-espresso-blends/&#34;&gt; &lt;span&gt; Blends &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-15781&#34; id=&#34;menu-item-15781&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/equipment-reports/&#34;&gt; &lt;span&gt; Equipment &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-post menu-item-19950&#34; id=&#34;menu-item-19950&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/burr-coffee-grinder-reviews/&#34;&gt; &lt;span&gt; Mid-Range Burr Coffee Grinders &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-post menu-item-19622&#34; id=&#34;menu-item-19622&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/equipment-report-digital-electric-gooseneck-pourover-kettles/&#34;&gt; &lt;span&gt; Electric Gooseneck Kettles &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19586&#34; id=&#34;menu-item-19586&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/interpreting-equipment-ratings/&#34;&gt; &lt;span&gt; Interpreting Equipment Ratings &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-12025&#34; id=&#34;menu-item-12025&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/blog/&#34;&gt; &lt;span&gt; Journal &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-post menu-item-19959&#34; id=&#34;menu-item-19959&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/recognizing-the-coffees-communities-and-contributions-of-black-owned-coffee-companies/&#34;&gt; &lt;span&gt; Recognizing Black-Owned Coffee Companies &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-19650&#34; id=&#34;menu-item-19650&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/covid-19-info/&#34;&gt; &lt;span&gt; COVID-19 Information &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-18977&#34; id=&#34;menu-item-18977&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/our-story/&#34;&gt; &lt;span&gt; About &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-18978&#34; id=&#34;menu-item-18978&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/our-story/&#34;&gt; &lt;span&gt; Our Story &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-18971&#34; id=&#34;menu-item-18971&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/kennethdavids/&#34;&gt; &lt;span&gt; Kenneth Davids &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13913&#34; id=&#34;menu-item-13913&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/our-team/&#34;&gt; &lt;span&gt; Our Team &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19050&#34; id=&#34;menu-item-19050&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/advertisers/&#34;&gt; &lt;span&gt; Our Advertisers &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19677&#34; id=&#34;menu-item-19677&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/our-sponsors/&#34;&gt; &lt;span&gt; Our Sponsors &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-18975&#34; id=&#34;menu-item-18975&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/learn/&#34;&gt; &lt;span&gt; Learn &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-18973&#34; id=&#34;menu-item-18973&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/interpret-coffee/&#34;&gt; &lt;span&gt; Interpreting Coffee Reviews &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-15773&#34; id=&#34;menu-item-15773&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/coffee-reference/&#34;&gt; &lt;span&gt; Reference &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-18974&#34; id=&#34;menu-item-18974&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/coffee-glossary/&#34;&gt; &lt;span&gt; Glossary &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-8925&#34; id=&#34;menu-item-8925&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/contact/&#34;&gt; &lt;span&gt; Contact Us &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-18956&#34; id=&#34;menu-item-18956&#34;&gt; &lt;a href=&#34;#&#34;&gt; &lt;span&gt; Trade &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-13549&#34; id=&#34;menu-item-13549&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/calendar/&#34;&gt; &lt;span&gt; Tasting Report Calendar &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-13543&#34; id=&#34;menu-item-13543&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/advertising/&#34;&gt; &lt;span&gt; Becoming an Advertiser &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19389&#34; id=&#34;menu-item-19389&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/what-we-would-do-campaign-packages/&#34;&gt; &lt;span&gt; Campaign Package Deals &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-13548&#34; id=&#34;menu-item-13548&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/review-services/&#34;&gt; &lt;span&gt; Getting Coffees Reviewed &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-18970&#34; id=&#34;menu-item-18970&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/guidelines/&#34;&gt; &lt;span&gt; Quoting Reviews &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19643&#34; id=&#34;menu-item-19643&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/award-certificates/&#34;&gt; &lt;span&gt; Award Certificates &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13544&#34; id=&#34;menu-item-13544&#34;&gt; &lt;a href=&#34;https://dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com/wp-content/uploads/2020/01/CR_Media_Kit_Jan_2020_v5.pdf&#34; rel=&#34;noopener noreferrer&#34; target=&#34;_blank&#34;&gt; &lt;span&gt; Media Kit &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-has-children menu-item-19401&#34; id=&#34;menu-item-19401&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/category/blog/green-coffee-origins-and-issues/&#34;&gt; &lt;span&gt; 中文 – Chinese &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-custom menu-item-object-custom menu-item-13537&#34; id=&#34;menu-item-13537&#34;&gt; &lt;a href=&#34;/types/coffees-from-taiwan/&#34;&gt; &lt;span&gt; 台灣送評的咖啡豆 &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19392&#34; id=&#34;menu-item-19392&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/%e5%a6%82%e4%bd%95%e5%b0%87%e6%82%a8%e7%9a%84%e5%92%96-%e5%95%a1%e9%80%81%e8%a9%95/&#34;&gt; &lt;span&gt; 如何將您的咖啡送評 &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19400&#34; id=&#34;menu-item-19400&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/%e8%a1%8c%e9%8a%b7%e6%94%bb%e7%95%a5-%e4%bf%83%e9%8a%b7%e6%b4%bb%e5%8b%95/&#34;&gt; &lt;span&gt; “行銷攻略” 促銷活動 &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-19674&#34; id=&#34;menu-item-19674&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/why-become-a-member/&#34;&gt; &lt;span&gt; Members &lt;/span&gt; &lt;/a&gt; &lt;ul class=&#34;sub-menu&#34;&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19676&#34; id=&#34;menu-item-19676&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/why-become-a-member/&#34;&gt; &lt;span&gt; WHY BECOME A MEMBER? &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19673&#34; id=&#34;menu-item-19673&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/member-benefits/&#34;&gt; &lt;span&gt; Member Benefits &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19675&#34; id=&#34;menu-item-19675&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/our-sponsors/&#34;&gt; &lt;span&gt; Our Sponsors &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19671&#34; id=&#34;menu-item-19671&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/programs-and-initiatives/&#34;&gt; &lt;span&gt; Programs and Initiatives &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li class=&#34;menu-item menu-item-type-post_type menu-item-object-page menu-item-19672&#34; id=&#34;menu-item-19672&#34;&gt; &lt;a href=&#34;https://www.coffeereview.com/member-support/&#34;&gt; &lt;span&gt; Member Support &lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/nav&gt; &lt;div class=&#34;site-inner&#34;&gt; &lt;div class=&#34;content-sidebar-wrap&#34;&gt; &lt;main class=&#34;content&#34; id=&#34;genesis-content&#34;&gt; &lt;article class=&#34;post-7685 review type-review status-publish format-standard types-supermarket entry override&#34;&gt; &lt;header class=&#34;entry-header&#34;&gt; &lt;/header&gt; &lt;div class=&#34;entry-content&#34;&gt; &lt;div class=&#34;review-template&#34;&gt; &lt;div class=&#34;row row-1&#34;&gt; &lt;div class=&#34;column col-1&#34;&gt; &lt;span class=&#34;review-template-rating&#34;&gt; 84 &lt;/span&gt; &lt;/div&gt; &lt;div class=&#34;column col-2&#34;&gt; &lt;p class=&#34;review-roaster&#34;&gt; Yuban &lt;/p&gt; &lt;h1 class=&#34;review-title&#34;&gt; 100% Colombian &lt;/h1&gt; &lt;/div&gt; &lt;div class=&#34;column col-3&#34;&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&#34;row row-2&#34;&gt; &lt;div class=&#34;column col-1&#34;&gt; &lt;table class=&#34;review-template-table&#34;&gt; &lt;tr&gt; &lt;td&gt; Roaster Location: &lt;/td&gt; &lt;td&gt; Rye Brook, New York &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Roast Level: &lt;/td&gt; &lt;td&gt; Very Dark &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Agtron: &lt;/td&gt; &lt;td&gt; 0/46 &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&#34;column col-2&#34;&gt; &lt;table class=&#34;review-template-table&#34;&gt; &lt;tr&gt; &lt;td&gt; Review Date: &lt;/td&gt; &lt;td&gt; March 2002 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Aroma: &lt;/td&gt; &lt;td&gt; 5 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Acidity : &lt;/td&gt; &lt;td&gt; 5 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Body: &lt;/td&gt; &lt;td&gt; 7 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; Flavor: &lt;/td&gt; &lt;td&gt; 7 &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;p&gt; &lt;strong&gt; Blind Assessment: &lt;/strong&gt; Sweet, balanced, restrained. The fruit and vegetal cocoa notes are faint but pleasant, the roasty tones understated. &lt;/p&gt; &lt;p&gt; &lt;strong&gt; Notes: &lt;/strong&gt; Sold roasted and ground in a 12-ounce can; 33 cents per ounce. Visit &lt;website link=&#34;www.yuban.com&#34;&gt; www.yuban.com &lt;/website&gt; or call 1-800-982-2649 for more info. &lt;/p&gt; &lt;p&gt; &lt;strong&gt; Who Should Drink It : &lt;/strong&gt; The sort of self-effacing, versatile coffee that doesn&#39;t call attention to itself, yet three cups later you&#39;re still pouring it. &lt;/p&gt; &lt;div class=&#34;row row-3&#34;&gt; &lt;div class=&#34;column col-1&#34;&gt; &lt;strong&gt; &lt;a href=&#34;/all-reviews/?roaster_name=Yuban&#34; title=&#34;Show all reviews for this roaster&#34;&gt; Show all reviews for this roaster &lt;/a&gt; &lt;/strong&gt; &lt;/div&gt; &lt;div class=&#34;column col-2&#34;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;br/&gt; This review originally appeared in the March, 2002 tasting report: &lt;a href=&#34;/the-robusta-fuss&#34; style=&#34;font-weight: bold;&#34;&gt; The Robusta Fuss &lt;/a&gt; &lt;/p&gt; &lt;/div&gt; &lt;footer class=&#34;entry-footer&#34;&gt; &lt;/footer&gt; &lt;/article&gt; &lt;img alt=&#34;&#34; class=&#34;dynamik-content-filler-img&#34; src=&#34;https://dlo9n43mpvj20faa12i3i3lh-wpengine.netdna-ssl.com/wp-content/themes/dynamik-gen/images/content-filler.png&#34;/&gt; &lt;/main&gt; &lt;aside aria-label=&#34;Primary Sidebar&#34; class=&#34;sidebar sidebar-primary widget-area&#34; id=&#34;genesis-sidebar-primary&#34; role=&#34;complementary&#34;&gt; &lt;h2 class=&#34;genesis-sidebar-title screen-reader-text&#34;&gt; Primary Sidebar &lt;/h2&gt; &lt;section class=&#34;widget widget_cr_advertiser_widget&#34; id=&#34;cr_advertiser_widget-2&#34;&gt; &lt;div class=&#34;widget-wrap&#34;&gt; &lt;div id=&#34;cr-advertiser-widget-content&#34;&gt; &lt;/div&gt; &lt;script&gt; &lt;/script&gt; &lt;/div&gt; &lt;/section&gt; &lt;/aside&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; . . The sample we use here is one of the page in coffee review, a simple table with some text. . *this page is not the same as the html sample . . One small tips to find the target data faster is to use the &quot;inspect&quot; function of you browser, just right click on the target data and select inspect, the browser will automatically locate the data position for us. . Most of the time we are playing with find and find_all functions in the BeautifulSoup class. (surely there are a lot more, e.g. we can find all a tag using soup.a, but most of the case there are too many other tags that we have to reduce our choices by some more specific conditions, like all a tags in a div tag named &quot;target&quot;) . find() will give you the first matched result, while find_all() can return all matched result in a list. . target_div = soup.find(&quot;div&quot;, name=&#39;target&#39;) # we can also search for class name target_div = soup.find(&quot;div&quot;, class_=&#39;target&#39;) # or id target_div = soup.find(&quot;div&quot;, id=&#39;target&#39;) . In the following example, we will extract the data in the table in dictionary form. As we can see the target data are locate inside the &quot;div&quot; tag with class &quot;entry-content&quot;, reducing the search area will give a less noisy result (e.g. you can tell when you search for the &quot;a&quot; tags for webpage of coffee rosters). . content = soup.find(&#39;div&#39;, class_=&#39;entry-content&#39;) content . &lt;div class=&#34;entry-content&#34;&gt;&lt;div class=&#34;review-template&#34;&gt;&lt;div class=&#34;row row-1&#34;&gt;&lt;div class=&#34;column col-1&#34;&gt; &lt;span class=&#34;review-template-rating&#34;&gt;84&lt;/span&gt;&lt;/div&gt;&lt;div class=&#34;column col-2&#34;&gt;&lt;p class=&#34;review-roaster&#34;&gt;Yuban&lt;/p&gt;&lt;h1 class=&#34;review-title&#34;&gt;100% Colombian&lt;/h1&gt;&lt;/div&gt;&lt;div class=&#34;column col-3&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;row row-2&#34;&gt;&lt;div class=&#34;column col-1&#34;&gt;&lt;table class=&#34;review-template-table&#34;&gt;&lt;tr&gt;&lt;td&gt;Roaster Location:&lt;/td&gt;&lt;td&gt;Rye Brook, New York&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Roast Level:&lt;/td&gt;&lt;td&gt;Very Dark&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Agtron:&lt;/td&gt;&lt;td&gt;0/46&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;div class=&#34;column col-2&#34;&gt;&lt;table class=&#34;review-template-table&#34;&gt;&lt;tr&gt;&lt;td&gt;Review Date:&lt;/td&gt;&lt;td&gt;March 2002&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Aroma:&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Acidity :&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Body:&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Flavor:&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Blind Assessment: &lt;/strong&gt;Sweet, balanced, restrained. The fruit and vegetal cocoa notes are faint but pleasant, the roasty tones understated.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Notes: &lt;/strong&gt;Sold roasted and ground in a 12-ounce can; 33 cents per ounce. Visit &lt;website link=&#34;www.yuban.com&#34;&gt;www.yuban.com&lt;/website&gt; or call 1-800-982-2649 for more info.&lt;/p&gt;&lt;p&gt;&lt;strong&gt; Who Should Drink It : &lt;/strong&gt;The sort of self-effacing, versatile coffee that doesn&#39;t call attention to itself, yet three cups later you&#39;re still pouring it.&lt;/p&gt;&lt;div class=&#34;row row-3&#34;&gt;&lt;div class=&#34;column col-1&#34;&gt; &lt;strong&gt;&lt;a href=&#34;/all-reviews/?roaster_name=Yuban&#34; title=&#34;Show all reviews for this roaster&#34;&gt;Show all reviews for this roaster&lt;/a&gt;&lt;/strong&gt;&lt;/div&gt;&lt;div class=&#34;column col-2&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style=&#34;text-align: center;&#34;&gt;&lt;br/&gt;This review originally appeared in the March, 2002 tasting report: &lt;a href=&#34;/the-robusta-fuss&#34; style=&#34;font-weight: bold;&#34;&gt;The Robusta Fuss&lt;/a&gt;&lt;/p&gt;&lt;/div&gt; . Compare to the original page, there is much less to handle, we can pick out data one by one. Let&#39;s start by getting the roaster, name of coffee and the overall score of the coffee (the title part of the page) . review_dict = {} review_dict[&#39;OverallRating&#39;] = content.find(&#39;span&#39;, class_=&#39;review-template-rating&#39;).text review_dict[&#39;Roaster&#39;] = content.find(&#39;p&#39;, class_=&#39;review-roaster&#39;).text review_dict[&#39;Name&#39;] = content.find(&#39;h1&#39;, class_=&#39;review-title&#39;).text review_dict . {&#39;OverallRating&#39;: &#39;84&#39;, &#39;Roaster&#39;: &#39;Yuban&#39;, &#39;Name&#39;: &#39;100% Colombian&#39;} . Surely we can parse the table in the same way, but the process can be simplified if we have a some knowledge on html format. In html, each row of table is within a &quot;tr&quot; tag, while each cell is inside a &quot;td&quot; tag. So if we loop through each &quot;tr&quot; tag, and by knowing that the first cell is the item name (key) and the second is the data (value), we can put everything in a loop by asking BeautifulSoup to search for all &quot;tr&quot; than &quot;td&quot;. . rows = content.find_all(&#39;tr&#39;) #loop through each row for row in rows: # convert all cells within a single row into a list cells = row.find_all(&#39;td&#39;) print(cells) # use .text get the text within the tag review_dict[cells[0].text] = cells[1].text # let&#39;s see the result review_dict . [&lt;td&gt;Roaster Location:&lt;/td&gt;, &lt;td&gt;Rye Brook, New York&lt;/td&gt;] [&lt;td&gt;Roast Level:&lt;/td&gt;, &lt;td&gt;Very Dark&lt;/td&gt;] [&lt;td&gt;Agtron:&lt;/td&gt;, &lt;td&gt;0/46&lt;/td&gt;] [&lt;td&gt;Review Date:&lt;/td&gt;, &lt;td&gt;March 2002&lt;/td&gt;] [&lt;td&gt;Aroma:&lt;/td&gt;, &lt;td&gt;5&lt;/td&gt;] [&lt;td&gt;Acidity :&lt;/td&gt;, &lt;td&gt;5&lt;/td&gt;] [&lt;td&gt;Body:&lt;/td&gt;, &lt;td&gt;7&lt;/td&gt;] [&lt;td&gt;Flavor:&lt;/td&gt;, &lt;td&gt;7&lt;/td&gt;] . {&#39;OverallRating&#39;: &#39;84&#39;, &#39;Roaster&#39;: &#39;Yuban&#39;, &#39;Name&#39;: &#39;100% Colombian&#39;, &#39;Roaster Location:&#39;: &#39;Rye Brook, New York&#39;, &#39;Roast Level:&#39;: &#39;Very Dark&#39;, &#39;Agtron:&#39;: &#39;0/46&#39;, &#39;Review Date:&#39;: &#39;March 2002&#39;, &#39;Aroma:&#39;: &#39;5&#39;, &#39;Acidity :&#39;: &#39;5&#39;, &#39;Body:&#39;: &#39;7&#39;, &#39;Flavor:&#39;: &#39;7&#39;} . Now basically we get most of the useful data in the page, and finally if we want to get all comments below the table, we can search for the &quot;p&quot; tag. But here we notice the text always start with bold format, so we can instead search for the &quot;strong&quot; format. . for item in content.find_all(&#39;strong&#39;): # .replace(&#39;: &#39;, &#39;&#39;) is to clear the &#39;: &#39; after the key, # and .strip() is to cut the space before and after the text. # here we use the .next_sibling method, which is taking the next item right after text review_dict[item.text.replace(&#39;: &#39;, &#39;&#39;).strip()] = item.next_sibling review_dict . {&#39;OverallRating&#39;: &#39;84&#39;, &#39;Roaster&#39;: &#39;Yuban&#39;, &#39;Name&#39;: &#39;100% Colombian&#39;, &#39;Roaster Location:&#39;: &#39;Rye Brook, New York&#39;, &#39;Roast Level:&#39;: &#39;Very Dark&#39;, &#39;Agtron:&#39;: &#39;0/46&#39;, &#39;Review Date:&#39;: &#39;March 2002&#39;, &#39;Aroma:&#39;: &#39;5&#39;, &#39;Acidity :&#39;: &#39;5&#39;, &#39;Body:&#39;: &#39;7&#39;, &#39;Flavor:&#39;: &#39;7&#39;, &#39;Blind Assessment&#39;: &#39;Sweet, balanced, restrained. The fruit and vegetal cocoa notes are faint but pleasant, the roasty tones understated.&#39;, &#39;Notes&#39;: &#39;Sold roasted and ground in a 12-ounce can; 33 cents per ounce. Visit &#39;, &#39;Who Should Drink It&#39;: &#34;The sort of self-effacing, versatile coffee that doesn&#39;t call attention to itself, yet three cups later you&#39;re still pouring it.&#34;, &#39;Show all reviews for this roaster&#39;: None} . Actually you can see there is a missing link in the &#39;Notes&#39;, which is inside an &#39;website&#39; tag, to extract that might need another .next_sibling and some if statments, or if we use the multiple from .next_sublings . for item in content.find_all(&#39;strong&#39;): for text in item.next_siblings: if not review_dict.get(item.text.replace(&#39;: &#39;, &#39;&#39;).strip()): review_dict[item.text.replace(&#39;: &#39;, &#39;&#39;).strip()] = &#39;&#39; if isinstance(text, str): add_content = text else: add_content = text.text review_dict[item.text.replace(&#39;: &#39;, &#39;&#39;).strip()] += add_content review_dict . {&#39;OverallRating&#39;: &#39;84&#39;, &#39;Roaster&#39;: &#39;Yuban&#39;, &#39;Name&#39;: &#39;100% Colombian&#39;, &#39;Roaster Location:&#39;: &#39;Rye Brook, New York&#39;, &#39;Roast Level:&#39;: &#39;Very Dark&#39;, &#39;Agtron:&#39;: &#39;0/46&#39;, &#39;Review Date:&#39;: &#39;March 2002&#39;, &#39;Aroma:&#39;: &#39;5&#39;, &#39;Acidity :&#39;: &#39;5&#39;, &#39;Body:&#39;: &#39;7&#39;, &#39;Flavor:&#39;: &#39;7&#39;, &#39;Blind Assessment&#39;: &#39;Sweet, balanced, restrained. The fruit and vegetal cocoa notes are faint but pleasant, the roasty tones understated.&#39;, &#39;Notes&#39;: &#39;Sold roasted and ground in a 12-ounce can; 33 cents per ounce. Visit www.yuban.com or call 1-800-982-2649 for more info.&#39;, &#39;Who Should Drink It&#39;: &#34;The sort of self-effacing, versatile coffee that doesn&#39;t call attention to itself, yet three cups later you&#39;re still pouring it.&#34;} . And that is it! With the script above (and add some exception case) we can basically extract data from the coffee review, the remaining work would be a) convert text to integer for statistic (can be done by int() function); b) review date should be in datetime format (can be done by datetime library); . If you prefer to learn the whole web scraping process, here is a youtube tutorial by freeCodeCamp. ;) .",
            "url": "https://tikuischan.github.io/fastpages_blog/python/dataextraction/beautifulsoup/2021/04/01/Data-Extraction-with-Python.html",
            "relUrl": "/python/dataextraction/beautifulsoup/2021/04/01/Data-Extraction-with-Python.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
  
    
        ,"post16": {
            "title": "List Comprehension",
            "content": "why and when we want to use it | how to use it | some examples | In this blog we will be looking at a very convinence syntex in python - list comprehension. . To begin with, the basic form of Python list comprehension look like this: . list_comp = [x for x in range(10)] print(list_comp) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . it is basically equal to a for loop, and put everything in a list . list_comp = [] for i in range(10): list_comp.append(i) print(list_comp) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Up to now you may already notice, list comprehension is basically a simplified for loop. Sometime you may want to add a if statment in a for loop: . output = [] # assume we want to get a list of even number using for loop for i in range(10): if i % 2 == 0: output.append(i) print(output) . [0, 2, 4, 6, 8] . well...surely you can do it in a cleverer way without a IF statement... . output = [] for i in range(0, 10, 2): output.append(i) print(output) . . [0, 2, 4, 6, 8] . with list comprehension, you can do it in a single line: . output = [x for x in range(10) if x % 2 == 0] print(output) output = [x for x in range(0, 10, 2)] print(output) . [0, 2, 4, 6, 8] [0, 2, 4, 6, 8] . So yes, we can add conditional statments if needed, here are some more examples: . data_in = [[1, 2, 3, 4], [5, 6], [7, 8], [9]] data_flat = [y for x in data_in for y in x] print(data_flat) # maybe our task is to grep all dict with attr &#39;name&#39; data_in = [{&#39;name&#39;: &#39;john&#39;}, {&#39;name&#39;: &#39;peter&#39;}, {&#39;person&#39;: &#39;sk&#39;}, {&#39;name&#39;: &#39;mary&#39;}, {&#39;phone&#39;: &#39;12364586225567&#39;}] data_out = [x for x in data_in if x.get(&#39;name&#39;)] print(data_out) # we can even generate a dict using list comprehension (or dict comprehension) keys = [&#39;name&#39;, &#39;age&#39;] values = [[&#39;john&#39;, &#39;10&#39;], [&#39;foo&#39;, &#39;24&#39;], [&#39;may&#39;, &#39;18&#39;]] data_out = [{k: v for k, v in zip(keys, var)} for var in values] print(data_out) . [1, 2, 3, 4, 5, 6, 7, 8, 9] [{&#39;name&#39;: &#39;john&#39;}, {&#39;name&#39;: &#39;peter&#39;}, {&#39;name&#39;: &#39;mary&#39;}] [{&#39;name&#39;: &#39;john&#39;, &#39;age&#39;: &#39;10&#39;}, {&#39;name&#39;: &#39;foo&#39;, &#39;age&#39;: &#39;24&#39;}, {&#39;name&#39;: &#39;may&#39;, &#39;age&#39;: &#39;18&#39;}] . as you can see in the above examples, list comprehension can also do multiple loops to handle / generate complicatied structure. Although it is extremely useful, try not to make a single line tooooo long such that others (or even yourselves!) can not read. .",
            "url": "https://tikuischan.github.io/fastpages_blog/python/2021/03/20/Python-list-comprehension.html",
            "relUrl": "/python/2021/03/20/Python-list-comprehension.html",
            "date": " • Mar 20, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "立法會投票數據統計",
            "content": "立法會會期快將結束，在新一屆立法會選舉開始前，讓我們利用 python 的 pandas 模組統計一下這一屆（第六屆立法會 2016-2020）立法會的投票紀錄。所有紀錄都可以在立法會公開數據庫找到原始數據（xml）。 . 統計分為三部份： . 立法會整體數據分析 | 議員個人投票統計 | 拫據黨派分析投票傾向 | 立法會的投票傾向視像化 (30/6 updated) | 導入 pandas（主要統計模組）, matplotlib（製作圖表用） 及 numpy（計算時有可能會用到）： . import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns plt.rcParams[&#39;axes.unicode_minus&#39;]=False sns.set_style(&quot;whitegrid&quot;) sns.set_context(&quot;poster&quot;) sns.set(font=&#39;SimHei&#39;, font_scale=1) . IPython 的指令，把圖表直接顯示在 notebook Output 裡 . %matplotlib inline . 把事先整合好的數據導入： . legco_cm = pd.read_csv(&#39;./cm/legco_cm_summary3.csv&#39;) legco_cm.head() . vote-id vote-date vote-time motion mover mover-type result 梁君彥 涂謹申 梁耀忠 ... overall-vote overall-yes overall-no overall-abstain 梁國雄 羅冠聰 姚松炎 劉小麗 梁頌恆 游蕙禎 . 0 20190515001 | 16/05/2019 | 10:34:10 | 《2019年撥款條例草案》 - 全體委員會審議 - 總目21的修正案 (修正案編號1) | 胡志偉 | Member | Negatived | Present | Absent | Yes | ... | 50 | 16 | 33 | 1 | NaN | NaN | NaN | NaN | NaN | NaN | . 1 20190515002 | 16/05/2019 | 10:39:52 | 縮短點名表決響鐘時間的議案 | 李慧琼 | Member | Passed | Present | Absent | No | ... | 51 | 33 | 18 | 0 | NaN | NaN | NaN | NaN | NaN | NaN | . 2 20190515003 | 16/05/2019 | 10:41:42 | 《2019年撥款條例草案》 - 全體委員會審議 - 總目22的修正案 (修正案編號2) | 鄺俊宇 | Member | Negatived | Present | Absent | Yes | ... | 50 | 18 | 31 | 1 | NaN | NaN | NaN | NaN | NaN | NaN | . 3 20190515004 | 16/05/2019 | 10:43:19 | 《2019年撥款條例草案》 - 全體委員會審議 - 總目33的修正案 (修正案編號3) | 朱凱廸 | Member | Negatived | Present | Absent | Yes | ... | 53 | 11 | 38 | 4 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 20190515005 | 16/05/2019 | 10:44:54 | 《2019年撥款條例草案》 - 全體委員會審議 - 總目42的修正案 (修正案編號4) | 陳志全 | Member | Negatived | Present | Absent | Yes | ... | 53 | 8 | 38 | 7 | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 97 columns . 大概看一下數據，有 ID，日期，時間，動議，動議人，動議種類，結果，地區出席（出席沒投票），地區投票，地區贊成票，地區反對票，地區棄權票，功能組別出席（出席沒投票），投票，贊成票，反對票，棄權票，全體立法會出席（出席沒投票），投票，贊成票，反對票，棄權票，然後就是各個議員在各個動議的投票狀態。 . legco_cm.dtypes . vote-id object vote-date object vote-time object motion object mover object ... 羅冠聰 object 姚松炎 object 劉小麗 object 梁頌恆 object 游蕙禎 object Length: 97, dtype: object . legco_cm.columns . Index([&#39;vote-id&#39;, &#39;vote-date&#39;, &#39;vote-time&#39;, &#39;motion&#39;, &#39;mover&#39;, &#39;mover-type&#39;, &#39;result&#39;, &#39;梁君彥&#39;, &#39;涂謹申&#39;, &#39;梁耀忠&#39;, &#39;石禮謙&#39;, &#39;張宇人&#39;, &#39;李國麟&#39;, &#39;林健鋒&#39;, &#39;黃定光&#39;, &#39;李慧琼&#39;, &#39;陳克勤&#39;, &#39;陳健波&#39;, &#39;梁美芬&#39;, &#39;黃國健&#39;, &#39;葉劉淑儀&#39;, &#39;謝偉俊&#39;, &#39;毛孟靜&#39;, &#39;田北辰&#39;, &#39;何俊賢&#39;, &#39;易志明&#39;, &#39;胡志偉&#39;, &#39;姚思榮&#39;, &#39;馬逢國&#39;, &#39;莫乃光&#39;, &#39;陳志全&#39;, &#39;陳恒鑌&#39;, &#39;梁志祥&#39;, &#39;梁繼昌&#39;, &#39;麥美娟&#39;, &#39;郭家麒&#39;, &#39;郭偉强&#39;, &#39;郭榮鏗&#39;, &#39;張華峰&#39;, &#39;張超雄&#39;, &#39;黃碧雲&#39;, &#39;葉建源&#39;, &#39;葛珮帆&#39;, &#39;廖長江&#39;, &#39;潘兆平&#39;, &#39;蔣麗芸&#39;, &#39;盧偉國&#39;, &#39;鍾國斌&#39;, &#39;楊岳橋&#39;, &#39;尹兆堅&#39;, &#39;朱凱廸&#39;, &#39;吳永嘉&#39;, &#39;何君堯&#39;, &#39;何啟明&#39;, &#39;林卓廷&#39;, &#39;周浩鼎&#39;, &#39;邵家輝&#39;, &#39;邵家臻&#39;, &#39;柯創盛&#39;, &#39;容海恩&#39;, &#39;陳沛然&#39;, &#39;陳振英&#39;, &#39;陳淑莊&#39;, &#39;張國鈞&#39;, &#39;許智峯&#39;, &#39;陸頌雄&#39;, &#39;劉國勳&#39;, &#39;劉業強&#39;, &#39;鄭松泰&#39;, &#39;鄺俊宇&#39;, &#39;譚文豪&#39;, &#39;范國威&#39;, &#39;區諾軒&#39;, &#39;鄭泳舜&#39;, &#39;謝偉銓&#39;, &#39;陳凱欣&#39;, &#39;geo-present&#39;, &#39;geo-vote&#39;, &#39;geo-yes&#39;, &#39;geo-no&#39;, &#39;geo-abstain&#39;, &#39;func-present&#39;, &#39;func-vote&#39;, &#39;func-yes&#39;, &#39;func-no&#39;, &#39;func-abstain&#39;, &#39;overall-present&#39;, &#39;overall-vote&#39;, &#39;overall-yes&#39;, &#39;overall-no&#39;, &#39;overall-abstain&#39;, &#39;梁國雄&#39;, &#39;羅冠聰&#39;, &#39;姚松炎&#39;, &#39;劉小麗&#39;, &#39;梁頌恆&#39;, &#39;游蕙禎&#39;], dtype=&#39;object&#39;) . 1. &#25972;&#39636;&#25237;&#31080;&#32113;&#35336; . - &#21443;&#33287;&#25237;&#31080;&#20154;&#25976;&#32113;&#35336; . 平均一次投票參與人數： . legco_cm[&#39;overall-present&#39;].mean() . 53.443120260021665 . 平均投票人數：（主席不投票、當中亦有連棄權也不選的情況） . legco_cm[&#39;overall-vote&#39;].mean() . 52.24918743228602 . - &#21205;&#35696;&#36890;&#36942;&#29575; . passed = (legco_cm[&#39;result&#39;] == &#39;Passed&#39;).sum() print(f&#39;紀錄動議總數: {legco_cm.shape[0]}&#39;) print(f&#39;獲通過的動議: {passed}&#39;) print(f&#39;總通過率: {passed / legco_cm.shape[0] * 100:.1f}%&#39;) labels = [&#39;Passed&#39;, &#39;Negatived&#39;] size = [passed, legco_cm.shape[0] - passed] fig1, ax1 = plt.subplots() ax1.pie(size, labels=labels, autopct=&#39;%1.1f%%&#39;, startangle=90, textprops={&#39;fontsize&#39;: 20}) ax1.axis(&#39;equal&#39;) ax1.set_title(&#39;For all 923 Motions:&#39;) plt.rcParams[&#39;figure.figsize&#39;] = (10, 10) plt.show() . 紀錄動議總數: 923 獲通過的動議: 271 總通過率: 29.4% . 紀錄中把動議分為立法會成員及公務員兩種。 . legco_cm[&#39;mover-type&#39;].unique() . array([&#39;Member&#39;, &#39;Public Officer&#39;], dtype=object) . member_motion = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;)].result.count() member_motion_passed = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Passed&#39;)].result.count() gov_motion = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Public Officer&#39;)].result.count() gov_motion_passed = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Public Officer&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Passed&#39;)].result.count() print(f&quot;由議員提出的議案：{member_motion}，獲得通過：{member_motion_passed}，通過率：{member_motion_passed / member_motion:.3f}&quot;) print(f&quot;由政府提出的議案：{gov_motion}，獲得通過：{gov_motion_passed}，通過率：{gov_motion_passed / gov_motion:.3f}&quot;) # Bar Chart: negatived = [(gov_motion - gov_motion_passed), (member_motion - member_motion_passed)] passed = [gov_motion_passed, member_motion_passed] p1 = plt.bar([0, 1], passed, 0.35, alpha=0.5) p2 = plt.bar([0, 1], negatived, 0.35, bottom=passed, alpha=0.5) plt.ylabel(&#39;Number of Motions&#39;) plt.title(&#39;Number of Motions by Member/Public Officer&#39;) plt.xticks([0, 1], (&#39;Public Officer&#39;, &#39;Members&#39;)) plt.legend((p1[0], p2[0]), (&#39;Passed&#39;, &#39;Negatived&#39;)) plt.rcParams[&#39;figure.figsize&#39;] = (10, 10) plt.show() print(&quot;Alternative: Pie charts&quot;) labels = [&#39;Passed&#39;, &#39;Negatived&#39;] member_size = [member_motion_passed, member_motion - member_motion_passed] gov_size = [gov_motion_passed, gov_motion - gov_motion_passed] fig, ax = plt.subplots(1, 2) fig.subplots_adjust(hspace=0.5, wspace=0.5) ax[1].pie(member_size, labels=labels, autopct=&#39;%1.1f%%&#39;, startangle=90, textprops={&#39;fontsize&#39;: 20}) ax[1].set_title(&#39;Motions by Members&#39;) ax[0].pie(gov_size, labels=labels, autopct=&#39;%1.1f%%&#39;, startangle=0, textprops={&#39;fontsize&#39;: 20}) ax[0].set_title(&#39;Motions by Goverment&#39;) plt.show() . 由議員提出的議案：789，獲得通過：138，通過率：0.175 由政府提出的議案：134，獲得通過：133，通過率：0.993 . Alternative: Pie charts . &#21807;&#19968;&#34987;&#31435;&#27861;&#26371;&#21542;&#27770;&#30001;&#25919;&#24220;&#25552;&#20986;&#30340;&#35696;&#26696; . 投票結果： . oops = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Public Officer&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Negatived&#39;)] oops[[&#39;motion&#39;, &#39;mover&#39;, &#39;overall-vote&#39;, &#39;overall-yes&#39;, &#39;overall-no&#39;]] . motion mover overall-vote overall-yes overall-no . 263 《2017年應課稅品(修訂)條例草案》 - 全體委員會審議 - 食物及衞生局局長的第二組修正... | 食物及衞生局局長 | 41 | 12 | 29 | . 有關動議及發言： https://www.info.gov.hk/gia/general/201802/07/P2018020700996.htm . - &#21205;&#35696;&#25237;&#31080;&#26178;&#38291;&#20998;&#24598; . legco_cm[&#39;vote-date&#39;] = pd.to_datetime(legco_cm[&#39;vote-date&#39;]) legco_cm[&#39;vote-time&#39;] = pd.to_datetime(legco_cm[&#39;vote-time&#39;]).dt.time legco_cm[[&#39;vote-id&#39;, &#39;vote-date&#39;, &#39;vote-time&#39;, &#39;mover-type&#39;, &#39;result&#39;]].head() . vote-id vote-date vote-time mover-type result . 0 20190515001 | 2019-05-16 | 10:34:10 | Member | Negatived | . 1 20190515002 | 2019-05-16 | 10:39:52 | Member | Passed | . 2 20190515003 | 2019-05-16 | 10:41:42 | Member | Negatived | . 3 20190515004 | 2019-05-16 | 10:43:19 | Member | Negatived | . 4 20190515005 | 2019-05-16 | 10:44:54 | Member | Negatived | . 首先看一下政府動議按月份分怖 . legco_cm[&#39;year-month&#39;] = legco_cm[&#39;vote-date&#39;].dt.to_period(&#39;M&#39;) legco_cm[legco_cm[&#39;mover-type&#39;] == &#39;Public Officer&#39;].groupby(&#39;year-month&#39;).size().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12164b710&gt; . 整合起來再看一次... . legco_cm[&#39;month&#39;] = legco_cm[&#39;vote-date&#39;].dt.month legco_cm[legco_cm[&#39;mover-type&#39;] == &#39;Public Officer&#39;].groupby(&#39;month&#39;).size().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12175b690&gt; . 看來大部份政府議案都在5、6、10、11月提出。 . &#28982;&#24460;&#30475;&#19968;&#19979;&#25919;&#24220;&#20027;&#35201;&#26159;&#22312;&#36913;&#24190;&#21205;&#35696;&#25237;&#31080; . legco_cm[&#39;day-of-week&#39;] = legco_cm[&#39;vote-date&#39;].dt.dayofweek legco_cm[legco_cm[&#39;mover-type&#39;] == &#39;Public Officer&#39;].groupby(&#39;day-of-week&#39;).size().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x121929650&gt; . 主要是週三、週四，其次是週五。 . &#20877;&#20358;&#30475;&#19968;&#19979;&#25237;&#31080;&#26178;&#38291;&#20998;&#24598; . legco_cm[&#39;hour&#39;] = pd.to_datetime(legco_cm[&#39;vote-time&#39;].astype(&#39;str&#39;)).dt.hour legco_cm[legco_cm[&#39;mover-type&#39;] == &#39;Public Officer&#39;].groupby(&#39;hour&#39;).size().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x121b44550&gt; . 看來午飯 （12 時）及晚飯/下班前（18 時）的動議數字比較高.... . - &#31435;&#27861;&#26371;&#20998;&#32068;&#25237;&#31080;&#30340;&#24433;&#38911;&#21147; . 在前面的統計中我們看到由議員提出的動議通過率只有 30% 左右，在這部份的統計我們來看一下分組投票對整體通過率的影響有多大。 . 算一下總體投票贊成高於反對但是被否決的動： . legco_cm[(legco_cm[&#39;overall-yes&#39;] &gt; legco_cm[&#39;overall-no&#39;]) &amp; (legco_cm[&#39;result&#39;] == &#39;Negatived&#39;)].shape[0] . 101 . 佔由議員動議而被否決的 15.51 % . 2. &#35696;&#21729;&#21205;&#35696;&#21450;&#25237;&#31080;&#25976;&#25818;&#32113;&#35336; . 梁頌恆及游蕙禎沒有參與過立法會會議就被取消資格。 . members = [&#39;梁君彥&#39;, &#39;涂謹申&#39;, &#39;梁耀忠&#39;, &#39;石禮謙&#39;, &#39;張宇人&#39;, &#39;李國麟&#39;, &#39;林健鋒&#39;, &#39;黃定光&#39;, &#39;李慧琼&#39;, &#39;陳克勤&#39;, &#39;陳健波&#39;, &#39;梁美芬&#39;, &#39;黃國健&#39;, &#39;葉劉淑儀&#39;, &#39;謝偉俊&#39;, &#39;毛孟靜&#39;, &#39;田北辰&#39;, &#39;何俊賢&#39;, &#39;易志明&#39;, &#39;胡志偉&#39;, &#39;姚思榮&#39;, &#39;馬逢國&#39;, &#39;莫乃光&#39;, &#39;陳志全&#39;, &#39;陳恒鑌&#39;, &#39;梁志祥&#39;, &#39;梁繼昌&#39;, &#39;麥美娟&#39;, &#39;郭家麒&#39;, &#39;郭偉强&#39;, &#39;郭榮鏗&#39;, &#39;張華峰&#39;, &#39;張超雄&#39;, &#39;黃碧雲&#39;, &#39;葉建源&#39;, &#39;葛珮帆&#39;, &#39;廖長江&#39;, &#39;潘兆平&#39;, &#39;蔣麗芸&#39;, &#39;盧偉國&#39;, &#39;鍾國斌&#39;, &#39;楊岳橋&#39;, &#39;尹兆堅&#39;, &#39;朱凱廸&#39;, &#39;吳永嘉&#39;, &#39;何君堯&#39;, &#39;何啟明&#39;, &#39;林卓廷&#39;, &#39;周浩鼎&#39;, &#39;邵家輝&#39;, &#39;邵家臻&#39;, &#39;柯創盛&#39;, &#39;容海恩&#39;, &#39;陳沛然&#39;, &#39;陳振英&#39;, &#39;陳淑莊&#39;, &#39;張國鈞&#39;, &#39;許智峯&#39;, &#39;陸頌雄&#39;, &#39;劉國勳&#39;, &#39;劉業強&#39;, &#39;鄭松泰&#39;, &#39;鄺俊宇&#39;, &#39;譚文豪&#39;, &#39;范國威&#39;, &#39;區諾軒&#39;, &#39;鄭泳舜&#39;, &#39;謝偉銓&#39;, &#39;陳凱欣&#39;, &#39;梁國雄&#39;, &#39;羅冠聰&#39;, &#39;姚松炎&#39;, &#39;劉小麗&#39;] # &#39;梁頌恆&#39;, &#39;游蕙禎&#39; . - &#35696;&#21729;&#30340;&#21205;&#35696;&#25976;&#37327;&#32113;&#35336; . &#26366;&#32147;&#21205;&#35696;&#30340;&#35696;&#21729;&#65306; . print(legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;)].mover.unique()) print(len(legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;)].mover.unique()), &#39;人&#39;) . [&#39;胡志偉&#39; &#39;李慧琼&#39; &#39;鄺俊宇&#39; &#39;朱凱廸&#39; &#39;陳志全&#39; &#39;區諾軒&#39; &#39;范國威&#39; &#39;林卓廷&#39; &#39;譚文豪&#39; &#39;郭家麒&#39; &#39;楊岳橋&#39; &#39;鄭松泰&#39; &#39;毛孟靜&#39; &#39;葉建源&#39; &#39;許智峯&#39; &#39;尹兆堅&#39; &#39;李國麟&#39; &#39;梁美芬&#39; &#39;梁志祥&#39; &#39;郭偉强&#39; &#39;吳永嘉&#39; &#39;蔣麗芸&#39; &#39;張超雄&#39; &#39;劉小麗&#39; &#39;田北辰&#39; &#39;黃國健&#39; &#39;葉劉淑儀&#39; &#39;張國鈞&#39; &#39;陳恒鑌&#39; &#39;陳克勤&#39; &#39;梁繼昌&#39; &#39;涂謹申&#39; &#39;葛珮帆&#39; &#39;邵家輝&#39; &#39;易志明&#39; &#39;謝偉俊&#39; &#39;陳淑莊&#39; &#39;郭榮鏗&#39; &#39;梁耀忠&#39; &#39;邵家臻&#39; &#39;盧偉國&#39; &#39;謝偉銓&#39; &#39;麥美娟&#39; &#39;柯創盛&#39; &#39;何君堯&#39; &#39;莫乃光&#39; &#39;潘兆平&#39; &#39;周浩鼎&#39; &#39;黃碧雲&#39; &#39;廖長江&#39; &#39;何啟明&#39; &#39;陸頌雄&#39; &#39;容海恩&#39; &#39;陳凱欣&#39; &#39;陳沛然&#39; &#39;姚松炎&#39; &#39;林健鋒&#39; &#39;劉國勳&#39; &#39;張華峰&#39; &#39;羅冠聰&#39; &#39;梁國雄&#39; &#39;張宇人&#39; &#39;姚思榮&#39; &#39;陳健波&#39; &#39;黃定光&#39; &#39;鄭泳舜&#39; &#39;馬逢國&#39; &#39;何俊賢&#39;] 68 人 . 把動議數、通過 / 否決數作成一個圖表 . move_count = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;)].groupby(&#39;mover&#39;).size().reset_index(name=&#39;counts&#39;).sort_values(&#39;counts&#39;) move_pass = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Passed&#39;)].groupby(&#39;mover&#39;).size().reset_index(name=&#39;passed&#39;) move_neg = legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Negatived&#39;)].groupby(&#39;mover&#39;).size().reset_index(name=&#39;negatived&#39;) move_count = pd.merge(move_count, move_pass, on=&#39;mover&#39;, how=&#39;outer&#39;) move_count = pd.merge(move_count, move_neg, on=&#39;mover&#39;, how=&#39;outer&#39;) move_count[[&#39;passed&#39;, &#39;negatived&#39;]] = move_count[[&#39;passed&#39;, &#39;negatived&#39;]].fillna(0).astype(&#39;int&#39;) plt.rcParams[&#39;figure.figsize&#39;] = (10, 20) inp = np.arange(move_count.shape[0]) p1 = plt.barh(inp, move_count[&#39;passed&#39;], 1, alpha=0.7) p2 = plt.barh(inp, move_count[&#39;negatived&#39;], 1, left=move_count[&#39;passed&#39;], alpha=0.7) plt.title(&#39;Number of Motions by Member&#39;) plt.yticks(inp, move_count[&#39;mover&#39;]) plt.legend((p1[0], p2[0]), (&#39;Passed&#39;, &#39;Negatived&#39;), loc=&#39;center right&#39;) plt.show() . 作圖的話 73 名議員太長不太好看... . - &#35696;&#21729;&#25237;&#31080;&#29575;&#32113;&#35336; . df = pd.DataFrame([legco_cm.groupby(member).size() for member in members]) df.fillna(0).astype(&#39;int&#39;) df[&#39;member&#39;] = members df[&#39;vote_num&#39;] = df.fillna(0)[&#39;Yes&#39;] + df.fillna(0)[&#39;No&#39;] + df.fillna(0)[&#39;Abstain&#39;] df[&#39;vote_rate&#39;] = df[&#39;vote_num&#39;] / (df[&#39;vote_num&#39;] + df.fillna(0)[&#39;Present&#39;] + df.fillna(0)[&#39;Absent&#39;]) * 100 df.sort_values(&#39;vote_rate&#39;).head(10).fillna(0) . Absent Present Abstain No Yes member vote_num vote_rate . 0 24.0 | 899.0 | 0.0 | 0.0 | 0.0 | 梁君彥 | 0.0 | 0.000000 | . 71 189.0 | 0.0 | 7.0 | 49.0 | 62.0 | 姚松炎 | 118.0 | 38.436482 | . 30 472.0 | 1.0 | 22.0 | 112.0 | 316.0 | 郭榮鏗 | 450.0 | 48.754063 | . 3 449.0 | 3.0 | 8.0 | 333.0 | 130.0 | 石禮謙 | 471.0 | 51.029252 | . 1 420.0 | 19.0 | 46.0 | 95.0 | 343.0 | 涂謹申 | 484.0 | 52.437703 | . 60 413.0 | 0.0 | 11.0 | 375.0 | 124.0 | 劉業強 | 510.0 | 55.254605 | . 16 399.0 | 0.0 | 37.0 | 309.0 | 178.0 | 田北辰 | 524.0 | 56.771398 | . 14 380.0 | 2.0 | 18.0 | 329.0 | 194.0 | 謝偉俊 | 541.0 | 58.613218 | . 40 378.0 | 0.0 | 27.0 | 341.0 | 177.0 | 鍾國斌 | 545.0 | 59.046587 | . 5 377.0 | 1.0 | 50.0 | 118.0 | 377.0 | 李國麟 | 545.0 | 59.046587 | . - &#25226;&#25237;&#31080;&#32113;&#35336;&#21644;&#21205;&#35696;&#32113;&#35336;&#32080;&#21512;&#23601;&#25104;&#28858;&#35696;&#21729;&#30340;&#19968;&#20491;&#31777;&#21934;&#25104;&#31309;&#34920; . member_summary = pd.merge(move_count, df[[&#39;Absent&#39;, &#39;Present&#39;, &#39;vote_num&#39;, &#39;vote_rate&#39;, &#39;member&#39;]], right_on=&#39;member&#39;, left_on=&#39;mover&#39;, how=&#39;outer&#39;) member_summary = member_summary.drop(columns=[&#39;mover&#39;]).fillna(0) member_summary.head(10) . counts passed negatived Absent Present vote_num vote_rate member . 0 1.0 | 0.0 | 1.0 | 115.0 | 0.0 | 808.0 | 87.540628 | 何俊賢 | . 1 1.0 | 1.0 | 0.0 | 42.0 | 0.0 | 494.0 | 92.164179 | 鄭泳舜 | . 2 1.0 | 1.0 | 0.0 | 4.0 | 0.0 | 313.0 | 98.738170 | 陳凱欣 | . 3 1.0 | 1.0 | 0.0 | 217.0 | 1.0 | 705.0 | 76.381365 | 張宇人 | . 4 1.0 | 0.0 | 1.0 | 50.0 | 2.0 | 871.0 | 94.366197 | 潘兆平 | . 5 2.0 | 0.0 | 2.0 | 212.0 | 0.0 | 711.0 | 77.031419 | 何君堯 | . 6 2.0 | 1.0 | 1.0 | 130.0 | 1.0 | 792.0 | 85.807151 | 劉國勳 | . 7 2.0 | 1.0 | 1.0 | 328.0 | 0.0 | 595.0 | 64.463705 | 馬逢國 | . 8 2.0 | 2.0 | 0.0 | 339.0 | 0.0 | 584.0 | 63.271939 | 陳沛然 | . 9 2.0 | 1.0 | 1.0 | 64.0 | 1.0 | 858.0 | 92.957746 | 陳健波 | . - &#20491;&#21029;&#35696;&#21729;&#30340;&#34920;&#29694;&#20998;&#26512; . &#21205;&#35696;&#23436;&#20840;&#27794;&#26377;&#29554;&#36890;&#36942;&#30340;&#35696;&#21729; . loser = [] for name in legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;)].mover.unique(): if name not in legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Passed&#39;)].mover.unique(): loser.append(name) print(&#39;Loser name list: &#39;, loser) print(len(loser), &#39;人&#39;) . Loser name list: [&#39;朱凱廸&#39;, &#39;陳志全&#39;, &#39;范國威&#39;, &#39;林卓廷&#39;, &#39;楊岳橋&#39;, &#39;鄭松泰&#39;, &#39;許智峯&#39;, &#39;劉小麗&#39;, &#39;涂謹申&#39;, &#39;柯創盛&#39;, &#39;何君堯&#39;, &#39;潘兆平&#39;, &#39;姚松炎&#39;, &#39;羅冠聰&#39;, &#39;梁國雄&#39;, &#39;何俊賢&#39;] 16 人 . member_summary[member_summary[&#39;member&#39;].isin(loser)] . counts passed negatived Absent Present vote_num vote_rate member . 0 1.0 | 0.0 | 1.0 | 115.0 | 0.0 | 808.0 | 87.540628 | 何俊賢 | . 4 1.0 | 0.0 | 1.0 | 50.0 | 2.0 | 871.0 | 94.366197 | 潘兆平 | . 5 2.0 | 0.0 | 2.0 | 212.0 | 0.0 | 711.0 | 77.031419 | 何君堯 | . 13 3.0 | 0.0 | 3.0 | 56.0 | 0.0 | 867.0 | 93.932828 | 柯創盛 | . 15 3.0 | 0.0 | 3.0 | 189.0 | 0.0 | 118.0 | 38.436482 | 姚松炎 | . 42 8.0 | 0.0 | 8.0 | 88.0 | 0.0 | 835.0 | 90.465872 | 鄭松泰 | . 46 11.0 | 0.0 | 11.0 | 52.0 | 0.0 | 364.0 | 87.500000 | 范國威 | . 49 12.0 | 0.0 | 12.0 | 349.0 | 7.0 | 567.0 | 61.430119 | 許智峯 | . 51 15.0 | 0.0 | 15.0 | 220.0 | 6.0 | 697.0 | 75.514626 | 林卓廷 | . 54 17.0 | 0.0 | 17.0 | 249.0 | 7.0 | 667.0 | 72.264355 | 楊岳橋 | . 56 17.0 | 0.0 | 17.0 | 107.0 | 0.0 | 200.0 | 65.146580 | 羅冠聰 | . 58 20.0 | 0.0 | 20.0 | 420.0 | 19.0 | 484.0 | 52.437703 | 涂謹申 | . 61 25.0 | 0.0 | 25.0 | 62.0 | 2.0 | 243.0 | 79.153094 | 劉小麗 | . 62 26.0 | 0.0 | 26.0 | 77.0 | 0.0 | 230.0 | 74.918567 | 梁國雄 | . 66 47.0 | 0.0 | 47.0 | 206.0 | 2.0 | 715.0 | 77.464789 | 朱凱廸 | . 67 150.0 | 0.0 | 150.0 | 70.0 | 5.0 | 848.0 | 91.874323 | 陳志全 | . &#26368;&#23569;&#26377;&#19968;&#20491;&#21205;&#35696;&#29554;&#36890;&#36942;&#30340;&#35696;&#21729; . print(legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Passed&#39;)].mover.unique()) print(len(legco_cm[(legco_cm[&#39;mover-type&#39;]==&#39;Member&#39;) &amp; (legco_cm[&#39;result&#39;]==&#39;Passed&#39;)].mover.unique()), &#39;人&#39;) . [&#39;李慧琼&#39; &#39;李國麟&#39; &#39;梁美芬&#39; &#39;郭偉强&#39; &#39;黃國健&#39; &#39;葉劉淑儀&#39; &#39;張國鈞&#39; &#39;陳恒鑌&#39; &#39;陳克勤&#39; &#39;葛珮帆&#39; &#39;鄺俊宇&#39; &#39;胡志偉&#39; &#39;邵家輝&#39; &#39;易志明&#39; &#39;張超雄&#39; &#39;梁志祥&#39; &#39;梁耀忠&#39; &#39;郭家麒&#39; &#39;區諾軒&#39; &#39;邵家臻&#39; &#39;莫乃光&#39; &#39;周浩鼎&#39; &#39;謝偉俊&#39; &#39;廖長江&#39; &#39;盧偉國&#39; &#39;何啟明&#39; &#39;陸頌雄&#39; &#39;麥美娟&#39; &#39;葉建源&#39; &#39;梁繼昌&#39; &#39;容海恩&#39; &#39;陳凱欣&#39; &#39;謝偉銓&#39; &#39;陳沛然&#39; &#39;黃碧雲&#39; &#39;尹兆堅&#39; &#39;張宇人&#39; &#39;鄭泳舜&#39; &#39;陳淑莊&#39; &#39;劉國勳&#39; &#39;陳健波&#39; &#39;黃定光&#39; &#39;張華峰&#39; &#39;毛孟靜&#39; &#39;譚文豪&#39; &#39;田北辰&#39; &#39;姚思榮&#39; &#39;蔣麗芸&#39; &#39;吳永嘉&#39; &#39;林健鋒&#39; &#39;馬逢國&#39; &#39;郭榮鏗&#39;] 52 人 . member_summary[~member_summary[&#39;member&#39;].isin(loser)] . counts passed negatived Absent Present vote_num vote_rate member . 1 1.0 | 1.0 | 0.0 | 42.0 | 0.0 | 494.0 | 92.164179 | 鄭泳舜 | . 2 1.0 | 1.0 | 0.0 | 4.0 | 0.0 | 313.0 | 98.738170 | 陳凱欣 | . 3 1.0 | 1.0 | 0.0 | 217.0 | 1.0 | 705.0 | 76.381365 | 張宇人 | . 6 2.0 | 1.0 | 1.0 | 130.0 | 1.0 | 792.0 | 85.807151 | 劉國勳 | . 7 2.0 | 1.0 | 1.0 | 328.0 | 0.0 | 595.0 | 64.463705 | 馬逢國 | . 8 2.0 | 2.0 | 0.0 | 339.0 | 0.0 | 584.0 | 63.271939 | 陳沛然 | . 9 2.0 | 1.0 | 1.0 | 64.0 | 1.0 | 858.0 | 92.957746 | 陳健波 | . 10 3.0 | 2.0 | 1.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 姚思榮 | . 11 3.0 | 3.0 | 0.0 | 182.0 | 1.0 | 740.0 | 80.173348 | 廖長江 | . 12 3.0 | 2.0 | 1.0 | 72.0 | 0.0 | 851.0 | 92.199350 | 張國鈞 | . 14 3.0 | 2.0 | 1.0 | 241.0 | 0.0 | 682.0 | 73.889491 | 林健鋒 | . 16 3.0 | 3.0 | 0.0 | 70.0 | 0.0 | 853.0 | 92.416035 | 易志明 | . 17 4.0 | 3.0 | 1.0 | 280.0 | 0.0 | 643.0 | 69.664139 | 葉劉淑儀 | . 18 4.0 | 3.0 | 1.0 | 94.0 | 0.0 | 829.0 | 89.815818 | 容海恩 | . 19 4.0 | 2.0 | 2.0 | 60.0 | 1.0 | 862.0 | 93.391116 | 黃定光 | . 20 4.0 | 2.0 | 2.0 | 98.0 | 7.0 | 818.0 | 88.624052 | 黃國健 | . 21 4.0 | 1.0 | 3.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 周浩鼎 | . 22 4.0 | 2.0 | 2.0 | 377.0 | 1.0 | 545.0 | 59.046587 | 李國麟 | . 23 5.0 | 3.0 | 2.0 | 145.0 | 0.0 | 778.0 | 84.290358 | 陳恒鑌 | . 24 5.0 | 2.0 | 3.0 | 56.0 | 6.0 | 861.0 | 93.282774 | 郭偉强 | . 25 5.0 | 4.0 | 1.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 陳克勤 | . 26 5.0 | 1.0 | 4.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 邵家輝 | . 27 5.0 | 1.0 | 4.0 | 380.0 | 2.0 | 541.0 | 58.613218 | 謝偉俊 | . 28 5.0 | 2.0 | 3.0 | 229.0 | 0.0 | 694.0 | 75.189599 | 蔣麗芸 | . 29 5.0 | 1.0 | 4.0 | 399.0 | 0.0 | 524.0 | 56.771398 | 田北辰 | . 30 5.0 | 1.0 | 4.0 | 87.0 | 0.0 | 836.0 | 90.574215 | 梁志祥 | . 31 6.0 | 3.0 | 3.0 | 179.0 | 0.0 | 744.0 | 80.606717 | 梁美芬 | . 32 6.0 | 3.0 | 3.0 | 317.0 | 0.0 | 606.0 | 65.655471 | 邵家臻 | . 33 6.0 | 5.0 | 1.0 | 82.0 | 6.0 | 835.0 | 90.465872 | 麥美娟 | . 34 6.0 | 2.0 | 4.0 | 81.0 | 6.0 | 836.0 | 90.574215 | 陸頌雄 | . 35 6.0 | 2.0 | 4.0 | 32.0 | 0.0 | 504.0 | 94.029851 | 謝偉銓 | . 36 6.0 | 5.0 | 1.0 | 200.0 | 0.0 | 723.0 | 78.331528 | 吳永嘉 | . 37 7.0 | 1.0 | 6.0 | 304.0 | 0.0 | 619.0 | 67.063922 | 葉建源 | . 38 7.0 | 4.0 | 3.0 | 134.0 | 1.0 | 788.0 | 85.373781 | 張華峰 | . 39 7.0 | 3.0 | 4.0 | 222.0 | 0.0 | 701.0 | 75.947996 | 莫乃光 | . 40 7.0 | 6.0 | 1.0 | 36.0 | 0.0 | 887.0 | 96.099675 | 葛珮帆 | . 41 8.0 | 1.0 | 7.0 | 267.0 | 5.0 | 651.0 | 70.530878 | 鄺俊宇 | . 43 9.0 | 1.0 | 8.0 | 472.0 | 1.0 | 450.0 | 48.754063 | 郭榮鏗 | . 44 9.0 | 7.0 | 2.0 | 38.0 | 6.0 | 832.0 | 94.977169 | 何啟明 | . 45 10.0 | 4.0 | 6.0 | 286.0 | 0.0 | 637.0 | 69.014085 | 梁耀忠 | . 47 11.0 | 1.0 | 10.0 | 141.0 | 26.0 | 756.0 | 81.906826 | 譚文豪 | . 48 11.0 | 7.0 | 4.0 | 11.0 | 0.0 | 912.0 | 98.808234 | 盧偉國 | . 50 13.0 | 2.0 | 11.0 | 326.0 | 15.0 | 582.0 | 63.055255 | 陳淑莊 | . 52 15.0 | 1.0 | 14.0 | 276.0 | 15.0 | 632.0 | 68.472373 | 黃碧雲 | . 53 17.0 | 14.0 | 3.0 | 93.0 | 24.0 | 806.0 | 87.323944 | 李慧琼 | . 55 17.0 | 5.0 | 12.0 | 303.0 | 5.0 | 615.0 | 66.630553 | 尹兆堅 | . 57 19.0 | 4.0 | 15.0 | 302.0 | 3.0 | 618.0 | 66.955580 | 梁繼昌 | . 59 21.0 | 2.0 | 19.0 | 269.0 | 6.0 | 648.0 | 70.205850 | 胡志偉 | . 60 21.0 | 1.0 | 20.0 | 203.0 | 0.0 | 720.0 | 78.006501 | 毛孟靜 | . 63 28.0 | 2.0 | 26.0 | 67.0 | 1.0 | 348.0 | 83.653846 | 區諾軒 | . 64 32.0 | 1.0 | 31.0 | 276.0 | 6.0 | 641.0 | 69.447454 | 郭家麒 | . 65 36.0 | 3.0 | 33.0 | 282.0 | 0.0 | 641.0 | 69.447454 | 張超雄 | . 68 0.0 | 0.0 | 0.0 | 24.0 | 899.0 | 0.0 | 0.000000 | 梁君彥 | . 69 0.0 | 0.0 | 0.0 | 449.0 | 3.0 | 471.0 | 51.029252 | 石禮謙 | . 70 0.0 | 0.0 | 0.0 | 378.0 | 0.0 | 545.0 | 59.046587 | 鍾國斌 | . 71 0.0 | 0.0 | 0.0 | 23.0 | 3.0 | 897.0 | 97.183099 | 陳振英 | . 72 0.0 | 0.0 | 0.0 | 413.0 | 0.0 | 510.0 | 55.254605 | 劉業強 | . &#23436;&#20840;&#27794;&#26377;&#21205;&#35696;&#30340;&#35696;&#21729; . member_summary[member_summary[&#39;counts&#39;] == 0].sort_values([&#39;Absent&#39;], ascending=False) . counts passed negatived Absent Present vote_num vote_rate member . 69 0.0 | 0.0 | 0.0 | 449.0 | 3.0 | 471.0 | 51.029252 | 石禮謙 | . 72 0.0 | 0.0 | 0.0 | 413.0 | 0.0 | 510.0 | 55.254605 | 劉業強 | . 70 0.0 | 0.0 | 0.0 | 378.0 | 0.0 | 545.0 | 59.046587 | 鍾國斌 | . 68 0.0 | 0.0 | 0.0 | 24.0 | 899.0 | 0.0 | 0.000000 | 梁君彥 | . 71 0.0 | 0.0 | 0.0 | 23.0 | 3.0 | 897.0 | 97.183099 | 陳振英 | . 能看得出 劉業強、鍾國斌、石禮謙、不但完全沒動議，連投票率也不到 60% . &#25237;&#31080;&#29575;&#26368;&#39640;&#30340; 10 &#21517;&#35696;&#21729;&#65306; . member_summary.sort_values(&#39;vote_num&#39;, ascending=False).head(10) . counts passed negatived Absent Present vote_num vote_rate member . 48 11.0 | 7.0 | 4.0 | 11.0 | 0.0 | 912.0 | 98.808234 | 盧偉國 | . 71 0.0 | 0.0 | 0.0 | 23.0 | 3.0 | 897.0 | 97.183099 | 陳振英 | . 40 7.0 | 6.0 | 1.0 | 36.0 | 0.0 | 887.0 | 96.099675 | 葛珮帆 | . 4 1.0 | 0.0 | 1.0 | 50.0 | 2.0 | 871.0 | 94.366197 | 潘兆平 | . 13 3.0 | 0.0 | 3.0 | 56.0 | 0.0 | 867.0 | 93.932828 | 柯創盛 | . 25 5.0 | 4.0 | 1.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 陳克勤 | . 10 3.0 | 2.0 | 1.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 姚思榮 | . 19 4.0 | 2.0 | 2.0 | 60.0 | 1.0 | 862.0 | 93.391116 | 黃定光 | . 21 4.0 | 1.0 | 3.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 周浩鼎 | . 26 5.0 | 1.0 | 4.0 | 61.0 | 0.0 | 862.0 | 93.391116 | 邵家輝 | . &#25237;&#31080;&#29575;&#26368;&#20302;&#30340; 10 &#21517;&#35696;&#21729;&#65306; . member_summary.sort_values(&#39;vote_rate&#39;, ascending=True).head(10) . counts passed negatived Absent Present vote_num vote_rate member . 68 0.0 | 0.0 | 0.0 | 24.0 | 899.0 | 0.0 | 0.000000 | 梁君彥 | . 15 3.0 | 0.0 | 3.0 | 189.0 | 0.0 | 118.0 | 38.436482 | 姚松炎 | . 43 9.0 | 1.0 | 8.0 | 472.0 | 1.0 | 450.0 | 48.754063 | 郭榮鏗 | . 69 0.0 | 0.0 | 0.0 | 449.0 | 3.0 | 471.0 | 51.029252 | 石禮謙 | . 58 20.0 | 0.0 | 20.0 | 420.0 | 19.0 | 484.0 | 52.437703 | 涂謹申 | . 72 0.0 | 0.0 | 0.0 | 413.0 | 0.0 | 510.0 | 55.254605 | 劉業強 | . 29 5.0 | 1.0 | 4.0 | 399.0 | 0.0 | 524.0 | 56.771398 | 田北辰 | . 27 5.0 | 1.0 | 4.0 | 380.0 | 2.0 | 541.0 | 58.613218 | 謝偉俊 | . 70 0.0 | 0.0 | 0.0 | 378.0 | 0.0 | 545.0 | 59.046587 | 鍾國斌 | . 22 4.0 | 2.0 | 2.0 | 377.0 | 1.0 | 545.0 | 59.046587 | 李國麟 | . 3. &#26681;&#25818;&#25919;&#40680;&#30340;&#25237;&#31080;&#20998;&#26512; . 從 Wikipedia 上找到各個議員的所屬政黨 . from bs4 import BeautifulSoup . with open(&#39;./party.html&#39;, &#39;r&#39;) as f: party = f.read() party_soup = BeautifulSoup(party, &#39;html.parser&#39;) tr = party_soup.find_all(&#39;tr&#39;) tr_text = [] for i in tr: j = i.text.strip().split(&#39; n n&#39;) a = j[0].split(&#39; n&#39;)[-1] b = j[-1].split(&#39; n&#39;)[0] tr_text.append([a, b]) tr_text.remove([&#39;備註&#39;, &#39;席位&#39;]) tr_text.remove([&#39;懸空&#39;, &#39;&#39;]) for _ in range(3): tr_text.remove([&#39;懸空&#39;, &#39;懸空&#39;]) party_pd = pd.DataFrame(tr_text) # correct error due to format party_pd.loc[party_pd[0] == &#39;吳永嘉&#39;, 1] = &#39;經民聯&#39; party_pd.loc[party_pd[0] == &#39;邵家輝&#39;, 1] = &#39;自由黨&#39; # add missing members missing = pd.DataFrame([{0: &#39;張華峰&#39;, 1: &#39;經民聯&#39;}, {0: &#39;何啟明&#39;, 1: &#39;工聯會&#39;}, {0: &#39;范國威&#39;, 1: &#39;香港本土&#39;}, {0: &#39;區諾軒&#39;, 1: &#39;獨立民主派&#39;}, {0: &#39;梁國雄&#39;, 1: &#39;社民連&#39;}, {0: &#39;羅冠聰&#39;, 1: &#39;眾志&#39;}, {0: &#39;姚松炎&#39;, 1: &#39;專業議政&#39;}, {0: &#39;劉小麗&#39;, 1: &#39;工黨&#39;}, # {0: &#39;梁頌恆&#39;, 1: &#39;青年新政&#39;}, # {0: &#39;游蕙禎&#39;, 1: &#39;青年新政&#39;} ]) party_pd = party_pd.append(missing, ignore_index=True) party_pd.replace(&#39;公民黨/專業議政&#39;, &#39;公民黨&#39;, inplace=True) party_pd.replace(&#39;教協/專業議政&#39;, &#39;專業議政&#39;, inplace=True) party_pd.replace(&#39;民建聯/新界社團聯會&#39;, &#39;民建聯&#39;, inplace=True) party_pd.replace(&#39;新民黨/公民力量&#39;, &#39;新民黨&#39;, inplace=True) party_pd.replace(&#39;公專聯/專業議政&#39;, &#39;公專聯&#39;, inplace=True) party_pd.replace(&#39;經民聯/西九新動力&#39;, &#39;經民聯&#39;, inplace=True) . party_pd[1].unique() . array([&#39;新民黨&#39;, &#39;工聯會&#39;, &#39;民主黨&#39;, &#39;民建聯&#39;, &#39;公民黨&#39;, &#39;經民聯&#39;, &#39;香港本土&#39;, &#39;獨立建制派&#39;, &#39;獨立民主派&#39;, &#39;實政圓桌&#39;, &#39;熱血公民&#39;, &#39;工黨&#39;, &#39;人民力量&#39;, &#39;自由黨&#39;, &#39;專業議政&#39;, &#39;公專聯&#39;, &#39;獨立中間派&#39;, &#39;勞聯&#39;, &#39;新論壇&#39;, &#39;街工&#39;, &#39;社民連&#39;, &#39;眾志&#39;], dtype=object) . party_pd.head() . 0 1 . 0 葉劉淑儀 | 新民黨 | . 1 郭偉强 | 工聯會 | . 2 許智峯 | 民主黨 | . 3 張國鈞 | 民建聯 | . 4 陳淑莊 | 公民黨 | . - &#20197;&#25919;&#40680;&#20316;&#21934;&#20301;&#30340;&#25237;&#31080;&#29575; . member_summary_party = pd.merge(member_summary, party_pd, left_on=&#39;member&#39;, right_on=0, how=&#39;left&#39;) member_summary_party.drop(columns=0, inplace=True) member_summary_party . counts passed negatived Absent Present vote_num vote_rate member 1 . 0 1.0 | 0.0 | 1.0 | 115.0 | 0.0 | 808.0 | 87.540628 | 何俊賢 | 民建聯 | . 1 1.0 | 1.0 | 0.0 | 42.0 | 0.0 | 494.0 | 92.164179 | 鄭泳舜 | 民建聯 | . 2 1.0 | 1.0 | 0.0 | 4.0 | 0.0 | 313.0 | 98.738170 | 陳凱欣 | 獨立建制派 | . 3 1.0 | 1.0 | 0.0 | 217.0 | 1.0 | 705.0 | 76.381365 | 張宇人 | 自由黨 | . 4 1.0 | 0.0 | 1.0 | 50.0 | 2.0 | 871.0 | 94.366197 | 潘兆平 | 勞聯 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 68 0.0 | 0.0 | 0.0 | 24.0 | 899.0 | 0.0 | 0.000000 | 梁君彥 | 經民聯 | . 69 0.0 | 0.0 | 0.0 | 449.0 | 3.0 | 471.0 | 51.029252 | 石禮謙 | 經民聯 | . 70 0.0 | 0.0 | 0.0 | 378.0 | 0.0 | 545.0 | 59.046587 | 鍾國斌 | 自由黨 | . 71 0.0 | 0.0 | 0.0 | 23.0 | 3.0 | 897.0 | 97.183099 | 陳振英 | 獨立建制派 | . 72 0.0 | 0.0 | 0.0 | 413.0 | 0.0 | 510.0 | 55.254605 | 劉業強 | 經民聯 | . 73 rows × 9 columns . pty_vote_rate = member_summary_party.groupby(1).vote_rate.mean().sort_values() plt.rcParams[&#39;figure.figsize&#39;] = (10, 10) ax = pty_vote_rate.plot(kind=&#39;barh&#39;, alpha=0.7, title=&#39;各政黨的平均投票率&#39;) ax.set_xlabel(&#39;投票率（％）&#39;) ax.set_ylabel(&quot;政黨&quot;) ax.axvline(x=50, color=&#39;red&#39;, ls=&#39;--&#39;, alpha=1, label=&#39;50%&#39;) . &lt;matplotlib.lines.Line2D at 0x12288eb90&gt; . - &#21508;&#25919;&#40680;&#25237;&#31080;&#30340;&#32113;&#19968;&#24615;&#20998;&#26512; . 為分析各政黨議員投票的統一性，我們先定義一個統一性的分數。 . $$Score = frac{A (Yes - No)^2 + B (Yes - Abstain)^2 + C (No - Abstain)^2}{(Yes + No + Abstain)^2} $$ . Score = 1 為同一次投票內選擇完全一致（不包括缺席），越分散分數越低。 . 就每一次的投票結果而言 Yes 和 No 及 Abstain 是對立的，但 No 和 Abstain 雖然立場有不同但做成結果一致，所以把 Yes-No 和 Yes-Abstain 的比重 (A, B) 設成 2，而 No-Abstain (C) 則設成 0.5。把 function 寫成可以改變比重的模式方便日後（反悔時）調整。 . legco_member_vote = legco_cm[members] def diff_vote(inputList, weight=[2, 2, 0.5]): yes = 0 no = 0 abstain = 0 for vote in inputList: if vote == &#39;Yes&#39;: yes += 1 elif vote == &#39;No&#39;: no += 1 elif vote == &#39;Abstain&#39;: abstain += 1 if (yes**2 + no**2 + abstain**2) &gt; 0: diff = 1 - (weight[0] * 2 * (yes * no) + weight[2] * 2 * (no * abstain) + weight[1] * 2 * (yes * abstain)) / (yes + no + abstain)**2 else: diff = np.nan return diff . 測試： . party_member_list = list(party_pd.groupby(1).get_group(&#39;民建聯&#39;)[0]) test = [] for i in range(legco_member_vote.shape[0]): diff = diff_vote(legco_member_vote[(member for member in legco_member_vote.columns if member in party_member_list)].loc[i]) test.append(diff) legco_cm[&#39;民建聯&#39;] = test print(legco_cm[&#39;民建聯&#39;].describe()) legco_cm[legco_cm[&#39;民建聯&#39;] &lt; 1][[member for member in legco_cm.columns if member in party_member_list]+[&#39;motion&#39;]] . count 923.000000 mean 0.997819 std 0.028279 min 0.479290 25% 1.000000 50% 1.000000 75% 1.000000 max 1.000000 Name: 民建聯, dtype: float64 . 黃定光 李慧琼 陳克勤 何俊賢 陳恒鑌 梁志祥 葛珮帆 蔣麗芸 周浩鼎 柯創盛 張國鈞 劉國勳 鄭泳舜 motion . 97 Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | No | Abstain | Abstain | 根據《立法會(權力及特權)條例》動議的議案 | . 125 Abstain | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | Yes | NaN | 縮短點名表決響鐘時間的議案 | . 463 No | No | Yes | No | No | No | No | Absent | Absent | No | No | No | NaN | 《2017年撥款條例草案》 - 全體委員會審議階段 - 總目90的修正案 (修正案編號72) | . 603 No | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Absent | Abstain | Abstain | 郭家麒議員對何君堯議員的「活化強制性公積金」議案作出的修正案 | . 730 Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Abstain | Yes | Yes | 陸頌雄議員對何啟明議員的「全面檢討勞工法例，改善勞工權益」議案作出的修正案 | . 786 Absent | Absent | Yes | Abstain | Yes | Yes | Yes | Yes | Yes | Yes | Absent | Absent | Yes | 修訂《2018年食物攙雜(金屬雜質含量)(修訂)規例》的擬議決議案 | . 828 No | No | No | No | No | No | No | No | No | No | No | No | Yes | 《國歌條例草案》 - 全體委員會審議 - 陳志全議員的第十四及十五項修正案 (修正案編號18... | . 如果每一個政黨都要把投票不一的議題全部列出太貼位置，把迴圈修改為只列出 motion id。同時亦把只有一名立法會議員的政黨除掉。 . party_vote_summary = [] for party in party_pd[1].unique(): party_member_list = list(party_pd.groupby(1).get_group(party)[0]) test = [] diff_list = [] for i in range(legco_member_vote.shape[0]): diff = diff_vote(legco_member_vote[(member for member in legco_member_vote.columns if member in party_member_list)].loc[i]) test.append(diff) if diff &lt; 1: diff_list.append(i) legco_cm[party] = test party_vote = {&#39;party-name&#39;: party, &#39;num-members&#39;: len([member for member in legco_member_vote.columns if member in party_member_list]), &#39;involed-num-motion&#39;: legco_cm[party].count(), &#39;num-motion-not-unify&#39;: len(diff_list), &#39;motion-list&#39;: diff_list, &#39;score-mean&#39;: legco_cm[party].mean(), &#39;score-sd&#39;: legco_cm[party].std() } party_vote_summary.append(party_vote) party_vote_summary_df = pd.DataFrame(party_vote_summary) party_vote_summary_df[party_vote_summary_df[&#39;num-members&#39;] &gt; 1].sort_values(&#39;score-mean&#39;) # print(legco_cm[legco_cm[party] &lt; 1][[member for member in legco_cm.columns if member in party_member_list]+[&#39;motion&#39;]]) . party-name num-members involed-num-motion num-motion-not-unify motion-list score-mean score-sd . 8 獨立民主派 | 4 | 864 | 201 | [8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22,... | 0.808489 | 0.364708 | . 7 獨立建制派 | 8 | 918 | 143 | [2, 63, 71, 72, 73, 75, 76, 77, 78, 86, 87, 88... | 0.914695 | 0.234340 | . 15 公專聯 | 2 | 739 | 46 | [8, 35, 45, 48, 93, 94, 110, 167, 211, 272, 29... | 0.951962 | 0.205512 | . 5 經民聯 | 8 | 923 | 69 | [65, 71, 76, 87, 88, 95, 97, 99, 105, 145, 146... | 0.968870 | 0.137125 | . 6 香港本土 | 2 | 751 | 26 | [601, 609, 610, 611, 612, 628, 638, 639, 719, ... | 0.970373 | 0.165945 | . 14 專業議政 | 2 | 639 | 21 | [65, 66, 72, 155, 287, 288, 289, 291, 295, 296... | 0.977700 | 0.138536 | . 11 工黨 | 2 | 727 | 15 | [65, 66, 72, 81, 283, 292, 294, 389, 555, 588,... | 0.987620 | 0.100882 | . 13 自由黨 | 4 | 923 | 14 | [0, 86, 87, 88, 112, 210, 261, 272, 395, 611, ... | 0.992672 | 0.070618 | . 2 民主黨 | 7 | 829 | 9 | [55, 214, 215, 216, 217, 284, 285, 399, 837] | 0.993640 | 0.067003 | . 4 公民黨 | 5 | 877 | 6 | [79, 270, 720, 797, 800, 810] | 0.995634 | 0.055960 | . 3 民建聯 | 13 | 923 | 7 | [97, 125, 463, 603, 730, 786, 828] | 0.997819 | 0.028279 | . 1 工聯會 | 5 | 914 | 3 | [60, 261, 549] | 0.998065 | 0.037059 | . 0 新民黨 | 2 | 867 | 1 | [292] | 0.998847 | 0.033962 | . 可以看到以獨立民主派分歧最大，新民黨最統一（但只有 2 人），其次是工聯會和民建聯。 . party_vote_summary = party_vote_summary_df[party_vote_summary_df[&#39;num-members&#39;] &gt; 1].sort_values(&#39;score-mean&#39;) inp = np.arange(party_vote_summary.shape[0]) plt.barh(inp, party_vote_summary[&#39;score-mean&#39;], alpha=0.7) plt.title(&#39;Voting Score of Parties&#39;) plt.yticks(inp, party_vote_summary[&#39;party-name&#39;]) plt.show() . - &#25509;&#19979;&#20358;&#26681;&#25818; wikipedia &#30340;&#23450;&#32681;&#25226;&#25919;&#40680;&#20998;&#28858;&#27867;&#27665;&#21644;&#24314;&#21046;&#20841;&#22823;&#38499;&#29151;&#20316;&#20998;&#26512; . 分類根據 wikipedia 對建制和泛民的定義 . proBJ = [&#39;民建聯&#39;, &#39;工聯會&#39;, &#39;經民聯&#39;, &#39;自由黨&#39;, &#39;新民黨&#39;, &#39;實政圓桌&#39;, &#39;新論壇&#39;, &#39;勞聯&#39;] proDem = [&#39;民主黨&#39;, &#39;公民黨&#39;, &#39;工黨&#39;, &#39;街工&#39;, &#39;公專聯&#39;, &#39;人民力量&#39;, &#39;社民連&#39;] all_parties = [&#39;新民黨&#39;, &#39;工聯會&#39;, &#39;民主黨&#39;, &#39;民建聯&#39;, &#39;公民黨&#39;, &#39;經民聯&#39;, &#39;香港本土&#39;, &#39;獨立建制派&#39;, &#39;獨立民主派&#39;, &#39;實政圓桌&#39;, &#39;熱血公民&#39;, &#39;工黨&#39;, &#39;人民力量&#39;, &#39;自由黨&#39;, &#39;專業議政&#39;, &#39;公專聯&#39;, &#39;獨立中間派&#39;, &#39;勞聯&#39;, &#39;新論壇&#39;, &#39;街工&#39;, &#39;社民連&#39;, &#39;眾志&#39;] . def getPartyMember(partyList): memberList = [] for party in partyList: memberList += list(party_pd.groupby(1).get_group(party)[0]) return memberList . side_vote_summary = [] side = [{&#39;name&#39;: &#39;建制&#39;, &#39;party-list&#39;: proBJ}, {&#39;name&#39;: &#39;泛民&#39;, &#39;party-list&#39;: proDem}, {&#39;name&#39;: &#39;立法會全體&#39;, &#39;party-list&#39;: all_parties} ] for s in side: party_member_list = getPartyMember(s[&#39;party-list&#39;]) test = [] diff_list = [] for i in range(legco_member_vote.shape[0]): diff = diff_vote(legco_member_vote[(member for member in legco_member_vote.columns if member in party_member_list)].loc[i]) test.append(diff) if diff &lt; 1: diff_list.append(i) legco_cm[s[&#39;name&#39;]] = test party_vote = {&#39;party-name&#39;: s[&#39;name&#39;], &#39;num-members&#39;: len([member for member in legco_member_vote.columns if member in party_member_list]), &#39;involed-num-motion&#39;: legco_cm[s[&#39;name&#39;]].count(), &#39;num-motion-not-unify&#39;: len(diff_list), &#39;motion-list&#39;: diff_list, &#39;score-mean&#39;: legco_cm[s[&#39;name&#39;]].mean(), &#39;score-sd&#39;: legco_cm[s[&#39;name&#39;]].std() } side_vote_summary.append(party_vote) . side_summary_df = pd.DataFrame(side_vote_summary) side_summary_df . party-name num-members involed-num-motion num-motion-not-unify motion-list score-mean score-sd . 0 建制 | 35 | 923 | 219 | [0, 60, 61, 62, 63, 64, 65, 71, 72, 73, 74, 75... | 0.876516 | 0.270388 | . 1 泛民 | 19 | 922 | 339 | [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ... | 0.759984 | 0.367609 | . 2 立法會全體 | 73 | 923 | 887 | [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... | 0.271340 | 0.275440 | . inp = np.arange(side_summary_df.shape[0]) plt.bar(inp, side_summary_df[&#39;score-mean&#39;], alpha=0.7) plt.title(&#39;Voting Score of Parties&#39;) plt.xticks(inp, [&#39;建制&#39;, &#39;泛民&#39;, &#39;立法會全體&#39;]) plt.show() . - &#20998;&#26512;&#65306;&#22914;&#26524;... . 在把立法會議員分成建制及泛民兩邊時我們不難發現建制派有著人數上的優勢，如果把獨立建制派也加在內的話總人數達 43 人，也就是 2/3 的總人數。但是把投票並沒有完全統一及缺席也加算在內的話，在議員動議統計也不難發現建制派的動議被否決或是泛民派的動議通過。在這部份我們將探討一下有多少動議是如果泛民派足夠團結/全員出席後有機會改變結果。 . proBJ += [&#39;獨立建制派&#39;] proBjMember = [x for x in members if (member_summary_party[(member_summary_party[&#39;member&#39;] == x)][1].values in proBJ)] proBJPass = np.zeros(legco_member_vote.shape[0]) proBJNeg = np.zeros(legco_member_vote.shape[0]) proBJAbs = np.zeros(legco_member_vote.shape[0]) for i in range(legco_member_vote.shape[0]): p = 0 n = 0 absend = 0 for member in proBjMember: if legco_member_vote[member].iloc[i] in [&#39;No&#39;, &#39;Astain&#39;]: n += 1 elif legco_member_vote[member].iloc[i] == &#39;Yes&#39;: p += 1 else: absend += 1 proBJPass[i] = p proBJNeg[i] = n proBJAbs[i] = absend legco_cm[&#39;Pro-BJ-pass&#39;] = proBJPass legco_cm[&#39;Pro-BJ-neg&#39;] = proBJNeg legco_cm[&#39;Pro-BJ-abs&#39;] = proBJAbs legco_cm[[&#39;Pro-BJ-pass&#39;, &#39;Pro-BJ-neg&#39;, &#39;Pro-BJ-abs&#39;, &#39;overall-yes&#39;, &#39;overall-no&#39;, &#39;result&#39;]].head() . Pro-BJ-pass Pro-BJ-neg Pro-BJ-abs overall-yes overall-no result . 0 0.0 | 32.0 | 11.0 | 16 | 33 | Negatived | . 1 33.0 | 0.0 | 10.0 | 33 | 18 | Passed | . 2 0.0 | 31.0 | 12.0 | 18 | 31 | Negatived | . 3 0.0 | 35.0 | 8.0 | 11 | 38 | Negatived | . 4 0.0 | 35.0 | 8.0 | 8 | 38 | Negatived | . 假如加上獨立民立派 4 名議員，泛民主派議員人數一共 23 人，查看一下有沒有 23 票能改變的議題。我們先把建制派的投票中支持票及反對票差別少於 23 票的動議找出來。（由議員提出的動議需要分組投票才能通過，為簡化過程這部份我們只看由政府提出的動議。） . legco_cm[&#39;pro-BJ-diff&#39;] = abs(legco_cm[&#39;Pro-BJ-pass&#39;] - legco_cm[&#39;Pro-BJ-neg&#39;]) legco_cm[(legco_cm[&#39;pro-BJ-diff&#39;] &lt;= 23) &amp; (legco_cm[&#39;mover-type&#39;] == &#39;Public Officer&#39;)][[&#39;vote-id&#39;, &#39;Pro-BJ-pass&#39;, &#39;Pro-BJ-neg&#39;, &#39;overall-vote&#39;, &#39;overall-yes&#39;, &#39;result&#39;]] . vote-id Pro-BJ-pass Pro-BJ-neg overall-vote overall-yes result . 104 20180328004 | 23.0 | 0.0 | 44 | 44 | Passed | . 263 20180207002 | 7.0 | 21.0 | 41 | 12 | Negatived | . 275 20170524001 | 16.0 | 0.0 | 35 | 34 | Passed | . 276 20170524002 | 16.0 | 0.0 | 35 | 35 | Passed | . 277 20170524003 | 22.0 | 0.0 | 39 | 38 | Passed | . 278 20170524004 | 20.0 | 0.0 | 36 | 35 | Passed | . 279 20170524005 | 20.0 | 0.0 | 37 | 36 | Passed | . 280 20170524006 | 19.0 | 0.0 | 36 | 35 | Passed | . 282 20170524008 | 21.0 | 0.0 | 38 | 38 | Passed | . 560 20170517163 | 23.0 | 0.0 | 38 | 23 | Passed | . 902 20180509067 | 21.0 | 0.0 | 29 | 22 | Passed | . 當中由政府提出的 20170517163 號及 20180509067 號看來是泛民主派有機會以及想要拉倒的動議。 . legco_cm[legco_cm[&#39;vote-id&#39;] == &#39;20170517163&#39;][[&#39;motion&#39;, &#39;mover&#39;, &#39;overall-vote&#39;, &#39;overall-yes&#39;, &#39;Pro-BJ-pass&#39;, &#39;overall-no&#39;]] . motion mover overall-vote overall-yes Pro-BJ-pass overall-no . 560 《2017年撥款條例草案》 - 全體委員會審議階段 - 總目21、22、25、26、28、3... | 財政司司長 | 38 | 23 | 23.0 | 15 | . legco_cm[legco_cm[&#39;vote-id&#39;] == &#39;20180509067&#39;][[&#39;motion&#39;, &#39;mover&#39;, &#39;overall-vote&#39;, &#39;overall-yes&#39;, &#39;Pro-BJ-pass&#39;, &#39;overall-no&#39;]] . motion mover overall-vote overall-yes Pro-BJ-pass overall-no . 902 《2018年撥款條例草案》 - 全體委員會審議 - 總目21、22、28、30、33、44、... | 財政司司長 | 29 | 22 | 21.0 | 5 | . 都是年度撥款條例草案，然而參與投票不足下未能達到想要的結果。（當然在現實建制派發現泛民主派投票人數增加也有動員參加投票的可能，所以並不能說現實上如果泛民主派努力一點團結一點就能成功。在這裡只是以這個假想情況作例子示範一下 pandas 如何幫我們找到想要的數據。） . 4. &#31435;&#27861;&#26371;&#30340;&#25237;&#31080;&#21462;&#21521;&#35222;&#20687;&#21270; . 在這個部份我們試著把各議員的投票傾向分類和視像化。首先我們會用上面提及的評分方法計算議員之間的距離評分，議員之間投票的相似度可以用 heat map 來視像化。然後我們會用 Multidimensional Scaling （MDS）方法把不易看懂的 heat map 轉換成二維坐標圖分析。 . - &#35696;&#21729;&#25237;&#31080;&#36317;&#38626;&#36317;&#38499; . 這裡以之前定義的評分來計算投票距離 （1-Score)，使 0 為最近，投票方向越不同距離越大。 . member_vote = legco_cm[members].drop(columns=&#39;梁君彥&#39;) # drop him as he is the chairman of cm who did not vote at all # This script took more then 30 mins to finish in my notebook... matrix = [] timer = 0 print(str(timer) + &quot;/73&quot;, end=&#39; &#39;) for member1 in member_vote.columns: mem_dict = {} for member2 in member_vote.columns: diff_list = np.zeros(member_vote.shape[0]) for i in range(member_vote.shape[0]): diff = diff_vote(member_vote[[member1, member2]].loc[i]) diff_list[i] = diff mem_dict[member2] = 1 - np.nanmean(diff_list) matrix.append(mem_dict) timer += 1 print(str(timer) + &quot;/73&quot;, end=&#39; &#39;) # just to enusre the program is running print() matrix_df = pd.DataFrame(matrix) . 0/73 1/73 2/73 3/73 4/73 5/73 6/73 7/73 8/73 9/73 10/73 11/73 12/73 13/73 14/73 15/73 16/73 17/73 18/73 19/73 20/73 21/73 22/73 23/73 24/73 25/73 26/73 27/73 28/73 29/73 30/73 31/73 32/73 33/73 34/73 35/73 36/73 37/73 38/73 39/73 40/73 41/73 42/73 43/73 44/73 45/73 46/73 47/73 48/73 49/73 50/73 51/73 52/73 53/73 54/73 55/73 56/73 57/73 58/73 59/73 60/73 61/73 62/73 63/73 64/73 65/73 66/73 67/73 68/73 69/73 70/73 71/73 72/73 . 把 index 換成名字。 . rename_dict = {} i = 0 for member1 in member_vote.columns: rename_dict[i] = member1 i += 1 matrix_df.rename(rename_dict, inplace=True) matrix_df.head() . 涂謹申 梁耀忠 石禮謙 張宇人 李國麟 林健鋒 黃定光 李慧琼 陳克勤 陳健波 ... 譚文豪 范國威 區諾軒 鄭泳舜 謝偉銓 陳凱欣 梁國雄 羅冠聰 姚松炎 劉小麗 . 涂謹申 0.000000 | 0.115141 | 0.304269 | 0.346774 | 0.071321 | 0.368205 | 0.337100 | 0.368750 | 0.370413 | 0.364872 | ... | 0.075898 | 0.080556 | 0.091052 | 0.330251 | 0.350293 | 0.204473 | 0.057258 | 0.039101 | 0.038986 | 0.043582 | . 梁耀忠 0.115141 | 0.000000 | 0.417655 | 0.585812 | 0.143733 | 0.481287 | 0.544711 | 0.564302 | 0.572987 | 0.573562 | ... | 0.081528 | 0.035663 | 0.029078 | 0.477149 | 0.508916 | 0.325403 | 0.018342 | 0.011954 | 0.018321 | 0.013926 | . 石禮謙 0.304269 | 0.417655 | 0.000000 | 0.013654 | 0.310056 | 0.006275 | 0.023687 | 0.023714 | 0.023783 | 0.021216 | ... | 0.425733 | 0.272196 | 0.250389 | 0.012327 | 0.011960 | 0.005426 | 0.089370 | 0.111250 | 0.043986 | 0.125000 | . 張宇人 0.346774 | 0.585812 | 0.013654 | 0.000000 | 0.377481 | 0.021825 | 0.060211 | 0.071266 | 0.072456 | 0.035675 | ... | 0.555459 | 0.333333 | 0.297990 | 0.043147 | 0.028958 | 0.033641 | 0.167197 | 0.142674 | 0.082882 | 0.170354 | . 李國麟 0.071321 | 0.143733 | 0.310056 | 0.377481 | 0.000000 | 0.328125 | 0.348077 | 0.368664 | 0.387472 | 0.390660 | ... | 0.117034 | 0.128582 | 0.136364 | 0.317760 | 0.366373 | 0.275920 | 0.032143 | 0.017988 | 0.021084 | 0.017085 | . 5 rows × 72 columns . 把距陣以 heatmap 展視。（基本上看不出什麼...） . ax = sns.heatmap(matrix_df, vmin=0, vmax=1) . 導入 scikit learn 的 MDS 模組。 . from sklearn.manifold import MDS model = MDS(n_components=2, dissimilarity=&#39;precomputed&#39;, random_state=1) . out = model.fit_transform(matrix_df) member_scatt = pd.DataFrame({&#39;member&#39;: matrix_df.columns, &#39;x&#39;: out[:, 0], &#39;y&#39;: out[:, 1]}) member_scatt = member_scatt.merge(member_summary_party[[&#39;member&#39;, 1]], on=&#39;member&#39;) member_scatt.head() . member x y 1 . 0 涂謹申 | 0.089394 | 0.165066 | 民主黨 | . 1 梁耀忠 | 0.127461 | 0.307847 | 街工 | . 2 石禮謙 | -0.065369 | -0.111455 | 經民聯 | . 3 張宇人 | -0.116345 | -0.132765 | 自由黨 | . 4 李國麟 | 0.126469 | 0.148297 | 獨立民主派 | . plt.rcParams[&#39;figure.figsize&#39;] = (15, 15) sns.scatterplot(member_scatt[&#39;x&#39;], member_scatt[&#39;y&#39;], hue=member_scatt[1], s=100) def label_point(x, y, val, ax): for i in range(len(x)): ax.text(x[i]+.005, y[i]-0.002, str(val[i])) label_point(out[:, 0], out[:, 1], matrix_df.columns, plt.gca()) . 在這個圖可以看出建制派整體的投票比較一致，泛民主派雖然投票傾向遠離建制派，但分怖比較離散。 .",
            "url": "https://tikuischan.github.io/fastpages_blog/project/pandas/sklearn/2020/03/18/legco6thAnalysis.html",
            "relUrl": "/project/pandas/sklearn/2020/03/18/legco6thAnalysis.html",
            "date": " • Mar 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Pervious Logs",
          "content": "26-6-2021 . Keeping blogging is not easy, especially when one of your majoy source of learning is blogs written by different experts, which makes my blog looks stupid… . 20-3-2021 . It is not the first time I tried to build a blog since the beginning of 2020 when I start learning Python and Machine Learning. I’ve tried blogger and build a static page hosting in github, those experience are not so good at all, mainly because of my ignorance (don’t know how and where to start) and laziness (it is hard to keep up if you have to update a post with tons of works to make it looks reasonable :P). A year later, I landed my new position as a Data Scientist, learnt a bit more, it seems to be a good time to restart :) . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tikuischan.github.io/fastpages_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tikuischan.github.io/fastpages_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}